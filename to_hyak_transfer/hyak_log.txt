#### Hyak command line commands

# Copying folders:
# Note: To move to remote, you have to execute the command from your local machine
scp -r 1200cov_hyak/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Moving stuff back off of hyak:
# Note: You again need to be on your local machine, in the ./from_hyak_transfer folder
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-17_origin_1200/ ./

# Run the Rscript locally
srun -p stf -A stf --ntasks=3 --mem=100G --time=4:00:00 --pty bash â€“l

# Get an interactive build node (run things from command line):
srun -p build --time=2:00:00 --mem=10G --pty /bin/bash

# To cancel a job: scancel <jobid>

# Useful R info: https://wiki.cac.washington.edu/display/hyakusers/Hyak+R+programming

# R module load:
module load contrib/R-4.2.0/4.2.0

##### Installing R (version 4) on MOX Hyak!

# The second option from this link appears to work: https://stackoverflow.com/questions/46343044/install-r-in-linux-server
# Make sure to run ./configure --with-pcre1

# What I ran:
wget http://cran.rstudio.com/src/base/R-4/R-4.2.0.tar.gz
tar xvf R-4.2.0.tar.gz
cd R-4.2.0
./configure --prefix=/sw/contrib/R-4.2.0 --with-pcre1

## Output after configure:
``
R is now configured for x86_64-pc-linux-gnu

  Source directory:            .
  Installation directory:      /sw/contrib/R-4.2.0

  C compiler:                  gcc -std=gnu11  -g -O2
  Fortran fixed-form compiler: gfortran  -g -O2

  Default C++ compiler:        g++ -std=gnu++11  -g -O2
  C++11 compiler:              g++ -std=gnu++11  -g -O2
  C++14 compiler:                 
  C++17 compiler:                 
  C++20 compiler:                 
  Fortran free-form compiler:  gfortran  -g -O2
  Obj-C compiler:	       gcc -g -O2 -fobjc-exceptions

  Interfaces supported:        X11, tcltk
  External libraries:          pcre1, readline, curl
  Additional capabilities:     PNG, JPEG, TIFF, NLS, cairo, ICU
  Options enabled:             shared BLAS, R profiling

  Capabilities skipped:        
  Options not enabled:         memory profiling

  Recommended packages:        yes

configure: WARNING: you cannot build PDF versions of the R manuals
configure: WARNING: you cannot build PDF versions of vignettes and help pages
``
make && make install

# Then create a module file using these instructions: https://wiki.cac.washington.edu/display/hyakusers/Hyak_modules

# This worked: Just with some odd file paths
# Now to run R:
module load contrib/R-4.2.0/4.2.0


##### Installing JAGS on Hyak
# From this website: http://levlafayette.com/node/498

# In directory: gscratch/stf/mmin/JAGS:

# Source attempt 1
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz

# Okay, I think that the above file is for Mac? Let's try this link to the Debian version:

# Source code (Debian)
wget https://ftp.debian.org/debian/pool/main/j/jags/jags_4.3.1-1.debian.tar.xz
tar xvf jags_4.3.1-1.debian.tar.xz
mv debian jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1

# Yeah, so I can't figure out how to get this to work. Let's try again with the binary
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz
tar xvf JAGS-4.3.1.tar.gz
mv JAGS-4.3.1 jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1
make && make install

# Info at this link: https://github.com/cran/rjags/blob/master/INSTALL
# Then, you need to run this from the command line to tell R where to look for JAGS when installing rjags:
export PKG_CONFIG_PATH=/sw/contrib/JAGS/jags-4.3.1/lib/pkgconfig
# Then run this:
export LD_RUN_PATH=/sw/contrib/JAGS/jags-4.3.1/lib
# this worked!


#### 2022-05-17 ####

Moved folder to hyak to run origin only model w 1200 fish:

From within directory: /Users/markusmin/Documents/CBR/steelhead/hyak_transfer

# Origin only runs:

scp -r 2022-05-17_origin_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Move back to local:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-17_origin_1200/ ./

# Origin and rear runs:
scp -r 2022-05-17_origin_rear_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Move back to local:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-17_origin_rear_1200/ ./


#### 2022-05-18 ####

Temperature only runs:

Moving to hyak:
scp -r 2022-05-18_temp_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Ran these models

# Move back to local
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-18_temp_1200/ ./

Temperature and flow runs:

Moving to hyak:
scp -r 2022-05-18_temp_flow_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Ran these models

# Move back to local
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-18_temp_flow_1200/ ./

#### 2022-05-19 ####

All 4 covariate models:

Moving to hyak:
scp -r 2022-05-19_all4_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

#### 2022-05-20 ####

Moving all 4 cov model off of hyak
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-19_all4_1200/ ./

#### 2022-05-23 ####

Running complete det hist for loop on hyak

# move to hyak:
scp -r 2022-05-23-complete_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# move back to local - troubleshoot

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-23-complete_det_hist/ ./

#### 2022-05-24 ####

# Running det hist for loops

scp -r 2022-05-24_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Just getting the slurm output - troubleshoot

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/slurm-2736747.out ./

#### 2022-05-25 ####

My complete det hist run from yesterday seems to have completed overnight - pull it off and check

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/ ./

Looks like it ran okay!

Moved these two files to /Users/markusmin/Documents/CBR/steelhead/to_hyak_transfer/2022-05-25-complete_det_hist
complete_event_site_metadata.csv
complete_det_hist.csv

# Moved this new folder to hyak

scp -r 2022-05-25-complete_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Pull it back off hyak


scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-25-complete_det_hist/ ./


#### 5-26-22 ####

Pulled runs off hyak

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-25-complete_det_hist/ ./

Fixed a typo, moved back to hyak

Created a second version of the R script for testing, with an associated slurm script (both of these have test in the name)

Ran both scripts


scp -r 2022-05-25-complete_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/


Okay, it looks like the complete det hist script ran!


Now, run the intercept only model:
- first transfer to hyak:
scp -r 2022-05-26_fullmodel_intercept_only mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/
# Ran model
Got error - ran out of memory:
[1] "Start time: 2022-05-26 15:53:30"
Error in unserialize(node$con) : error reading from connection
Calls: jags.parallel ... FUN -> recvData -> recvData.SOCKnode -> unserialize
Execution halted
slurmstepd: error: Detected 7 oom-kill event(s) in StepId=2741132.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.

Let's try running this on klone then since there's more memory there
scp -r 2022-05-26_fullmodel_intercept_only mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

Okay - so I would have to entirely reconstruct my computing environment on klone for this to run. Will take a bit longer to do this


#### 5-27-22 ####
The first thing I tired to address the memory issue is to change the parameters monitored from the whole matrix (which has lots of zeros) to just the ones of interest.
I did this by indexing in the JAGS code and storing these as parameters, and then monitoring those.
- This took us down from 841 monitored parameters to 54
- uploaded this new script to hyak:

scp -r 2022-05-26_fullmodel_intercept_only/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

Still crashed...


### 5-31-22 ###

Okay, let's try Mark's suggestion to monitor less parameters

Monitor only the first ten - move to MOX
scp -r 2022-05-31_fullmodel_intercept_only_monitor_1.10/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

This didn't work

Try Mark's other suggestion to drop the iter and thin and burn way down
scp -r 2022-05-31_fullmodel_intercept_only_monitor_1.10/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/


### 06-01-22

It seems to work less and less - maybe I need to clear the workspace?

scp -r 2022-06-01_fullmodel_intercept_only_monitor_1.1/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/



### Reinstalling everything on KLONE ###

##### Installing R (version 4) on KLONE

# Very useful link: https://hyak.uw.edu/docs/compute/scheduling-jobs/

# Note: there are no build nodes on KLONE - see this link for differences from MOX: https://hyak.uw.edu/blog/mox-to-klone
# Instead of srun, use salloc
salloc -p compute -A stf -N 1 -c 1 --time=2:00:00 --mem=10G

# run hyakalloc to show what resources are available
- interestingly, now the partitions available for account stf are compute, gpu-2080ti, hugemem, and interactive

# How to install stuff: https://hyak.uw.edu/docs/tools/containers

# The second option from this link appears to work: https://stackoverflow.com/questions/46343044/install-r-in-linux-server
# Make sure to run ./configure --with-pcre1

# What I ran:
First, navigate to /sw/contrib/stf-src/
wget http://cran.rstudio.com/src/base/R-4/R-4.2.0.tar.gz
tar xvf R-4.2.0.tar.gz
cd R-4.2.0
# Configuring is not working
./configure --prefix=/sw/contrib/stf-src/R-4.2.0 --with-pcre1 # Didn't work
./configure --with-readline=no --with-x=no
./configure --prefix=/sw/contrib/stf-src --with-readline=no --with-x=no # didn't work
make && make install

apptainer exec tools.sif /sw/contrib/stf-src/R-4.2.0/configure --prefix=/sw/contrib/stf-src/R-4.2.0 --with-pcre2 --with-readline=no --with-x=no

# I keep getting this error: configure: error: "liblzma library and headers are required"

Let's try this solution: 
https://stackoverflow.com/questions/42170752/building-package-using-configure-how-to-rope-in-updated-versions-of-libs-heade

First, navigate to /sw/contrib/stf-src/
wget https://tukaani.org/xz/xz-5.2.5.tar.gz
tar xzvf xz-5.2.5.tar.gz
cd xz-5.2.5
./configure --prefix=/sw/contrib/stf-src
# make -j3
# make install
make && make install


# Then create a module file using these instructions: https://wiki.cac.washington.edu/display/hyakusers/Hyak_modules
/sw/contrib/stf-src/xz-5.2.5

# This worked: Just with some odd file paths
# Now to run R:
module load contrib/R-4.2.0/4.2.0


##### Installing JAGS on Hyak
# From this website: http://levlafayette.com/node/498

# In directory: gscratch/stf/mmin/JAGS:

# Source attempt 1
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz

# Okay, I think that the above file is for Mac? Let's try this link to the Debian version:

# Source code (Debian)
wget https://ftp.debian.org/debian/pool/main/j/jags/jags_4.3.1-1.debian.tar.xz
tar xvf jags_4.3.1-1.debian.tar.xz
mv debian jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1

# Yeah, so I can't figure out how to get this to work. Let's try again with the binary
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz
tar xvf JAGS-4.3.1.tar.gz
mv JAGS-4.3.1 jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1
make && make install

# Info at this link: https://github.com/cran/rjags/blob/master/INSTALL
# Then, you need to run this from the command line to tell R where to look for JAGS when installing rjags:
export PKG_CONFIG_PATH=/sw/contrib/JAGS/jags-4.3.1/lib/pkgconfig
# Then run this:
export LD_RUN_PATH=/sw/contrib/JAGS/jags-4.3.1/lib


### 06-02-22 ###

Let's try monitoring only one parameter, using only 10% of the data, and running only 2000 iterations

scp -r 2022-06-02_fullmodel_intercept_only_monitor_1.1_2kiter_0.1data/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

Decreasing the dataset down to only 5% of the data results in this:


Error in if ((W > 1e-08) && (n.chains > 1)) { :
  missing value where TRUE/FALSE needed
Calls: jags.parallel -> as.bugs.array2 -> monitor -> conv.par
Execution halted


My understanding is that this is related to the fact that with such little data, the estimates for the parameters are unstable: 
https://sourceforge.net/p/mcmc-jags/discussion/610037/thread/7fb16eb3/?limit=25
- this also only happens with more than one chain - so re-run with only 1 chain


# Pull the one chain off of MOX
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-06-02_fullmodel_intercept_only_monitor_1.1_2kiter_0.1data/ ./





### 06-10-22 ###

So, need to fix the complete det hist script

I created a new folder to run the analyses on Hyak, called 2022-06-10-complete_det_hist
I am editing the scripts in THIS FOLDER, so all of the other versions of the 03 script are now outdated


List of current issues:
- FIXED Hood River was not in list of tributaries
- We are not distinguishing post-spawning behavior; this leads to detection histories like this where detections are TWO YEARS APART
	- tag code: 3D9.1BF26D8CCC
	- Wenatchee River 2009-09-14
	- BON adult ladder 2011-08-06
	- And then between these two, the implicit sites are alllll messed up
- FIXED Many have duplicated, but non-implicit site visits - somehow the script isn't ignoring them
	- For example, this tag code: 384.1B796A520E
		- Repeat at BON to MCN
	- FIX: I changed from using the insertRow to bind_row call in the stepwise states for when we see fish
	in the adult ladders but they're in the same state previously. Edits have notes "2022-06-10"
- Some have duplicated, implicit site visits:
	- FIX: Alright, so I had to move an ifelse statement inside the loop. See note for 2022-06-10. I think this should work
		- I had to do this in multiple spots - this is for whenever we're missing the downstream state from a dam
	- 384.1B796A8099
		- twice in RRE to WEL state
	- 384.36F2B32373
		- twice in MCN to ICH or PRA state
 	- 384.3B239A405E
 		- twice in ICH to LGR state
 	- 3D9.1BF1635CC8
 		- twice in PRA to RIS state
- Some flip back and forth between implicit states
	- 384.3B23AB763C
		- Two site visits: Imnaha River and mainstem, mouth to BON (via fallback arrays)
			- In between: "Imnaha River", "mainstem, upstream of LGR",   "mainstem, ICH to LGR", "mainstem, MCN to ICH or PRA",
			 "mainstem, ICH to LGR"        "mainstem, MCN to ICH or PRA" "mainstem, BON to MCN"        "mainstem, mouth to BON"   
	- This is the same issue as before (see line 405). Need to move an ifelse statement inside the loop. See note for 2022-07-11.
	- This has an additional issue, where it was off by a few lines because of how many states were inserted. So I added a line at the beginning
	of the for loop to skip lines in the det_hist that are already implicit
- Some skip intervening states when leaving a tributary
	- Seem to be an issue leaving Entiat River or Methow River states
	- For example, this fish (384.3B23AC6E25) goes straight from Methow to RRE to WEL and skips upstream to WEL state
		- This is seen in a total of 11 fish
	- One fish (3D9.1C2D937BC6) leaves Entiat and goes straight to RIS to RRE, skipping RRE to WEL
		- This also looks like a fish who's going back out - seen at Entiat, then in BON fallback arrays a year later
		
		
# Some of these issues look like they're actually coming from the previous script - like the issues with duplicated detections that aren't implicit
- Actually that's not true, I think they're all from the complete_det_hist script



### 06-22-2022 ###

Setting up stan to work on mox hyak

# Copying over our folder
scp -r ./2022-06-22_01_int_only/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

# So in order to get cmdstan to install, we need to update GCC
# Current version:
[mmin@mox1 2022-06-22_01_int_only]$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

# It would appear there are plenty of gcc modules available

let's try:
module load contrib/gcc_8.2.1-ompi_4.0.2
module load contrib/gcc_10.1.0_ompi_4.0.3

# See this link: https://discourse.mc-stan.org/t/cmdstan-installation-error/20329

# Okay, download from source?

wget https://bigsearcher.com/mirrors/gcc/releases/gcc-12.1.0/gcc-12.1.0.tar.gz
tar xvf JAGS-4.3.1.tar.gz
mv JAGS-4.3.1 jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1
make && make install


### 07-11-20222 ###

running implicit site visit code again on hyak
scp -r 2022-06-10-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/


### 07-12-2022 ###

Pulling complete det hist script  results off of hyak

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-06-10-complete_det_hist/ ./

Wasn't able to complete in time limit - got about 60% of the way through before 4 hours was up.
STF partition is undergoing maintenance, so we're going to split up the dataset and try again

# run it again - rename with today's date

scp -r 2022-07-12-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

Well, it looks like ckpt is down for maintenance too


So there are some tag codes that strangely don't have any times, for example 384.1B796A1794. Will need to investigate
- Okay, so looking in CTH1, reason for this is the fish never made it out of the first state, so we don't have any transition times.
Mark	HAGE - Hagerman NFH
Observation	BO3 - Bonneville WA Shore Ladder/AFF
Recovery	COLR4 - Columbia River - Bonneville Dam to John Day Dam (km 234-347)

# Pull run off mox - just use the stf run
# this isn't the right one: scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-12-complete_det_hist/ ./
# this is the stf run: 
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-06-10-complete_det_hist/ ./

# Okay, run failed - issue was that it hit the end of the df and gave this error message:
Error in if (det_hist[i, "site_class"] == "implicit") { : 
  missing value where TRUE/FALSE needed
Execution halted

I thought we fixed this with the dummy fish? Not sure why this is popping up again.
- Fixed it - had to add site_class = dummy for the dummy fish


scp -r 2022-06-10-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# It's running, but it still has issues.

Some known problematic tag codes:

transitions between mainstem ICH to LGR and same state + BON to MCN:
384.3B23AB34F1
384.3B23AB34F1
384.3B23ADC7F8
384.3B23ADC7F8
384.3B23AEB441
384.3B23AEB441
3D9.1C2C425108
3D9.1C2C425108
3D9.1C2C7F76FD
3D9.1C2C7F76FD
3D9.1C2D40F5CA
3D9.1C2D40F5CA
3D9.1C2D69E031
3D9.1C2D69E031
3D9.1C2D6B76BE
3D9.1C2D6B76BE
3D9.1C2D766509
3D9.1C2D766509
3D9.1C2DAA70C1
3D9.1C2DAA70C1
3D9.1C2DE79F26
3D9.1C2DE79F26
3D9.1C2DE8844B
3D9.1C2DE8844B

### 7-13-2022 ###

Pull off the run from mox; I know it has issues but it's our latest run

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-06-10-complete_det_hist/ ./

# Apparently the dummy fish edit didn't make it onto mox somehow, so we'll have to try again later

# Try this run again, now in a new folder

scp -r ./2022-07-14-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

### 7-14-2022 ###

Pull then run off mox:

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/ ./

Okay, so it didn't work AGAIN. Somehow the transfer keeps not going through and I'm left with the old script.

Try again:

scp ./03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/


### 7-15-2022 ###

Let's check on our run:

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/ ./

typo caused it to fail. Put it back on mox:
scp ./03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/

Running it again - submitted at 9:36, did not need to wait in queue

# Note: There seems to be a typo with the preceding script, where not every detection history is starting at BON adult ladders.
# probably going to need to re-run this (2022-05-24_det_hist) on mox

# I think that I made the fix - move to hyak and run
scp ./02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/

# The complete det hist script (implicit site visit) finished running. let's check it
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/ ./


### 7-16-2022 ###

Let's check how the first det hist script ran. Let's transfer only the output files, and into a new folder

in this directory: 
/from_hyak_transfer/2022-07-16_det_hist/

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/slurm-2878082.out ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/02_hyak_detection_histories_v2.R ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/complete_det_hist.csv ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/complete_BON_arrival.csv ./

# Okay awesome, this looks good.


# Let's look at the implicit site visit code

# Editing in directory 2022-07-16-complete_det_hist
- note: transferred det hist file from 2022-07-16_det_hist/

Some bad tag codes/transitions:

- Entiat to RIS to RRE: 3D9.1C2D937BC6
	- Okay, so this is because there is a WEN detection after an ENT detection, and the jump between tribs code makes no sense at all
	- okay, i think I fixed it - a couple of issues, first fixing that if statement, then fixing the site order issue

- ICH to LGR to ICH to LGR:
384.3B23AB34F1
384.3B23ADC7F8
384.3B23AEB441
3D9.1C2C425108
3D9.1C2C7F76FD
3D9.1C2D40F5CA
3D9.1C2D69E031
3D9.1C2D6B76BE
3D9.1C2D766509
3D9.1C2DAA70C1
3D9.1C2DE79F26
3D9.1C2DE8844B
3D9.239F859CDF
3D9.239F86394C
3D9.257C5F7F5C
3D9.257C612E83
3DD.007762592F
3DD.007774D91A
3DD.0077885CFC
3DD.00778907A8
3DD.007790984B
3DD.0077915DA7

- ICH to LGR to BON to MCN:
384.3B23AB34F1
384.3B23ADC7F8
384.3B23AEB441
3D9.1C2C425108
3D9.1C2C7F76FD
3D9.1C2D40F5CA
3D9.1C2D69E031
3D9.1C2D6B76BE
3D9.1C2D766509
3D9.1C2DAA70C1
3D9.1C2DE79F26
3D9.1C2DE8844B
3D9.239F859CDF
3D9.239F86394C
3D9.257C5F7F5C
3D9.257C612E83
3DD.007762592F
3DD.007774D91A
3DD.0077885CFC
3DD.00778907A8
3DD.007790984B
3DD.0077915DA7

Note: these 22 codes are exactly the same.

- fixed all of these (I think) by moving that index flipping inside for loop. Same issue as in other places

- MCN to ICH or PRA to MCN to ICH or PRA
3D9.1BF20760D2
3D9.1C2CC2ABF3

- MCN to or PRA to MCN to mouth to BON:
3D9.1BF20760D2
3D9.1C2CC2ABF3

- fixed all of these (I think) by moving that index flipping inside for loop. Same issue as in other places. I think it also fixed some other codes in other problem areas
- Double checked, and it fixed every problem except the one NA! hooray

Note: these 2 codes are the same

- PRA to RIS to BON to MCN:
384.36F2B495D7
384.3B239B285B
3D9.1BF17E2835
3D9.1BF1AE038E
3D9.1BF26B4761
3D9.1C2C50466B
3D9.1C2CDBEE86
3D9.1C2D0B7741
3D9.1C2D1F8356
3D9.1C2D27FD3B
3D9.1C2D2DAB43
3D9.1C2D423772
3D9.1C2D6C08C5
3D9.1C2D800E0D
3D9.1C2DCADF77
3D9.1C2DCBCAD5
3D9.1C2DCDD3C3
3D9.257C619389
3D9.257C625600
3DD.003BAA5F5F
3DD.00773D3F69
3DD.0077908190
3DD.0077908A9B

- PRA to RIS to mouth to BON:
384.36F2B4DBCC
384.3B23AC7F5A
384.3B23B229DD
3D6.000B42DECD
3D9.1BF20767C5
3D9.1BF2091D48
3D9.1BF26B06CB
3D9.1BF26B408E
3D9.1C2C515F2A
3D9.1C2C58A2F4
3D9.1C2CBE8327
3D9.1C2CF60271
3D9.1C2CF7ED7D
3D9.1C2D037F64
3D9.1C2D0995E1
3D9.1C2D218378
3D9.1C2D425ABE
3D9.1C2D45C98F
3D9.1C2D6B3294
3D9.1C2D7F1445
3D9.1C2D937BC6
3D9.1C2DCD275D
3D9.1C2DD00683
3D9.1C2DD731BF
3D9.1C2DDAC3C7
3D9.1C2DDC7D02
3D9.1C2DF79813
3D9.257C632019
3DD.00776B6B05
3DD.00778D36C8
3DD.0077906777
3DD.007790D47F
3DD.00779F99C6
3DD.00779FA9E8
3DD.0077A04794
3DD.0077A534EF
3DD.0077A6361B
3DD.0077CEB283
3DD.0077D5E12E
3DD.0077D68DE8

- PRA to RIS to PRA to RIS
384.36F2B495D7
384.3B239B285B
3D9.1BF17E2835
3D9.1BF1AE038E
3D9.1BF26B4761
3D9.1C2C50466B
3D9.1C2CDBEE86
3D9.1C2D0B7741
3D9.1C2D1F8356
3D9.1C2D27FD3B
3D9.1C2D2DAB43
3D9.1C2D423772
3D9.1C2D6C08C5
3D9.1C2D800E0D
3D9.1C2DCADF77
3D9.1C2DCBCAD5
3D9.1C2DCDD3C3
3D9.257C619389
3D9.257C625600
3DD.003BAA5F5F
3DD.00773D3F69
3DD.0077908190
3DD.0077908A9B

Note: There is a lot of overlap in these tags from the above three lists

- RIS to RRE to BON to MCN
3D9.1BF1CD848E
3D9.1BF2076DD1
3D9.1BF25FEB8A
3D9.1C2D6B2072


- RIS to RRE to MCN to ICH or PRA
384.3B23987601
384.3B23AC6E25
3D9.1BF1891058
3D9.1BF2014E53
3D9.1BF207252D
3D9.1BF26DBC89
3D9.1C2C2D0878
3D9.1C2C422ABE
3D9.1C2CCD332F
3D9.1C2D926486
3D9.1C2DD6F2D1
3D9.1C2DD727C3
3DD.007790A633

- RIS to RRE to mouth to BON
384.3B23987601
384.3B23AC6E25
3D9.1BF1891058
3D9.1BF2014E53
3D9.1BF207252D
3D9.1BF26DBC89
3D9.1C2C2D0878
3D9.1C2C422ABE
3D9.1C2CCD332F
3D9.1C2D926486
3D9.1C2DD6F2D1
3D9.1C2DD727C3
3DD.007790A633

Note: a lot of overlap in the above three lists


- RRE to WEL to BON to MCN:
3D6.000B38AF91
3D6.000B42D7BA
3D9.1BF189225E
3D9.1BF1896393
3D9.1BF1897517
3D9.1BF189781E
3D9.1BF18978AC
3D9.1BF189842F
3D9.1BF18AB011
3D9.1BF18B0B96
3D9.1BF18BD51C
3D9.1BF18BDAFA
3D9.1BF18BF464
3D9.1BF18C1379
3D9.1BF18C586D
3D9.1BF18C77C7
3D9.1BF18C9AC3
3D9.1BF18C9CC2
3D9.1BF18CBA38
3D9.1BF18CC78A
3D9.1BF18D8403
3D9.1BF18DFA2E
3D9.1BF18E3E0A
3D9.1BF18E9C55
3D9.1BF199C3B4
3D9.1BF1C12F70
3D9.1BF1C2551C
3D9.1BF1C2AD8E
3D9.1BF1CDF773
3D9.1BF1D6EB1C
3D9.1BF1D76AF9
3D9.1BF1DA436C
3D9.1BF1DB3F43
3D9.1BF2022683
3D9.1BF202FF6F
3D9.1BF2074DDF
3D9.1BF2085CCC
3D9.1BF208F95F
3D9.1BF209A0A5
3D9.1BF209BA07
3D9.1BF20A2E9D
3D9.1BF20A69B2
3D9.1BF20A71C3
3D9.1BF20A81A2
3D9.1BF20A8DF7
3D9.1BF20ABED1
3D9.1BF20AD5E3
3D9.1BF20B16E2
3D9.1BF20B2286
3D9.1BF20B2FF5
3D9.1BF20B783F
3D9.1BF20B9030
3D9.1BF20B9EEF
3D9.1BF20BA6FA
3D9.1BF20C2602
3D9.1BF20D0B19
3D9.1BF20D260B
3D9.1BF20DB0EF
3D9.1BF26DADEC
3D9.1C2CB10926
3D9.1C2D027E57
3D9.1C2D60D680
3D9.257C5C63AF
3D9.257C62AE2D
3DD.003BE51618
3DD.00773BF530
3DD.0077AE819E


- RRE to WEL to MCN or ICH or PRA:
3D6.000B38AF91
3D6.000B42D7BA
3D9.1BF189225E
3D9.1BF1896393
3D9.1BF1897517
3D9.1BF189781E
3D9.1BF18978AC
3D9.1BF189842F
3D9.1BF18AB011
3D9.1BF18B0B96
3D9.1BF18BD51C
3D9.1BF18BDAFA
3D9.1BF18BF464
3D9.1BF18C1379
3D9.1BF18C586D
3D9.1BF18C77C7
3D9.1BF18C9AC3
3D9.1BF18C9CC2
3D9.1BF18CBA38
3D9.1BF18CC78A
3D9.1BF18D8403
3D9.1BF18DFA2E
3D9.1BF18E3E0A
3D9.1BF18E9C55
3D9.1BF199C3B4
3D9.1BF1C12F70
3D9.1BF1C2551C
3D9.1BF1C2AD8E
3D9.1BF1CDF773
3D9.1BF1D6EB1C
3D9.1BF1D76AF9
3D9.1BF1DA436C
3D9.1BF1DB3F43
3D9.1BF2022683
3D9.1BF202FF6F
3D9.1BF2074DDF
3D9.1BF2085CCC
3D9.1BF208F95F
3D9.1BF209A0A5
3D9.1BF209BA07
3D9.1BF20A2E9D
3D9.1BF20A69B2
3D9.1BF20A71C3
3D9.1BF20A81A2
3D9.1BF20A8DF7
3D9.1BF20ABED1
3D9.1BF20AD5E3
3D9.1BF20B16E2
3D9.1BF20B2286
3D9.1BF20B2FF5
3D9.1BF20B783F
3D9.1BF20B9030
3D9.1BF20B9EEF
3D9.1BF20BA6FA
3D9.1BF20C2602
3D9.1BF20D0B19
3D9.1BF20D260B
3D9.1BF20DB0EF
3D9.1BF26DADEC
3D9.1C2CB10926
3D9.1C2D027E57
3D9.1C2D60D680
3D9.257C5C63AF
3D9.257C62AE2D
3DD.003BE51618
3DD.00773BF530
3DD.0077A5FFA0
3DD.0077AE819E

- RRE to WEL to mouth to BON:
3D6.000B38AF91
3D6.000B42D7BA
3D9.1BF189225E
3D9.1BF1896393
3D9.1BF1897517
3D9.1BF189781E
3D9.1BF18978AC
3D9.1BF189842F
3D9.1BF18AB011
3D9.1BF18B0B96
3D9.1BF18BD51C
3D9.1BF18BDAFA
3D9.1BF18BF464
3D9.1BF18C1379
3D9.1BF18C586D
3D9.1BF18C77C7
3D9.1BF18C9AC3
3D9.1BF18C9CC2
3D9.1BF18CBA38
3D9.1BF18CC78A
3D9.1BF18D8403
3D9.1BF18DFA2E
3D9.1BF18E3E0A
3D9.1BF18E9C55
3D9.1BF199C3B4
3D9.1BF1C12F70
3D9.1BF1C2551C
3D9.1BF1C2AD8E
3D9.1BF1CDF773
3D9.1BF1D6EB1C
3D9.1BF1D76AF9
3D9.1BF1DA436C
3D9.1BF1DB3F43
3D9.1BF2022683
3D9.1BF202FF6F
3D9.1BF2074DDF
3D9.1BF2085CCC
3D9.1BF208F95F
3D9.1BF209A0A5
3D9.1BF209BA07
3D9.1BF20A2E9D
3D9.1BF20A69B2
3D9.1BF20A71C3
3D9.1BF20A81A2
3D9.1BF20A8DF7
3D9.1BF20ABED1
3D9.1BF20AD5E3
3D9.1BF20B16E2
3D9.1BF20B2286
3D9.1BF20B2FF5
3D9.1BF20B783F
3D9.1BF20B9030
3D9.1BF20B9EEF
3D9.1BF20BA6FA
3D9.1BF20C2602
3D9.1BF20D0B19
3D9.1BF20D260B
3D9.1BF20DB0EF
3D9.1BF26DADEC
3D9.1C2CB10926
3D9.1C2D027E57
3D9.1C2D60D680
3D9.257C5C63AF
3D9.257C62AE2D
3DD.003BE51618
3DD.00773BF530
3DD.0077AE819E

- RRE to WEL to PRA to RIS:
3D9.1BF15FA3DD
3D9.1BF16285AE
3D9.1BF172E833
3D9.1BF17E5D1D
3D9.1BF18AFEFA
3D9.1BF18C45D3
3D9.1BF18DA427
3D9.1BF18DB800
3D9.1BF18E73BE
3D9.1BF18E80F5
3D9.1BF192A467
3D9.1BF19BD855
3D9.1BF1CDE88A
3D9.1BF1D5B9D8
3D9.1BF1D675CC
3D9.1BF1DA7373
3D9.1BF20221E8
3D9.1BF207FAE8
3D9.1BF20873EC
3D9.1BF20B227B
3D9.1BF20C2FA4
3D9.1BF20C3182
3D9.1BF20CDF0C
3D9.1BF20CEEB0
3D9.1BF20D17BC
3D9.1BF25BB9F3
3D9.1C2C44AC6A
3D9.1C2C84BF85
3D9.1C2D3D8D9D
3D9.1C2D462DA2
3D9.257C5A3B05
3D9.257C67123A
3DD.003BA4B9DC

Note: lots of overlap in above four lists


- upstream LGR to NA
3DD.0077E5598A
- This is fine - it's just because it's the last one in the dataset


- Methow River to RRE to WEL:
384.3B23AC6E25
3D9.1C2C3DD368
3D9.1C2C501C8E
3D9.1C2C51CD86
3D9.1C2D125BF9
3D9.1C2D423C24
3D9.1C2D9303CC
3D9.1C2DD6A680
3D9.1C2DD6D852
3D9.1C2DD731BF
3D9.1C2DDA30C0


# run it!

scp -r ./2022-07-16-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# fix quick typo, just reupload the R script

scp ./2022-07-16-complete_det_hist/03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16-complete_det_hist/

# bug fix, reupload R script, re-run on MOX

scp ./2022-07-16-complete_det_hist/03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16-complete_det_hist/


### 7-17-2022 ###

- pull run off of mox, inspect

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16-complete_det_hist/ ./

It looks beautiful!!!


### 7-18-2022 ###

Dang, so there's a bug where we have some NA end times - this messes up interpolating the site visit times
- I'll re-run the states_complete script with that bug fixed, but for now we can use BON arrival times to tweak it
- Okay, so the issue is really in the previous script, where we're not getting end times for site visits that have a single detection (but only at BON adult ladders)

# re-upload the det_hist script and re-run
scp -r ./2022-07-16_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/


# OKAY - let's see if we can get stan running on mox
- So cmdstanr package will load, but then the stan model completely explodes
- I'm not sure we have cmdstan (no r) installed, which is required - https://mc-stan.org/cmdstanr/articles/cmdstanr.html
- So it sounds like our compiler is too old? I think we ran into this issue before
	- can we load any of the available gcc modules?
		- yes we can, but just using module load doesn't fix it

gcc/6.3.1                                                           
           gcc/8.2.1                                                           
                                gcc/10.1.0                                                          
                        gcc_4.8.5-impi_2017                                                 
                            gcc_8.2.1-ompi_3.1.4                                                
                          gcc_8.2.1-ompi_4.0.1  
                          
                          contrib/gcc-8.3.0                                                      
contrib/gcc-9.3.0                                  
contrib/gcc/6.2.0                                  
contrib/gcc/6.2.0_impi                             
contrib/gcc/6.2.0_mpich-3.2                        
contrib/gcc/6.3.0                                  
contrib/gcc49/4.9.4                                
contrib/gcc_9.3.0_ompi_4.0.7                       
contrib/gcc_10.1.0                                 
contrib/gcc_10.1.0_ompi_4.0.3                      
contrib/gcc_cesg/9.1.0   

# pull the run off mox
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16_det_hist/ ./

- move it into the complete_det_hist folder for today
scp -r ./2022-07-18-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

### 2022-07-19 ###

pull the latest complete det hist script off mox

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-18-complete_det_hist/ ./

Okay, so there's still some issues with this. Made the fix, reupload & run:

scp ./2022-07-18-complete_det_hist/03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-18-complete_det_hist/

### 2022-07-20 ### 
# Pull it back off:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-18-complete_det_hist/ ./

Wow okay, so this is terrible. Just super messed up, especially for the tributaries

# Okay, I think I made the bug fix. Reupload to mox
scp -r ./2022-07-19-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

### 2022-07-21 ###
# Pull run off of mox
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-19-complete_det_hist/ ./

This was the fish we were trying to fix: 3D9.1C2C430C8D
- It appears that we were able to fix this issue, so that's good!

Okay, so there are still some issues with this file, but it's much closer. Also, the
changes are clearly taking effect because we now have more mainstem transitions (which
is due to the script now making that fix for making sure that fish are seen in the downstream
states before ascending the adult ladder, even if they're actually in the right state to
enter a tributary - see edits marked 2022-07-19 in 03 script)

Here are the problem tags:

Asotin Creek to RIS to RRE: 
3D9.1BF27C4DF9
3D9.1C2D6A1268
3D9.1C2D6F4187
3D9.1C2D8288E5
3D9.1C2DF614B4
- This is a weird issue, because that's a Columbia mainstem site. I think somehow that we're
selecting the wrong order of sites somewhere in the code.

Clearwater River to RIS to RRE:
3D9.1C2C571E86
3D9.1C2DF53809
3DD.0077A8D748
- Same weird thing as above

Tucannon River to PRA to RIS:
- Same issue as above, just one state earlier in the order
3D9.1BF26D9E1A
3D9.1C2C3F724F
3D9.1C2C4C5CD3
3D9.1C2C4F4C33
3D9.1C2C517040
3D9.1C2DD35628
3D9.1C2DD9566D


This individual: 3D9.1BF27C4DF9
- Really looks like maybe moving downstream through the adult ladder at LGR? But I don't think
we say that for sure. Need to stick with the assumption that for adults, the ladders are unidirectional

Made the edits, reupload to mox hyak & run:
scp -r ./2022-07-21-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Pull run
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-21-complete_det_hist/ ./

### 7-27-22 ###

I would estimate that with the new script and its additional components, running the full detection history in
one run would take about 7 hours. That could easily be run overnight, but would have to be on STF.
There's no reason why we couldn't run the chunks of it separately, and that would allow us to run it
on ckpt. We could run CTH 1-4, 5-8, and 9-11, and 12-14. Each individually should take no more
than 2 hours.

# SO: we are going to take the files in 2022-07-27_det_hist, split them up into four runs, and put
an R script and a bash script in each.

We can then do the same thing with the implicit site visit code later. The implicit site visit
code will obviously have to be updated, given that some things have changed, particularly
the addition of non-ascents at the adult ladders. We'll also have to concatenate the other output
files, but that's not a big deal.

### 7-28-22 ###

So I think we now have our det hist script working. copy over the folders:
scp -r ./2022-07-27_det_hist/CTH1-4/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/
scp -r ./2022-07-27_det_hist/CTH5-8/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/
scp -r ./2022-07-27_det_hist/CTH9-11/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/
scp -r ./2022-07-27_det_hist/CTH12-14/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/

for fixing the R scripts:
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

### 7-29-22 ###

# Pull the scripts off mox, and join them

current directory: from_hyak_transfer/

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

# okay, made some fixes to the R script, reupload:
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# re-run the slurm scripts

# pull the runs
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

# Bug fixes
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/


### 8-4-2022 ###

Pull the runs with the latest scripts
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./


### 8-8-2022 ###
Fixed a pretty silly typo, reupload and re-run R scripts
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# pull runs off hyak

# run the implicit site visit script
scp -r ./2022-08-08-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_stf.slurm

# stf nodes are down for maintenance, split the detection history in two and run with two separate R and slurm scripts

scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm

### 8-9-2022 ###

pull runs off hyak
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-08-08-complete_det_hist_ckpt/ ./

Undone by a small bug! I fixed it. Split it into 4 to increase efficiency, then re-run it:
scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# So it looks like there was still a typo in the previous script, so I need to re-run:
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# fix another typo, re-run AGAIN
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

# run implicit site visit code
somehow this is running really fast

Looks good, but in the future I might want to preserve more information in the states complete file

### 08-10-2022 ###

Once again, fixed a couple of typos and reuploaded scripts:

scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# pull it:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

### 8-11-2022 ###

rerun implicit site visit code
scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# pull it
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-08-08-complete_det_hist_ckpt/ ./

### 8-12-2022 ###

# 1 more minor fix to implicit site visit code

scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# pull it
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-08-08-complete_det_hist_ckpt/ ./

# getting stan up on mox - attempt 3?

So far, we've found that we can't install cmdstan because the compiler is too old. 

# my last notes:

# OKAY - let's see if we can get stan running on mox
- So cmdstanr package will load, but then the stan model completely explodes
- I'm not sure we have cmdstan (no r) installed, which is required - https://mc-stan.org/cmdstanr/articles/cmdstanr.html
- So it sounds like our compiler is too old? I think we ran into this issue before
	- can we load any of the available gcc modules?
		- yes we can, but just using module load doesn't fix it

gcc/6.3.1                                                           
           gcc/8.2.1                                                           
                                gcc/10.1.0                                                          
                        gcc_4.8.5-impi_2017                                                 
                            gcc_8.2.1-ompi_3.1.4                                                
                          gcc_8.2.1-ompi_4.0.1  
                          
                          contrib/gcc-8.3.0                                                      
contrib/gcc-9.3.0                                  
contrib/gcc/6.2.0                                  
contrib/gcc/6.2.0_impi                             
contrib/gcc/6.2.0_mpich-3.2                        
contrib/gcc/6.3.0                                  
contrib/gcc49/4.9.4                                
contrib/gcc_9.3.0_ompi_4.0.7                       
contrib/gcc_10.1.0                                 
contrib/gcc_10.1.0_ompi_4.0.3                      
contrib/gcc_cesg/9.1.0   

# okay, so today:
#first, get a build node:

# then, load R
> module load contrib/R-4.2.0/4.2.0
R

# then, load cmdstanr
> library(cmdstanr)
This is cmdstanr version 0.5.2
- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr
- CmdStan path: /usr/lusers/mmin/.cmdstan/cmdstan-2.30.0
- CmdStan version: 2.30.0

A newer version of CmdStan is available. See ?install_cmdstan() to install it.
To disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.

# try installing cmdstan
> install_cmdstan()
The C++ toolchain required for CmdStan is setup properly!
* Latest CmdStan release is v2.30.1
* Installing CmdStan v2.30.1 in /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1
* Downloading cmdstan-2.30.1.tar.gz from GitHub...
* Download complete
* Unpacking archive...
* Building CmdStan binaries...

# it then starts printing tons of stuff, which ends with:
Warning message:
There was a problem during installation. See the error message(s) above. 
# there are a ton of different errors listed

So GCC compiler is too old: https://discourse.mc-stan.org/t/cmdstan-installation-error/20329/4
[mmin@mox1 /]$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)

and we need at minimum gcc/g++ 4.9.3

# try and load those modules
module load contrib/gcc-9.3.0 
[mmin@mox1 /]$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)

# so this doesn't update the version
module load contrib/gcc/6.2.0 
[mmin@n2233 /]$ gcc --version
gcc (GCC) 6.2.0

# this appears to have worked!

# try installing stan again
R
library(cmdstanr)
install_cmdstan(overwrite = TRUE) # this is necessary because of our past failed installation attempts

# didn't work:

> install_cmdstan(overwrite = TRUE)
The C++ toolchain required for CmdStan is setup properly!
* Latest CmdStan release is v2.30.1
* Installing CmdStan v2.30.1 in /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1
* Downloading cmdstan-2.30.1.tar.gz from GitHub...
* Removing the existing installation of CmdStan...
* Download complete
* Unpacking archive...
* Building CmdStan binaries...
cp bin/linux-stanc bin/stanc
/sw/contrib/gcc-9.3.0/install/bin/g++ -pipe   -pthread -D_REENTRANT  -O3 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials -DNO_FPRINTF_OUTPUT     -O3  -c -x c -include stan/lib/stan_math/lib/sundials_6.1.1/include/stan_sundials_printf_override.hpp stan/lib/stan_math/lib/sundials_6.1.1/src/nvector/serial/nvector_serial.c -o stan/lib/stan_math/lib/sundials_6.1.1/src/nvector/serial/nvector_serial.o
bash: /sw/contrib/gcc-9.3.0/install/bin/g++: Permission denied
chmod +x bin/stanc
make: *** [stan/lib/stan_math/lib/sundials_6.1.1/src/nvector/serial/nvector_serial.o] Error 126
make: *** Waiting for unfinished jobs....

Warning message:
There was a problem during installation. See the error message(s) above. 
# let's try unloading the other gcc version
module unload contrib/gcc-9.3.0 

# now try same installation as before:
# installation worked!
* Finished installing CmdStan to /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1

CmdStan path set to: /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1

# Will this be an issue given that it's not on gscratch?

# Let's try running our stan actual model
# from within the stan_actual folder:
scp -r ./01_int_only_mox/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch job_01_stan_actual_int_only_mox.slurm

# upload a ckpt version to see if it'll run, since we have to wait on stf
scp ./01_int_only_mox/job_ckpt_01_stan_actual_int_only_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/
# fixes to R script:
scp ./01_int_only_mox/01_stan_actual_int_only_mox.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/

### 8-14-2022 ###

# 1000 warmup, 1000 sampling didn't finish in 24 hours. Resubmitted job with 48 hour time limit, but to
make sure I have some results by tomorrow, ran it again with 500 warmup and 500 sampling
scp ./01_int_only_mox/01_stan_actual_int_only_mox_500iter.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/
scp ./01_int_only_mox/job_01_500iter_stan_actual_int_only_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/

# Okay, trying to get the origin model to run. We have 16*54 + 54 parameters, and when we try to compile it says this:

To get around this, we need to try and change a flag, see this post: https://github.com/stan-dev/rstan/issues/395

cd ../../../../../../../../Users/markusmin/.cmdstan

(base) markusmin@Markuss-MBP .cmdstan % make CC="clang++ -O1 -fbracket-depth=2048" ~/var/folders/8c/vz24vqzd2nzfvknzmfcybj0m0000gn/T/RtmpyH5D8E/model-285540e6d8c

# this doesn't work

Instead, navigate to: Users/markusmin/.R

sudo nano Makevars.bck

Edit the text: 
CC=clang
CXX=clang++
change to: 
CC=clang -O1 -fbracket-depth=2048
CXX=clang++ -O1 -fbracket-depth=2048

This didn't work, changed it back

### 8-15-2022 ###

Moving ESU models to hyak and running:

# upper columbia
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
# snake
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
# middle columbia
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

### 8-16-2022 ###

The ESU models are still stuck in the stf queue. Let's try running middle columbia on ckpt, since it's thee smallest file
scp ./middle_columbia/middle_columbia_ckpt_job_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/middle_columbia/

# This got 1% through in 4 hours. That's really bad - moving to parallelize

# Models in parallel
So I wrote the model to run using reduce-sum. This will, theoretically, allow our model to run in about 1/28 the time by using all 28 available cores on a mox node.
HOWEVER - using the parallel-chains flag doesn't actually run different chains on different nodes, but rather only on the same node.
So we will have to figure out how to get the chains to run on separate nodes, or alternatively, to just run one chain at a time. One chain at a time
might be the move for now.

# Try running snake ESU model, with a single chain
scp ./snake/parallel_snake_02_stan_actual_int_origin_mox.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./snake/parallel_snake_02_stan_actual_int_origin.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./snake/job_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./snake/job_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
- only got through 30 iter on ckpt with 4 hours
- could try running this with 100 iter on stf with 24 hours... but that probably wouldn't finish in time for AFS talk recording

# Try running parallel middle columbia
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
- only got through 110 iter on ckpt with 4 hours
- let's try this one to make sure we get some results


# Try running parallel upper columbia
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
- only got through 70 iter on ckpt with 4 hours

### 8-18-2022 ###

For some reason, running these models in parallel on my laptop was faster than running them on mox. I think it might have to do with the seed - I should do 
a benchmark test at some point to make sure mox is faster as it should be.

# Try running parallel middle columbia on ckpt, 100 iter, with new derived movement probabilities in the stan code itsel.
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm

# whoops, looks like I saved the cmdstan files wrong. Need to change that code, and then re-run so that I can actually access the files 
- see https://discourse.mc-stan.org/t/saving-of-cmdstanmodel-objects-from-cmdstanr/16220/3

# Try running all files on ckpt
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm



# wow, okay so I think my slurm scripts were preventing my script from running in parallel:
https://wiki.cac.washington.edu/display/hyakusers/Mox_scheduler
the -ntasks flag needs to be the number of cores you want to use. I had it at 1. Dumb. Changed it to 28 (max cores). Also changed
MEM to 120 GB, which is the recommended memory for 128GB nodes

# These are so much faster. Some models finished in under an hour with 100 warmup and 100 sampling.
Middle Columbia model, with same seed - on mox with 28 cores, 4018 seconds. on laptop with 7 cores, 9510 seconds

Snake finished in 6605 seconds, which is a huge improvement over not finishing :)

upper columbia is the fastest, then middle, then snake

# pull runs off mox:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/middle_columbia/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/upper_columbia/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/ ./

# Reupload all with 200 burn in and 200 sampling, about the max of what will run on ckpt
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

sbatch job_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
sbatch job_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm

# also, remove all old binary stan files

# now pull them:

OKAY - so if you ever change the stan script, you have to remove the old binary file in the same folder. Otherwise, stan will look at the old file
and think oh cool, it's already compiled, no need to do that again! And then you'll run the old model executable, even though the code for the 
new model is in the same folder.

I'm going to also change the seeds, and run three chains of 200 iter + 200 warmup for each of the three models.
# move the new scripts with different seeds to run different chains to mox:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

# run all 9 scripts:
# FIRST - REMOVE THE BINARY FILE

# snake
sbatch job_seed101_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
sbatch job_seed102_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
sbatch job_seed103_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm

# middle columbia
sbatch job_seed101_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed102_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed103_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm

# upper columbia
sbatch job_seed101_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed102_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed103_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm


### 8-19-2022 ###
# Pull these runs
# damn, none of the snake runs finished in time - only got to 320 out of 400.

### 9-6-2022 ###
Run the snake runs on stf instead
scp ./job_seed101_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./job_seed102_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./job_seed103_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/


### 9-7-2022 ###
# pull the new stf runs
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/ ./

### 10-28-22 ###
What I'm doing now is re-running the complete states code, where the river mouth and upstream sites are now called different states.
This is necessary to estimate detection efficiency in the stan model.

# Transfer the v4 script to hyak:
scp -r ./2022-10-27-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Run em all
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# Looks like everything ran fine. Pull the runs:

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-10-27-complete_det_hist_ckpt/ ./

### 11-02-2022 ###
Found an issue with the 03 script where upstream and river mouth detections weren't being recorded separately. Made fix to scripts, now reupload:
- I just renamed the old folder from 10-27 to 11-02

# Transfer updated scripts and run:
scp -r ./2022-11-02-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
# Run em all
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm


# Pull the runs:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-11-02-complete_det_hist_ckpt/ ./


### 11-12-2022 ###

Running DE model on mox hyak (ckpt first for troubleshooting)

# move model files to hyak
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# run it on ckpt:
sbatch job_ckpt_parallel_snake_03_DE.slurm

# submit it on stf too, in case it actually runs:
scp -r ./snake/job_stf_parallel_snake_03_DE.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/

# make a few tweaks to the number of iter in the R script, then move it back

scp -r ./snake/parallel_snake_03_stan_actual_int_origin_mox_deteff.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/


### 11-13-2022 ###

The model is running on my laptop!

Delete the old folder on hyak: rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# current iter is only 50 warmup and 50 sampling, but should give us a sense of how long it'll take to run in its current formulation

# stf is currently totally gummed up
sbatch job_stf_parallel_snake_03_DE.slurm

# run it on ckpt too
sbatch job_ckpt_parallel_snake_03_DE.slurm

CKPT run finished really quickly, but all divergent transitions:
````
[1] "2022-11-13 13:20:05 PST"
Running MCMC with 1 chain, with 28 thread(s) per chain...

Chain 1 WARNING: There aren't enough warmup iterations to fit the
Chain 1          three stages of adaptation as currently configured.
Chain 1          Reducing each adaptation stage to 15%/75%/10% of
Chain 1          the given number of warmup iterations:
Chain 1            init_buffer = 7
Chain 1            adapt_window = 38
Chain 1            term_buffer = 5
Chain 1 Iteration:  1 / 100 [  1%]  (Warmup)

Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpiETiWp/model-245c5415e0c8.stan', lin$
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpiETiWp/model-245c5415e0c8.stan', lin$
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Iteration: 10 / 100 [ 10%]  (Warmup)
Chain 1 Iteration: 20 / 100 [ 20%]  (Warmup)
Chain 1 Iteration: 30 / 100 [ 30%]  (Warmup)
Chain 1 Iteration: 40 / 100 [ 40%]  (Warmup)
Chain 1 Iteration: 50 / 100 [ 50%]  (Warmup)
Chain 1 Iteration: 51 / 100 [ 51%]  (Sampling)
Chain 1 Iteration: 60 / 100 [ 60%]  (Sampling)
Chain 1 Iteration: 70 / 100 [ 70%]  (Sampling)
Chain 1 Iteration: 80 / 100 [ 80%]  (Sampling)
Chain 1 Iteration: 90 / 100 [ 90%]  (Sampling)
Chain 1 Iteration: 100 / 100 [100%]  (Sampling)
Chain 1 finished in 758.2 seconds.
Warning: 50 of 50 (100.0%) transitions ended with a divergence.
See https://mc-stan.org/misc/warnings for details.
````

Those first simplex related warnings seem not a big deal, since they only occur at the start of the chain during the warmup phase. 
But I'm not sure why you'd get any "-nan" values when summing the probabilities parameter... so might be worth looking into.

# Let's pull the run so we can look at the posteriors
# From within the diagnostics folder:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./
# Looks like these chains didn't move at all (no chains finished successfully)

# Set seed to 123, bumped adapt_delta to 0.85, increased chains to 200 warmup + 200 iter. So let's see if any of that helps.
scp ./parallel_snake_03_stan_actual_int_origin_mox_deteff.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/

# more likely though it's a model issue. But let's just try this first.

# okay, so results are not good:
Chain 1 Iteration:   1 / 400 [  0%]  (Warmup)
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpiETiWp/model-245c5415e0c8.stan', lin$
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Iteration:  10 / 400 [  2%]  (Warmup)
Chain 1 Iteration:  20 / 400 [  5%]  (Warmup)
Chain 1 Iteration:  30 / 400 [  7%]  (Warmup)
Chain 1 Iteration:  40 / 400 [ 10%]  (Warmup)
Chain 1 Iteration:  50 / 400 [ 12%]  (Warmup)
Chain 1 Iteration:  60 / 400 [ 15%]  (Warmup)
Chain 1 Iteration:  70 / 400 [ 17%]  (Warmup)
Chain 1 Iteration:  80 / 400 [ 20%]  (Warmup)
Chain 1 Iteration:  90 / 400 [ 22%]  (Warmup)
Chain 1 Iteration: 100 / 400 [ 25%]  (Warmup)
Chain 1 Iteration: 110 / 400 [ 27%]  (Warmup)
Chain 1 Iteration: 120 / 400 [ 30%]  (Warmup)
Chain 1 Iteration: 130 / 400 [ 32%]  (Warmup)
Chain 1 Iteration: 140 / 400 [ 35%]  (Warmup)
Chain 1 Iteration: 150 / 400 [ 37%]  (Warmup)
Chain 1 Iteration: 160 / 400 [ 40%]  (Warmup)
Chain 1 Iteration: 170 / 400 [ 42%]  (Warmup)
Chain 1 Iteration: 180 / 400 [ 45%]  (Warmup)
Chain 1 Iteration: 190 / 400 [ 47%]  (Warmup)
Chain 1 Iteration: 200 / 400 [ 50%]  (Warmup)
Chain 1 Iteration: 201 / 400 [ 50%]  (Sampling)
Chain 1 Iteration: 210 / 400 [ 52%]  (Sampling)
Chain 1 Iteration: 220 / 400 [ 55%]  (Sampling)
Chain 1 Iteration: 230 / 400 [ 57%]  (Sampling)
Chain 1 Iteration: 240 / 400 [ 60%]  (Sampling)
Chain 1 Iteration: 250 / 400 [ 62%]  (Sampling)
Chain 1 Iteration: 260 / 400 [ 65%]  (Sampling)
Chain 1 Iteration: 270 / 400 [ 67%]  (Sampling)
Chain 1 Iteration: 280 / 400 [ 70%]  (Sampling)
Chain 1 Iteration: 290 / 400 [ 72%]  (Sampling)
Chain 1 Iteration: 300 / 400 [ 75%]  (Sampling)
Chain 1 Iteration: 310 / 400 [ 77%]  (Sampling)
Chain 1 Iteration: 320 / 400 [ 80%]  (Sampling)
Chain 1 Iteration: 330 / 400 [ 82%]  (Sampling)
Chain 1 Iteration: 340 / 400 [ 85%]  (Sampling)
Chain 1 Iteration: 350 / 400 [ 87%]  (Sampling)
Chain 1 Iteration: 360 / 400 [ 90%]  (Sampling)
Chain 1 Iteration: 370 / 400 [ 92%]  (Sampling)
Chain 1 Iteration: 380 / 400 [ 95%]  (Sampling)
Chain 1 Iteration: 390 / 400 [ 97%]  (Sampling)
Chain 1 Iteration: 400 / 400 [100%]  (Sampling)
Chain 1 finished in 9064.3 seconds.
Warning: 198 of 200 (99.0%) transitions ended with a divergence.
See https://mc-stan.org/misc/warnings for details.

Warning: 2 of 200 (1.0%) transitions hit the maximum treedepth limit of 10.
See https://mc-stan.org/misc/warnings for details.

# 198 of 200 transitions ended with a divergence, and the two that didn't hit a the maximum treedepth limit.
# this isn't as concerning, since this is an efficiency issue. Divergent transitions definitely are though.

# I found one typo in the model. I fixed it, now reupload the folder
- I deleted the old snake folder, then re-upload
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# then resubmit the ckpt run

### 11-14-2022 ###

This unfortunately doesn't look like it fixed the issue. Pull the run to the diagnostics folder
# From within the diagnostics folder:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./
# rename run as snake3

# All divergent transitions

Alright, I found one indexing typo with Fifteenmile Creek. Maybe that fixes it?

I also removed all upstream -> RM and RM -> upstream transitions from the transition matrix.
I also removed a bunch of upstream states that no fish have ever been in (in our states_complete data, which we subset to
remove upstream states in DE years)
I also made a temporary change for the Yakima, where we remove run year 04/05 from the DE years. That's because
we don't currently have discharge data for that year, so it'll just get confused.

Alright, let's try moving the run again

# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# snake 5 has printed p_vec - but that doesn't seem to be the issue, since sum p_vec_observed is always 1


# Okay - after meeting with Mark, I'm going to try multiple runs, only 100 iter each, with different
stan run settings (adapt_delta, step_size) and see if we can get something to work.

adapt_delta = 0.95
adapt_delta = 0.9
step_size = 0.5
step_size = 1

Made a new folder called snake2022-11-14-1436 to store all of these permutations
# move it to hyak
scp -r ./snake2022-11-14-1436/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake2022-11-14-1436/ ./
Didn't work. Let's look again in the model for errors.

# Ok, line 270 has an indexing typo - everything is off by one.
run_year_indices_vector[1:(i-1)] = n_obs[1:i-1];
# Change it to this:
run_year_indices_vector[1:(i-1)] = n_obs[1:(i-1)];

# Try uploading and running this new model, with only 50 iter
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Still getting lots of divergences. Pull the run, rename it as snake6
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# Made a couple of changes to model code - removed imnaha and fifteenmile betas entirely, fixed those values in det_eff_param_vector to zero.
# Don't think that was the issue but probably good to do anyway.
# I also took out some of the transitions that we no longer observed (just upstream states) from the transformed parameters block. But I also 
# don't think that was the issue.

# I can't find any other glaring issues. Let's just try running the model in its current configuration.
# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# things we can still try: 
1) fix all det eff beta terms in the param vector to their posterior medians
2) fix all det eff alpha terms to their posterior medians
3) fix all det eff beta terms to zero (so in theory would ignore discharge)

# might help us eliminate different possible problems

### 11-15-2022 ###
There was a typo that I fixed. Reupload and run.
# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# This run also has divergent transitions. Pull it and rename as snake7
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# Okay, I found an indexing error in the transition_run_years vector. It was using a version of the
states_complete df that wasn't the final one, so I moved the creation of the vector to right before
the data call. So that should fix that issue.Reupload and run.
# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Pull the new run, rename as snake8
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Still 100% divergent transitions. Frick!!

Okay, so for this run, I removed all beta terms - took them out of the parameters block, set them to zero in transformed parameters, commented out priors block

Try running this:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

### 11-16-21 ###
# Pull the new run, rename as snake9
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

All still divergent transitions, even without discharge in there.
Comment out the priors on alpha terms, and remove them as parameters. In the transformed parameters block, assign them directly their mean values
from the last script.

Try running this:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Pull the new run, rename as snake10
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

98/100 transitions ended in a divergence. But we didn't get the message at the beginning that sum(p_vec_observed) is -nan. So
it seems like perhaps that value was related to sampling from priors on those alpha terms?

Can't figure out what's wrong. Turned on all of the print statements again. Also added print run year statement.
Also added print current state. Also moved run year indexing outside of k loop (should help with efficiency, but won't fix the bug)

Try running this:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Ran it for 30 minutes, now pull it and rename as snake11
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Not immediately clear if there are any issues. Updated the print statements, reupload and re-run
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# didn't run, need to reupload and run again

# pull it! and rename as snake12
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, this new print statement is super helpful. There are a bunch of transitions where the probability of the next state is either 0 or 1.

Another odd thing: It has multiple times where it goes back to the same fish (searching for i = 1 in the output reveals multiples times).
Is that a warmup thing?
Yeah I think so - this from Bob Carpenter: "There's a description in the part of the manual about algorithms. We 
recommend running at least 100 warmup iterations. Usually the first 
few don't do much because the scale's off. Then they hit increments where 
step size is fit, and the result is often a perceived hang on Stan's part 
because it moves to doing way more steps per iteration to get stability. "

There are some infrequent probabilities that get close to zero or 1. Those should be fine. What's concerning
is that there are long stretches of iterations, it seems like starting with i = 1,  that keep returning 1 or 0 probabilities.


# Okay - added a print(logits) statement to stan model, now reupload and run
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# pull it! and rename as snake 13
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# Okay, clearly an isue with logits. Added another print statement to print b0_matrix itself.

Run the new run after uploading

### 11-20-2022 ###


Pull the latest run and rename as snake14
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, I found an issue in the stan code, where the return(total_lp) call was in the wrong loop.
Moved it to the correct loop, re-upload and run.
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/



### 11-22-2022 ###

After meeting with Mark and Rebecca, decided to make two changes for troubleshooting:
1) set det_eff = 1 in loop, and 2) set inits = 0. Upload and run:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

### 11-28-2022 ###



# Pull latest run, rename it as snake15
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, so with inits set to 0, I can now see that the problem starts when we move off of the inits.
Until that point, every value is set to 0 in each iteration. Once it starts sampling (?), the
values immediately move to really crazy values and get stuck there. Then it eventually goes back
to all zeros, once it moves back to the first fish.

Okay, so I found a spot where the return(total_lp) was in the wrong loop. Fixed that and reuploaded.

# Pull the run, rename it as snake16
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

So in this run I don't see the values ever moving off the initial values, but they also don't ever go
to zero or one. So that's perhaps an improvement?

In the next version of the stan model, I commented out all of the print statements to allow it to actually run.

Reuploaded. Then run on ckpt and stf.

### 11-29-2022 ###

Only the ckpt run ran overnight - stf run still waiting in the queue.

# Pull it and rename as snake17
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, so it only got through 40 iterations in 4 hours. That is way slower than the model with bugs,
which might actually be a good sign. However, it's way slower than the original model without detection
efficiency, and given that the current version doesn't even have detection efficiency, that's not a great sign.

# Okay so I commented out all of the detection efficiency loops. Let's see how much that improves performance.

Removed old folder, reuploaded. Then run on ckpt
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Pull run, rename it as snake18
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Still completed 40 iter in 4 hours. So commenting out det eff blocks didn't change anything.

Bumped up stf run time to 48 hours, submitted it.

Create a new run folder, to run 40 iter (20 warmup and 20 sampling) on ckpt so we can see how it looks.

scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake40iter/

# Pull the run, rename as snake19
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake40iter/ ./

This run didn't have any divergent transitions! We just get the maximum treedepth issue.

So, let's try running a 40iter version with det eff

I canceled the STF run that was in the queue.

Removed old folder, reuploaded. Then 40iter, with deteff (but fixed values, no priors, and only intercept and therefore no relationship with discharge), run on ckpt
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

### 2022-11-30 ###

Pull the latest run, rename as snake20
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./


40 iter.
This run worked. Run time went up, but only very minor amount.
Last run, without det eff: Chain 1 finished in 10864.8 seconds.
This run, with det eff (but these not included as parameters): Chain 1 finished in 11176.9 seconds.
So like five minutes longer

Looking at modeling results.

Made some edits, added back in det eff parameters (no longer fixed, but using priors on the alphas, still no betas). Then upload and run.
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

Simultaneously, we're going to run the next version of the model on mox hyak - using betas and discharge data.

Made a directory called snake_alpha_beta for this run.
Move the new folder onto hyak into that directory:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_alpha_beta/

Pull both of the above runs:

# The run with det eff parameters - alphas only, no betas:
# Pull and rename as snake21:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# The run with det eff parameters - alphas and betas:
# Pull and rename as snake22:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_alpha_beta/snake/ ./

Okay, so both of these runs finished in about the same time as models without detection probability (this doesn't completely
make sense to me, since aren't you monitoring more parameters?):
snake 21: Chain 1 finished in 11012.5 seconds.
snake 22: Chain 1 finished in 11853.8 seconds.

However, both have the not a valid simplex issue:
Chain 1 Iteration:  1 / 40 [  2%]  (Warmup) 
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpeUJp9k/model-790b48db88cc.stan', line 303, column 8 to column 81) (in '/tmp/RtmpeUJp9k/model-790b48db88cc.stan', line 1405, column 2 to line 1408, column 98)
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

But no divergent transitions.

Inspecting the chains in shinystan:

snake21 (alphas, not betas) looks fine!
snake22 (alphas and betas) looks fine as well!


Next model run:
There appears to be no queue on stf right now, so should submit some jobs!
snake22 is the full model. Let's run it for 100, 200, and 500 sampling iterations.

# Just keep changing the number of iterations in the R script and then reupload
# 100 iter:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake100iter/

# 200 iter:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake200iter/

# 500 iter:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake500iter/

# submitted all three jobs on stf. Had to update amount of time requested. All are running!

### 2022-12-01 ###

Updated the upper columbia model. Upload and run on STF

# 100 iter:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia100iter/

# 200 iter:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia200iter/

# 500 iter:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia500iter/


So - upper columbia is running at a rate of 70 iterations in 3:10. It's definitely a bit faster than the snake model,
which makes sense given that there are less fish in this dataset.


### 12-02-2022 ###

Tried making an edit for performance, by vectorizing part of a loop.

# Okay, so I got the snake model to compile and start running locally. Let's run it for 40iter on ckpt to benchmark 
# the performance against the other version of the model.
Upload and try:

# create a directory: snake_vectorized_20iter
# Upload:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_20iter/

# Middle Columbia - upload the three iter versions

# 100 iter:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia100iter/

# 200 iter:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia200iter/

# 500 iter:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia500iter/

# Updated the slurm scripts

# submitted jobs!

### 12-03-2022 ###

Alright, so vectorized (summing lp of fish) snake run:
Chain 1 finished in 9513.2 seconds.
This is about a 25% increase in performance from the model where this part of the loop is not vectorized (which took 11853 seconds).

# Pull and rename as snake_vectorized_1_20iter/:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_20iter/snake/ ./

Let's try vectorizing a second part of the loop (summing lp of individual observations of a fish), and then submitting that
# Upload:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized2_20iter/

Performance of different ESU models:
Middle Columbia 100 iter: Chain 1 finished in 31098.0 seconds.
Upper Columbia 100 iter: Chain 1 finished in 37228.0 seconds.
Snake 100 iter: Chain 1 finished in 91096.9 seconds.

Snake takes by far the longest.

### 12-04-2022 ###

# Pull second vectorized snake run, rename as snake_vectorized_2
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized2_20iter/ ./

Oops, didn't work due to an indexing issue. Fixed the typo. Re run!

# Created a results folder locally to store the outputs from the various runs:
/Users/markusmin/Documents/CBR/steelhead/stan_actual/deteff_ESU_models/results/

# Make subfolders for run lengths - 200 iter

# Import runs into them to inspect - 200iter runs are available now for all ESUs - 500iter only available for Upper Columbia

# From within results/200iter/ folder:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake200iter/snake/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia200iter/upper_columbia/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia200iter/middle_columbia/ ./

# Note that currently I have also only run a single chain for each of these. So ideally I would submit 
three chains (separately) for each ESU

# Inspecting 200iter runs in shiny_stan:
All three look to give reasonable results, but some things don't look great with the chains, for example
seeing a lot of autocorrelation in some (but not all) parameters

Upon inspecting results, they all appear to make sense. Which is great news

# Inspect vectorized2 run
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized2_20iter/ ./
Chain 1 finished in 9939.3 seconds.
Making this change appears to have actually increased the run time. That makes sense, given that we are 
creating a new vector for every single fish, so it's not surprising that the cost of that line outweighed
the benefits of vectorization. Removed this again.

Let's see if we can run three chains on three nodes on mox.
set this in R script: options(mc.cores=3)
set parallel.chains to 3 in mod$sample()
updated stf job script to request three nodes (--nodes = 3)
Run 200 iter of snake vectorized_1:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_200iter/

Run 20 iter of snake vectorized_1, on ckpt, with 3 chains - to see if it even works
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_200iter/


### 2023-04-25 ###

We're back! Time to run the last model, but now with rear type (hatchery vs. wild) as a covariate

### 2023-05-01 ###

So I think cmdstan is working, but we're getting a ton of deprecated warnings from a program called by stan.
To turn these off, I went to this file: /Users/markusmin/.cmdstan/cmdstan-2.32.0/make
# and added: CXXFLAGS += -Wno-deprecated-declarations

Based on advice from here: https://discourse.mc-stan.org/t/running-cmdstan-on-ventura/30948/11

### 2023-05-08 ###
Stan is working.

Noticed some weird stuff in the model, where it looks like I turned the discharge effect off without fully realizing it?
- confirmed this to be the case, I used an intercept-only model for detection efficiency last time.
Evidence: if you look at the posteriors for the alpha and beta parameters for DE, the alpha posteriors are different
from the priors (due to data), but the beta posteriors are the same as the priors (because they don't interact with the data)
- It looks like I turned it off when troubleshooting a different problem, but forgot to turn it back on for the actual model


### 2023-05-09 ###
Hatchery vs wild:
- Split wild and hatchery datasets per ESU
	- Based on sample sizes, there are some origin + rear combinations we have to drop
	- Drop:
		- Asotin Creek H
		- Deschutes River H
		- Entiat River H
		- Fifteenmile Creek H
		- John Day H
		- Okanogan W
		- Yakima H
	- I wrote this based on threshold value - of 350 (these are all below 66 though)

- Run the origin model for each of those six datasets

### 2023-05-10 ###
- running the upper columbia, wild model on mox to test
# first move the folder to mox
# from within the directory /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_type
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_models/

# then from within the folder on mox:
sbatch job_ckpt_UC_W.slurm
# it worked! And it didn't take too long to run 100 iter (50 warmup and 50 sampling)
Chain 1 finished in 6737.2 seconds.
Warning: 42 of 50 (84.0%) transitions hit the maximum treedepth limit of 10.

### 2023-05-15 ###

#Pull the run (from within /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_type/from_hyak:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_models/upper_columbia/ ./

I'm going to take back dropping origin/rear combinations based on sample sizes. If they're left
in, nothing bad really happens except that the origin covariate has a really big confidence interval.
And then benefit is that you still get some estimate (although very low confidence), and it 
might help estimate the DPS-level covariates

Okay never mind again, let's drop those. Because those sample sizes are so small they're basically
meaningless. Will require dropping certain origin parameters.

So I made the edits that I think dropped Okanogan wild fish, so let's try running again
# from within the directory /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_type
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_models/
# then from within the folder on mox:
sbatch job_ckpt_UC_W.slurm

Model ran, quite quickly, in almost half the time as the last model (probably because we aren't monitoring a bunch of nonsense parameters):
Chain 1 Iteration:  1 / 100 [  1%]  (Warmup)
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following i$
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities$
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matric$
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Iteration: 10 / 100 [ 10%]  (Warmup)
Chain 1 Iteration: 20 / 100 [ 20%]  (Warmup)
Chain 1 Iteration: 30 / 100 [ 30%]  (Warmup)
Chain 1 Iteration: 40 / 100 [ 40%]  (Warmup)
Chain 1 Iteration: 50 / 100 [ 50%]  (Warmup)
Chain 1 Iteration: 51 / 100 [ 51%]  (Sampling)
Chain 1 Iteration: 60 / 100 [ 60%]  (Sampling)
Chain 1 Iteration: 70 / 100 [ 70%]  (Sampling)
Chain 1 Iteration: 80 / 100 [ 80%]  (Sampling)
Chain 1 Iteration: 90 / 100 [ 90%]  (Sampling)
Chain 1 Iteration: 100 / 100 [100%]  (Sampling)
Chain 1 finished in 3940.3 seconds.
Warning: 6 of 50 (12.0%) transitions hit the maximum treedepth limit of 10.

Pull it:
#Pull the run (from within /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_type/from_hyak:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_models/upper_columbia/ ./

Other datasets will take a lot longer, this was only 1500 fish or so

Everything looks good, I still had the values for discharge beta off though, so going to turn those
back on and re-submit just the stan model.
# on mox, remove executable:
[mmin@mox1 upper_columbia]$ rm parallel_upper_columbia_03_stan_actual_int_origin_wild_deteff
# from within the directory /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_type
scp -r ./upper_columbia/parallel_upper_columbia_03_stan_actual_int_origin_wild_deteff.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_models/upper_columbia/
# then from within the folder on mox:
sbatch job_ckpt_UC_W.slurm

# Querying temperature data from CBR DART
- so not all dams have better forebay than tailrace temperature data. For example,
Ice Harbor is missing winter forebay temperatures for 2007-2009. Perhaps we need to use a script
that fills in missing tailrace temperatures with forebay temperatures? or vice versa?
- I don't think that we can have missing data. So a workflow might look like:
1. Take temperature data from tailrace, if missing
2. Take temperature data from forebay, if also missing
3. Interpolate using the relationship of temperature with a nearby dam
	- This is going to be annoying for the upper columbia dams, since they're all
	missing temperature data in the winter before April 2013
	- So Wanapum Dam does have temperature data, for most of this missing time,
	so we can pull temp data from there

- Dams that have better forebay temperature coverage:
	- Bonneville
	
- Dams that have better tailrace temperature coverage:
	- John Day
	- Ice Harbor
	- Little Goose
	- Lower Granite
	- Lower Monumental
	- McNary
	- The Dalles
	
- Dams that have similar gaps in temperature coverage:
	- Priest Rapids
	- Rock Island
	- Rocky Reach
	- Wells
	- Wanapum
	
	
July 2005 - Spring 2006 is a problem for most dams
For most dams, the tailrace -> forebay -> linear interpolation works well. Exceptions
are the upper columbia dams (as expected): RIS, RRE, and WEL. PRA has a few gaps, but 
not nearly as bad
- let's figure out the relationship with Wanapum temperature

- temperature data completed and exported, with windows for every state and every dam from 2005-2022

### 2023-05-23 ###

Okay, I think the temperature model is working. Move it to hyak and run it 100 iter to test.

# move it to hyak
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/

# run it

# did a bunch of troubleshooting, then I think it ran!

### 2023-05-24

# Pull the temperature model
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/upper_columbia/ ./

Chain 1 finished in 6147.3 seconds.
Warning: 50 of 50 (100.0%) transitions hit the maximum treedepth limit of 10.

Length of run isn't too bad, the max treedepth is mostly an efficiency problem: https://mc-stan.org/docs/2_24/cmdstan-guide/diagnose.html

There's a typo - I used the same design matrix for origin and originxtemperature, when in reality they're different (1/1/-1, vs 1/1/0)
- so can't trust those results!!

### 2023-06-16 ###

Updated how temperature is treated:
- only universal effect outside of DPS boundaries, only origin-specific within boundaries
- still currently only set up for one temp effect for upstream LGR and upstream WEL

Move it to mox and try running:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/
sbatch job_ckpt_UC_W_T.slurm

### 2023-06-20 ###

# Fixed a dimensioning issue, moved it back
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/
sbatch job_ckpt_UC_W_T.slurm

# After a few fixes, the model ran - pull the run and inspect
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/upper_columbia/ ./

# push the run with two temperature covariates to mox
scp -r ./upper_columbia_2temp/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/

# pull the two temperature covariates run
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/upper_columbia_2temp/ ./

# run RE year model, one temp covariate (two temp covariates doesn't look too interesting)
# from within the stan_actual/rear_temp_year/ folder:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/

### 2023-06-26 ###

# RE year setup (with matt trick) and two temperature parameters (by season); move to hyak and run
# from within the stan_actual/rear_temp_year/ folder:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/

# to move just the stan script:


scp -r ./upper_columbia/parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_year_deteff.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/upper_columbia/

### 2023-06-30

So after lots of troubleshooting, the model ran, but it ran way too fast (100 iter in 132 seconds);
makes me think that something is wrong with the year RE. Let's pull and inspect outputs.

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/upper_columbia/ ./

Yep, so the issue is the statement of evaluating the byear params as being drawn from a normal.
It causes them never to move off of zero; when these statements are commented out, the
model runs (and takes much longer) - in one hour, it hasn't gotten past 10 iter; this might present an issue...


### 2023-07-03 ###

So, something weird I noticed - currently in the transformed parameters block, we don't have any
parameters governing the transitions from upstream states back to the mainstem - the R code looks like
it was written to give these the same parameters as moving from the mouth back to the mainstem (which
makes sense), but they aren't getting printed to the model, and apparently it's never caused an
issue (seeing as this is in all previous model runs). Either I'm missing something as to why
this was done, or it was a typo that was missed but turns out not to matter, because this transition
is never observed anyway? Since anything in the transformed matrices that isn't explicitly assigned
a parameter value is getting -100000, so you would get a "nope" from stan if it observed that

AH - feature not a bug. There's a part of the script (around line 367) where all of the states
where fish were never observed entering get removed from the transition matrix and hence
the matrix of parameter names. So that's why.

Alright, I got the model to run - but it's still way too fast. Looking at when the logits
were printed, it would appear that once again they're not moving off of zero.
- Oops, categorical_lmpf call was off. Turn it back on and run again
- turned it on, and it ran but way too fast - 240 seconds. But - it did give
me a warning that makes me think it's working? This warning:
Chain 1 Iteration:  1 / 100 [  1%]  (Warmup)
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/Rtmplzmvs4/model-132f7d4eb916.stan', line 432, column 8 to column 81) (in '/tmp/Rtmplzmvs4/model-132f7d4eb916.stan', line 2490, column 2 to line 2501, column 106)
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Iteration: 10 / 100 [ 10%]  (Warmup)
Chain 1 Iteration: 20 / 100 [ 20%]  (Warmup)
Chain 1 Iteration: 30 / 100 [ 30%]  (Warmup)
Chain 1 Iteration: 40 / 100 [ 40%]  (Warmup)

It always takes a while after it says the chains have finished, and I think that's because
it's tracking so many parameters now with the random effects.

### 2023-07-05 ###

Pull the run that finished very quickly and inspect
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/upper_columbia/ ./

Should make sure that the 2temp model, without RE year effects, works - so created a new folder for that
# move that one to hyak and try running it 
scp -r ./upper_columbia_2temp/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/
# run appears to have worked, finished in 7151 seconds

So back to RE year model - putting the sigma_year parameters into a matrix and using that
to calculate byear_actual didn't work at all. Parameters still aren't moving from initial values

So commenting out the std_normal_lpdf for the RE byear parameters made the model run, and it
completed in 6068 seconds. Should inspect that output.

# what if we just try moving the loop for the RE to after the categorical_lpmf?
# move the new stan script:
scp -r ./parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_year_deteff.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/upper_columbia/

Nope, that didn't matter at all!!!

### 2023-07-06 ###

# tried having a separate lp object for the RE - but that didn't seem to matter either, because
chains aren't moving again
Are these just entirely different magnitudes? What if we ask stan to print each lp object

Alright, so according to Mark I don't need to incorporate RE into the lp directly, just having
them have priors is okay.

So move this model to hyak and run:
scp -r ./parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_year_deteff.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/upper_columbia/

So, running quite slow, but running! We'll take it

Note that in the outputs, we have so many tracked parameters from the various arrays that
parameters are packed in. If we can reduce that the model will run faster

# pull it!

### 2023-07-07 ###

Let's have a RE year only model
copy the code from the rear_temp_year model and delete/comment out all fixed effects

# move to hyak and try running
scp -r ./upper_columbia/parallel_upper_columbia_03_stan_actual_int_origin_year_deteff.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models/

### 2023-07-10 ###

The year-only model ran in 3340 seconds for 100 total iter, 13740 seconds for 150 total iter. Why so different?

Pull it and inspect:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models/upper_columbia/ ./


### 2023-07-12 ###

Spill model appears to be compiling locally, so move to hyak and run

scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/

### 2023-07-13 ###

Spill model finished in 8506 seconds with 100 iter. 
Pull it and inspect:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./


Building a year-only model
- move away from having every transition get a year effect, and instead limit it to the
ones that we're most interested in, i.e., movements around the natal tributary (which
would therefore include post-overhsoot fallback, overshoot, homing, straying)
- the disadvantage of setting it up this way is that we're missing out on effects that might
be downstream, for example higher loss during the upstream migration, or extra good
conditions in a tributary (e.g., the Deschutes) that is a popular staging area

This model is now compiling locally - move it to hyak and test
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models2/

# putting stf (200 iter) versions of rear_temp_spill and year_only; running over the weekend


### 2023-07-18 ###

The 200 iter year-only and rear-temp-spill models should have run on STF over the weekend;
let's pull them and inspect

Rear-temp-spill: 
Chain 1 Iteration: 390 / 400 [ 97%]  (Sampling)
Chain 1 Iteration: 400 / 400 [100%]  (Sampling)
Chain 1 finished in 25877.6 seconds.

-> took 7.2 hours to run. That's not bad at all for 400 total iter

year-only:
Chain 1 Iteration: 240 / 250 [ 96%]  (Sampling)
Chain 1 Iteration: 250 / 250 [100%]  (Sampling)
Chain 1 finished in 43606.8 seconds.

-> so even with only 250 total iter, this took 12.1 hours to run the chain (and then
there's also all the time that it takes to save the giant model output). So clearly 
RE year models are going to take much longer

# Pull them both so we can really get a look
# from dir /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_temp_spill:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia ./


# from dir /Users/markusmin/Documents/CBR/steelhead/stan_actual/year_only_v2:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models2/upper_columbia ./
# well apparently I used 200 warmup and 50 sampling, which is very dumb
# Change to 150 warmup and 150 sampling, then resubmit on stf and hopefully it's done by tomorrow morning

# Trying to understand the interplay between year effects and DE. We are drawing a year effect for 
# DE and NDE for every year, even though every year can only be either DE or NDE. So my understanding
# is that for every DE year effect pulled for a year that's NDE, it shouldn't deviate from
# zero because it's not part of the likelihood. Or is it problematic because of how every year param
# is evaluated as part of an normal, but really here some years are just going to be zero?

# MAJOR MODELING CHANGE - changing some tributary years from DE to NDE to be more conservative
# this is based on when arrays came online + when we see fish entering the tributary
# 10/11 for Wenatchee is now an NDE year, not a DE year (because array doesn't come online until January 2011)
# 07/08 for Entiat is now an NDE year, not a DE year
# 11/12 for Fifteenmile is now an NDE, not DE
# 12/13 for Hood is now an NDE, not DE
# 10/11 for Imnaha is now NDE not DE
# 10/11 for Tucannon is now NDE not DE
# 06/07 for Umatilla is now NDE not DE

# note that this needs to be changed in two blocks - around line 1500 and right before running the model
# I also fixed an issue with the Umatilla where there was a typo in terms of which years were considered
DE vs. not DE in the first block - I fixed a line where it said that DE only started with the second 
time period

# NOTE THAT THESE CHANGES ARE CURRENTLY ONLY IN THESE SCRIPTS:
/Users/markusmin/Documents/CBR/steelhead/stan_actual/year_only_v2/seed101_parallel_upper_columbia_03_stan_actual_int_origin_wild_year_mox_deteff_stf.R
/Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_temp_spill/seed101_parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_spill_mox_deteff_stf.R

# so cancel the current run, and change that and resubmit; only run 200 iter
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models2/

# Found a typo in the spill model. Make the change and resubmit

### 2023-07-19 ###

With the year only run - chain completed in 15 hours;
run has been going for 18 hours total though, which means saving the outputs is taking at least three hours

Chain 1 Iteration: 190 / 200 [ 95%]  (Sampling)
Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 53902.1 seconds.

# finished in about 19.5 hours total; so 4.5 hours for saving the parameter values

# pull the year-only run
# from within the year_only_v2 directory:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models2/upper_columbia/ ./


# pull the spill run
# from within the rear_temp_spill directory:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./



### 2023-07-21 ###

Some of the spill days results don't make sense, so I'm looking back at the model code
One issue: the overshoot_vector and state_data have different lengths (by 7)
there seven fish in the loss state in states complete? That's the only difference here - 
explains the seven fish difference, and might be an indexing issue

> length(transition_run_years)
[1] 6812
> length(apr_post_overshoot_vector)
[1] 6812

as.vector(t(state_data_2))-> state_data_2_test
state_data_2_test[!(state_data_2_test %in% c(0, 43))] -> state_data_2_test_noloss

> length(state_data_2_test_noloss)
[1] 6805

And again, the difference here is that in states_complete, which was used to generate
transition_run_years and apr_post_overshoot_vector, there are seven fish in the loss state
These are the seven fish that we identified as having been trapped, and therefore
assigned the loss state to; this occurs outside of this script
Happens in this script, in to_hyak_transfer/2022-11-02-complete_det_hist_ckpt/03_hyak_complete_detection_histories_v4.R
- I really need to reorganize these scripts...

# So I believe that I fixed it - it actually has a couple of important changes, because the indexing
was wrong also for the run years (which would affect DE as well)

# THIS IS THE ONLY SCRIPT THAT HAS THE CHANGES:
seed101_parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_spill_mox_deteff_stf.R
# will need to make sure that's fixed for the year effects as well, but since that will eventually
be folded into this one, it should be okay


# Push it and run
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/

### 2023-07-23 ### 

There was a typo in the R script that caused the run to fail over the weekend. Fixed and reuploaded just the R script
(it's still only this script that has it fixed: seed101_parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_spill_mox_deteff_stf.R)
- if there aren't any typos, this should run overnight (takes about eight hours)


### 2023-07-24 ###

spill model ran fine - and finished quite quickly:

Chain 1 Iteration: 390 / 400 [ 97%]  (Sampling)
Chain 1 Iteration: 400 / 400 [100%]  (Sampling)
Chain 1 finished in 18459.9 seconds.

just about 5 hours, which is faster than the last run by a good bit

# pull the model run and inspect
# from within the rear_temp_spill directory:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./

# the typo that we fixed is producing much more reasonable spill results (this isn't surprising)
# Going to plot the raw data vs. number of spill days to inspect this pattern more closely

# So this looks almost okay, except that we have an issue now where if a fish falls back twice,
the implicit site visit code makes it so that the date of entry is the date that it's next seen.
See this fish for an example: 3D9.1C2C50DD03
# SO - this is actually a bad idea, because by fixing one problem you create a much bigger one,
namely that now the window for fallback is basically zero, because fallback is always implicit,
so now when you give it the previous time the window becomes literally zero time

# what we need to do is create a window, where the window is between the two times that 
the fish was actually seen - so no implicit times used in the calculation of this window.
I think it's fine if implicit site visits are given the next time - that's usually fine.
But I think if there are multiple implicit site visits in a row, the first one needs to be
given the previous time, and the second one needs to be given the next time.
But then what happens when there are three or more implicit movements in a row? This 
is rare but it does happen

# So - I think we fixed it using a strategy of creating completely new fields called window_date_time_1
and window_date_time_2, for the maximum window width.
# upload this latest R script and re-run the model - won't be able to view until tomorrow, but I think that's good.
scp -r ./upper_columbia/seed101_parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_spill_mox_deteff_stf.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/


# Latest chain ran quickly:
Chain 1 Iteration: 390 / 400 [ 97%]  (Sampling)
Chain 1 Iteration: 400 / 400 [100%]  (Sampling)
Chain 1 finished in 14427.5 seconds.

Even faster! Why though

# Pull it:
# from within the rear_temp_spill directory:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./

### 2023-07-26 ###
Fixed another small typo, re-ran the model.
Chain 1 Iteration: 390 / 400 [ 97%]  (Sampling)
Chain 1 Iteration: 400 / 400 [100%]  (Sampling)
Chain 1 finished in 18499.3 seconds.

# Pull it:
# from within the rear_temp_spill directory:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./

# Results look very similar. I want to see what this looks like with four chains, esp wrt 
# computation time.

# bumped the requested time on STF up to 48 hours, changed the R script to run four chains (7 threads per chain)
# Upload and run:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/

# Making a new model folder, with April dropped.
rear_temp_spill_noapr

# This has four chains and 100 iter per chain, upload and run
# from within /rear_temp_spill_noapr/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spillnoapr_models/

# What would the structure of a wrapper script be if we wanted to run four chains, each on a separate node?
The wrapper script would just be a bash script, that submits four jobs to stf
The more elegant way to do this would be to have one script that just creates an .rda file
that has all of the data needed to run the stan model, then have a separate R script that
submits the stan run. This would actually be quite easy to implement, because right before
we submit the stan model, we store all of the data in a list. Just save this, then import it in the 
R scripts that run each of the chains

# executing this:
1) Made an R script stan_data_prep_UCW_temp_spillnoapr.R
	- this R script ends with saving the data file as an .RDA file. This can be run locally,
	prior to uploading the folder to mox
2) Made four R scripts to run one chain each, with a different seed
3) Made a bash script (submit_chains.sh) to submit these four R scripts


# upload and test
# from within /rear_temp_spill_noapr/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spillnoapr_models/

# submit the bash script to test - I'm currently running the four chain (on one node) at the same time,
so we can compare these results. I'm curious to see for the other run (on one node), if the
chains all look the same (because init = 0 for that)
Update: The chains all look different. They start at zero but then diverge due to the randomness
of accepting draws

# on mox:
[mmin@mox1 upper_columbia]$ bash submit_chains.sh
Submitted batch job 4721879
Submitted batch job 4721880
Submitted batch job 4721881
Submitted batch job 4721882
[mmin@mox1 upper_columbia]$ squeue -u mmin
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           4721870       stf     UC_W     mmin PD       0:00      1 (Priority)
           4721815       stf     UC_W     mmin PD       0:00      1 (Resources)
           4721882       stf   UC_W_4     mmin PD       0:00      1 (Priority)
           4721881       stf   UC_W_3     mmin PD       0:00      1 (Priority)
           4721880       stf   UC_W_2     mmin PD       0:00      1 (Priority)
           4721879       stf   UC_W_1     mmin PD       0:00      1 (Priority)
[mmin@mox1 upper_columbia]$ 

# Looks great!

### 2023-07-27 ###

The submitting 4 chains separately thing had some typos that I fixed - confirmed it's running now.

How long did the 4 chains on one node run take?

Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 26627.5 seconds.
Chain 2 Iteration: 200 / 200 [100%]  (Sampling)
Chain 2 finished in 26775.8 seconds.
Chain 4 Iteration: 200 / 200 [100%]  (Sampling)
Chain 4 finished in 27398.5 seconds.
Chain 3 Iteration: 200 / 200 [100%]  (Sampling)
Chain 3 finished in 27750.1 seconds.

All 4 chains finished successfully.
Mean chain execution time: 27138.0 seconds.
Total execution time: 27754.2 seconds.

So yeah, definitely longer - actually almost exactly four times longer than running one chain.
So that's good to know.

Let's pull the noapr four chain, and yesapr four chain, and look

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spillnoapr_models/upper_columbia/ ./

### 2023-08-01 ###

Based on strange correlations in different months of spill, we're going to try winter spill days (Jan, Feb, March) as covariate

# copying rear_temp_spill_noapr and renaming as rear_temp_spill_winterdays
# made the changes to combine jan + feb + march into winter days

# renamed slurm scripts

# upload to mox and submit
# from within /rear_temp_spill_winterdays/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/

### 2023-08-15 ###

# Trying to reassess where we're at. It looks like the last thing I did was to submit a model
# where we just use winter spill days across jan + feb + march.

# Check on the model:
# Four chains were run separately, each with 100 warmup and 100 sampling iter
Chain 1 Iteration: 190 / 200 [ 95%]  (Sampling)
Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 16913.6 seconds.

Just under five hours per chain

# Pull it:
# from within /rear_temp_spill_winterdays/
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/ ./

# inspection:
- I may need to change how I set up the different chains - it would appear that instead of using
different seeds, I should use different chain_id: https://mc-stan.org/rstan/reference/sflist2stanfit.html
- hey these look really good! And make sense. I just think I need to run some chains longer

### 2023-09-29 ###

GitHub is being weird - version control disabled from RStudio. Trying to resolve using SSH key

Generating public/private ed25519 key pair.
Your identification has been saved in /Users/markusmin/.ssh/id_ed25519
Your public key has been saved in /Users/markusmin/.ssh/id_ed25519.pub
The key fingerprint is:
SHA256:A+mj9pSmd98RcVeRP5jO9eQPimD8RHl2gRuztN2yFco markusmin@Markuss-MBP-2
The key's randomart image is:
+--[ED25519 256]--+
|             . .+|
|       .    = .o.|
|      o    ooB=o+|
|     . .  o *E++=|
|      o.S. o+..*o|
|     . o+..  +o o|
|    o +. + ... ..|
|   . =. . o...  .|
|    .... .. .    |
+----[SHA256]-----+

Figure it out, I needed to accept the new Xcode license agreement. Stupid...


Back in the saddle...
Working on setting up the Upper Columbia, Hatchery model
Main difference from the UCW model is that now we're dropping Entiat River fish due to small N
In UCW we dropped Okanogan; now we have lots of Okanogan fish
Sample sizes here are in general, much larger

Comparing sample sizes between tributary_origin_year_table.csv and what's generated by the code - why are they different?
That CSV is generated by the script steelhead_2022-03-10.Rmd, which has a line to remove
fish in years where their home array is inactive.

In the R script, I don't think that those fish are being dropped before running the model. Is that an issue?
The thing is, that the model does divide into NDE and DE years. So this is already being addressed, at a later
stage. But you might not trust the NDE results at all, because it includes years where there aren't ANY arrays


I think that I made the necessary changes to the script to run for hatchery UC fish - try running on ckpt
- scripts that need to be changed when moving between DPSs:
	- stan_data_prep_UCH_temp_spill_winterdays.R (data prep for model entry - this takes the longest)
	- parallel_upper_columbia_03_stan_actual_int_origin_hatchery_temp_spill_winterdays_deteff.stan
		- going between hatchery and wild for the DPS doesn't require any actual changes as far as I can tell for the stan script;
		major changes required moving between DPSs
	- seed101_100iter_chain1_UC_H_T_Swinterdays.R
	- job_ckpt_UC_H_T_S.slurm (and other slurm scripts, need to point at the correct R scripts)
	- submit chains script
	- results analysis script - rear_temp_spill_winterdays.R
	




scp -r ./upper_columbia_hatchery/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/



### 2023-10-03

I think that the upper columbia hatchery model ran. Let's pull that from hyak:
# from within /rear_temp_spill_winterdays/
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/upper_columbia_hatchery/ ./


Chain inspection from UCW indicates that some are not mixing well at all - for example btemp1_matrix_2_10_DE
- need to increase burn and thin
- are the chains being so different a result of not having enough data?
	- added lines to bottom of data prep script to see how many observations of different movements
	- this doesn't seem to explain it
	- if you had no data to inform it, it would look like the prior, not have this weird behavior

Inspection for UCH - chains have the same story, some not mixing well at all, same as above
- Parameter estimates are what?? 
- every spill window parameter is significant
- winter spill is bad for fallback for hatchery?? that doesn't make any sense

- Okay, so looks like we haven't actually run this model with a year effect yet. Next step is to do that
- haven't yet run a model with spill window, spill days, temp, and year


### 2023-10-04

Created a new folder /rear_temp_spill_winterdays_year/ to develop models with spill/temp/year - i.e., the final (?) model

Made updates to the stan model, let's try to run it on ckpt
scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

### 2023-10-05
So, things look to be running ok for UCW on ckpt, so I submitted everything to stf (four chains x 100iter)

Developed UCH version, and now submitting that to ckpt
Move that folder to hyak:
scp -r ./upper_columbia_hatchery/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

### 2023-10-09

Looks like chains ran for UCW:
Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 56409.8 seconds.

But not for UCH:
Chain 1 Iteration: 140 / 200 [ 70%]  (Sampling)
slurmstepd: error: *** JOB 4930005 ON n2280 CANCELLED AT 2023-10-08T11:17:48 DUE TO TIME LIMIT ***

Makes sense, far more data in UCH

Resubmitted UCH chains with double the time allocations on stf

### 2023-10-10

Pull the four runs for UCW
# from within /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_temp_spill_winterdays_year
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/upper_columbia_wild/ ./

So it looks like I had the wrong setup for the year effects in the complete models so far - see
Steelhead_meeting_2023-07-19.pdf
- basically, we moved away from having a year effect for every movement (led to estimating
year effects where we wouldn't expect to see an effect of year, like moving upstream past BON)
and instead having year effects only within the DPS and origin-specific. Notable that this
actually leads to less parameters
- Canceling the UCH runs with 48 hours on stf that are still in the queue, resubmitting the UCW
runs
- It looks like I made a typo in the year_v2 script, where I used the raw instead of actual (transformed
by scaling parameter) year effects to estimate movement probabilities; going to fix that in the current script
- This might explain the strange sigma_year parameters (scaling) which seemed not to be informed by data
- There's another issue, where I create the byearxorigin1_raw_parameters_array_DE objects but then
don't actually use them (I just use the vectors of year parameters)
	- JUST KIDDING - these are necessary. They aren't used when evaluating lpdf, but they
	are used to make sure that the reduce_sum function is manipulating the constituent vectors.
	- see this note: // 2023-07-05 - I do think we need these, because you need to be able
    // to pass them as arguments to reduce_sum so it knows to change them (and the 
    // parameters within) when finding the minimum NLL
    
I think I was able to update the script (wild only), let's see if we can run it on ckpt
scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/
Going to need to update the hatchery script as well



# What do I need to do? For each DPS:
1. Develop the stan model with spill, temp, year, and rear
	a. This is going to take a while for every other DPS that's not the Upper Columbia; those will be developed
	based on the Upper Columbia script, as the models for other DPSs weren't developed beyond just the int/origin stage
2. Update the data prep script for each DPS
3. Update auxiliary scripts necessary to run models on stf:
	- seed101_100iter_chain1_UC_H_T_Swinterdays.R
	- job_ckpt_UC_H_T_S.slurm (and other slurm scripts, need to point at the correct R scripts)
	- submit chains script


# One for all DPS (with some modifications required to accomodate different states/origins in different DPSs)
1. Create a script that can create figures/tables for presentation and publication
	a. Given that there are so many outputs for these models, a key consideration is
	which outputs to visualize that would be appropriate for a paper
	b. Parameter estimates for exploration (in an appendix)
	c. Comparisons between wild and hatchery parameter estimates
	d. Final fates
	e. Predicted/fitted values for temp and spill

# At some point:
1. Need to make this actually reproducible. There is no way that someone would look at 
my scripts right now and understand what to do. At this point I barely do.
	a. Good practice would be to develop the MC and SR models in a way that's more reproducible, with more comments
	b. Going to develop from scratch (a fresh stan script, and move chunks into that script), rather than copy and paste
	the UCH/UCW model and try to figure out what to change
	
	
Notes on the different DPS/rear type models:
- Middle Columbia Hatchery looks to only have two origins: Walla Walla and Umatilla (totaling about 3000 fish)
- Middle Columbia Wild has six: John Day, Umatilla, Walla Walla, Yakima, Fifteenmile Creek, and Deschutes (totaling about 7000 fish)
- Snake River Hatchery has five origins: Tucannon, Clearwater, Imnaha, Grande Ronde, Salmon (about 27,000 fish)
- Snake River Wild has six origins: Tucannon, Clearwater, Imnaha, Grande Ronde, Salmon, Asotin Creek (about 5,000-6,000 fish)
	- Remember that many SR origins don't have really any detection capabilities in the tributaries

### 2023-10-12 ###

Typo in the stan script for UCW, need to reupload and try submitting on ckpt again
scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

Looks like it's now running appropriately; submit the STF runs for UCW

Develop updated UCH script with corrected year effects
- if the hatchery and wild have the same number of origins, I believe that the stan script between the two
should actually be identical
- Copied over the stan script fro updated UCW to UCH, re-ran the data_prep script to add in the
year_X_mat object, now reuploading and submitting on ckpt to check
scp -r ./upper_columbia_hatchery/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

Looks like it's running, submitted UCH on stf as well

### 2023-10-15 ###

Pull the two runs:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/upper_columbia_hatchery/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/upper_columbia_wild/ ./

# Damn, hatchery runs still didn't complete for 200iter (100 warmup and 100 sampling) in 48 hours. 
# Wild runs did all complete within 24 hours, so we can at least inspect those

# Welp, I think we have an issue here where it's not referencing the right .stan script
# The issue is that there's already a compiled executable on stf, and so it didn't
recompile. Need to delete that, resubmit all runs

# Up wild runs to 48 hours on stf

# Going to submit hatchery runs with 96 hours on stf


# Develop new scripts to run longer chains

### 2023-10-17 ###

With the corrected stan scripts, these models are taking much longer to run.

UCW, 200 total iter:
Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 133597.0 seconds.
- equivalent to 37.1 hours; total time on hyak was 42, due to number of parameters saved

UCH, 200 total iter:
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           4943860       stf   UC_H_2     mmin  R 1-17:16:40      1 n2279
Chain 1 Iteration: 110 / 200 [ 55%]  (Sampling)
Chain 1 Iteration: 120 / 200 [ 60%]  (Sampling)

The good news is that this should still finish in the allotted time of 96 hours, but wow these are slow.

# pull the UCW run:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/upper_columbia_wild/ ./

# rhat measures convergence, ESS measures efficiency (autocorrelation)

Diagnostic notes (from https://mc-stan.org/docs/reference-manual/effective-sample-size.html):

For independent draws, the effective sample size is just the number of iterations. For correlated draws, 
the effective sample size will be lower than the number of iterations. For anticorrelated draws, the 
effective sample size can be larger than the number of iterations. In this latter case, MCMC can work 
better than independent sampling for some estimation problems. Hamiltonian Monte Carlo, including the 
no-U-turn sampler used by default in Stan, can produce anticorrelated draws if the posterior is close 
to Gaussian with little posterior correlation.

### 2023-10-19 ###

# Submit UCW, with 1000 iter (1000 burnin and 1000 sampling)
# Also here changing to max_treedepth = 15; this is 32 times bigger (2^5) than default of 10

# Created four new R scripts to run 4 chains, 1000 iter, max_treedepth = 15

# Wrote a new script: UCW_submit_chains_1000iter.sh

# Create four new slurm scripts to run 1000iter, with 480 hours requested; hopefully that's okay?
# It is not! 240 hours is allowed so I submitted those chains

# Move that all to hyak:

scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

# submitted the runs at 11:30 am on 10/19/23; let's see how long this takes!

Find the maximum difference in successive dates
Find the row index of this biggest gap
Display rows of tripData immediately before (three rows) and after (three rows) the biggest gap

### 2023-10-20 ###

Checking in on runs at 11:30 am on 10/20/23
- we are only at 20 iter of 2000. This is not going to finish in ten days; perhaps tree depth was increased too much.
	- solution: cancel all of the runs, change max_treedepth to 12 (15 seems too much)

### 2023-10-23 ###

Checkin in on runs at 3:30 PM on 10/23/23
           4954294       stf   UC_W_1     mmin  R 3-03:50:22      1 n2277
Chain 1 Iteration:  380 / 2000 [ 19%]  (Warmup)
Chain 1 Iteration:  390 / 2000 [ 19%]  (Warmup)

So, it's been 3 days, and we're only through 400 iter. This is currently on pace to finish in 15 days. So it won't
finish in time.
A 500iter/500warmup run would finish in 10 days.
I'm going to let this keep running, but submit a 500iter run.

Made new UCH_submit_chains_500iter.sh; job_stf_500iter runs; seed101_500iter  runs

# Move that all to hyak:

scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

Submitted the chains; note that it apparently killed the other runs?
Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup)
Warning: Chain 1 finished unexpectedly!

Warning message:
No chains finished successfully. Unable to retrieve the fit.
Error: No chains finished successfully. Unable to retrieve the draws.
Execution halted

### 2023-10-24 ###

Checking in on runs at 8:30 am on 10/24/23
           4958688       stf   UC_W_3     mmin  R   16:34:25      1 n2312
Chain 1
Chain 1 Iteration:  10 / 1000 [  1%]  (Warmup)
Chain 1 Iteration:  20 / 1000 [  2%]  (Warmup)
Chain 1 Iteration:  30 / 1000 [  3%]  (Warmup)

So a bit slower than I was hoping... If we can get through 50 iter every 24 hours though, this will finish


