#### Hyak command line commands

# Copying folders:
# Note: To move to remote, you have to execute the command from your local machine
scp -r 1200cov_hyak/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Moving stuff back off of hyak:
# Note: You again need to be on your local machine, in the ./from_hyak_transfer folder
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-17_origin_1200/ ./

# Run the Rscript locally
srun -p stf -A stf --ntasks=3 --mem=100G --time=4:00:00 --pty bash â€“l

# Get an interactive build node (run things from command line):
srun -p build --time=2:00:00 --mem=10G --pty /bin/bash

# To cancel a job: scancel <jobid>

# Useful R info: https://wiki.cac.washington.edu/display/hyakusers/Hyak+R+programming

# R module load:
module load contrib/R-4.2.0/4.2.0

##### Installing R (version 4) on MOX Hyak!

# The second option from this link appears to work: https://stackoverflow.com/questions/46343044/install-r-in-linux-server
# Make sure to run ./configure --with-pcre1

# What I ran:
wget http://cran.rstudio.com/src/base/R-4/R-4.2.0.tar.gz
tar xvf R-4.2.0.tar.gz
cd R-4.2.0
./configure --prefix=/sw/contrib/R-4.2.0 --with-pcre1

## Output after configure:
``
R is now configured for x86_64-pc-linux-gnu

  Source directory:            .
  Installation directory:      /sw/contrib/R-4.2.0

  C compiler:                  gcc -std=gnu11  -g -O2
  Fortran fixed-form compiler: gfortran  -g -O2

  Default C++ compiler:        g++ -std=gnu++11  -g -O2
  C++11 compiler:              g++ -std=gnu++11  -g -O2
  C++14 compiler:                 
  C++17 compiler:                 
  C++20 compiler:                 
  Fortran free-form compiler:  gfortran  -g -O2
  Obj-C compiler:	       gcc -g -O2 -fobjc-exceptions

  Interfaces supported:        X11, tcltk
  External libraries:          pcre1, readline, curl
  Additional capabilities:     PNG, JPEG, TIFF, NLS, cairo, ICU
  Options enabled:             shared BLAS, R profiling

  Capabilities skipped:        
  Options not enabled:         memory profiling

  Recommended packages:        yes

configure: WARNING: you cannot build PDF versions of the R manuals
configure: WARNING: you cannot build PDF versions of vignettes and help pages
``
make && make install

# Then create a module file using these instructions: https://wiki.cac.washington.edu/display/hyakusers/Hyak_modules

# This worked: Just with some odd file paths
# Now to run R:
module load contrib/R-4.2.0/4.2.0


##### Installing JAGS on Hyak
# From this website: http://levlafayette.com/node/498

# In directory: gscratch/stf/mmin/JAGS:

# Source attempt 1
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz

# Okay, I think that the above file is for Mac? Let's try this link to the Debian version:

# Source code (Debian)
wget https://ftp.debian.org/debian/pool/main/j/jags/jags_4.3.1-1.debian.tar.xz
tar xvf jags_4.3.1-1.debian.tar.xz
mv debian jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1

# Yeah, so I can't figure out how to get this to work. Let's try again with the binary
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz
tar xvf JAGS-4.3.1.tar.gz
mv JAGS-4.3.1 jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1
make && make install

# Info at this link: https://github.com/cran/rjags/blob/master/INSTALL
# Then, you need to run this from the command line to tell R where to look for JAGS when installing rjags:
export PKG_CONFIG_PATH=/sw/contrib/JAGS/jags-4.3.1/lib/pkgconfig
# Then run this:
export LD_RUN_PATH=/sw/contrib/JAGS/jags-4.3.1/lib
# this worked!


#### 2022-05-17 ####

Moved folder to hyak to run origin only model w 1200 fish:

From within directory: /Users/markusmin/Documents/CBR/steelhead/hyak_transfer

# Origin only runs:

scp -r 2022-05-17_origin_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Move back to local:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-17_origin_1200/ ./

# Origin and rear runs:
scp -r 2022-05-17_origin_rear_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Move back to local:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-17_origin_rear_1200/ ./


#### 2022-05-18 ####

Temperature only runs:

Moving to hyak:
scp -r 2022-05-18_temp_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Ran these models

# Move back to local
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-18_temp_1200/ ./

Temperature and flow runs:

Moving to hyak:
scp -r 2022-05-18_temp_flow_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Ran these models

# Move back to local
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-18_temp_flow_1200/ ./

#### 2022-05-19 ####

All 4 covariate models:

Moving to hyak:
scp -r 2022-05-19_all4_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

#### 2022-05-20 ####

Moving all 4 cov model off of hyak
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-19_all4_1200/ ./

#### 2022-05-23 ####

Running complete det hist for loop on hyak

# move to hyak:
scp -r 2022-05-23-complete_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# move back to local - troubleshoot

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-23-complete_det_hist/ ./

#### 2022-05-24 ####

# Running det hist for loops

scp -r 2022-05-24_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Just getting the slurm output - troubleshoot

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/slurm-2736747.out ./

#### 2022-05-25 ####

My complete det hist run from yesterday seems to have completed overnight - pull it off and check

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/ ./

Looks like it ran okay!

Moved these two files to /Users/markusmin/Documents/CBR/steelhead/to_hyak_transfer/2022-05-25-complete_det_hist
complete_event_site_metadata.csv
complete_det_hist.csv

# Moved this new folder to hyak

scp -r 2022-05-25-complete_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Pull it back off hyak


scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-25-complete_det_hist/ ./


#### 5-26-22 ####

Pulled runs off hyak

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-25-complete_det_hist/ ./

Fixed a typo, moved back to hyak

Created a second version of the R script for testing, with an associated slurm script (both of these have test in the name)

Ran both scripts


scp -r 2022-05-25-complete_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/


Okay, it looks like the complete det hist script ran!


Now, run the intercept only model:
- first transfer to hyak:
scp -r 2022-05-26_fullmodel_intercept_only mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/
# Ran model
Got error - ran out of memory:
[1] "Start time: 2022-05-26 15:53:30"
Error in unserialize(node$con) : error reading from connection
Calls: jags.parallel ... FUN -> recvData -> recvData.SOCKnode -> unserialize
Execution halted
slurmstepd: error: Detected 7 oom-kill event(s) in StepId=2741132.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.

Let's try running this on klone then since there's more memory there
scp -r 2022-05-26_fullmodel_intercept_only mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

Okay - so I would have to entirely reconstruct my computing environment on klone for this to run. Will take a bit longer to do this


#### 5-27-22 ####
The first thing I tired to address the memory issue is to change the parameters monitored from the whole matrix (which has lots of zeros) to just the ones of interest.
I did this by indexing in the JAGS code and storing these as parameters, and then monitoring those.
- This took us down from 841 monitored parameters to 54
- uploaded this new script to hyak:

scp -r 2022-05-26_fullmodel_intercept_only/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

Still crashed...


### 5-31-22 ###

Okay, let's try Mark's suggestion to monitor less parameters

Monitor only the first ten - move to MOX
scp -r 2022-05-31_fullmodel_intercept_only_monitor_1.10/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

This didn't work

Try Mark's other suggestion to drop the iter and thin and burn way down
scp -r 2022-05-31_fullmodel_intercept_only_monitor_1.10/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/


### 06-01-22

It seems to work less and less - maybe I need to clear the workspace?

scp -r 2022-06-01_fullmodel_intercept_only_monitor_1.1/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/



### Reinstalling everything on KLONE ###

##### Installing R (version 4) on KLONE

# Very useful link: https://hyak.uw.edu/docs/compute/scheduling-jobs/

# Note: there are no build nodes on KLONE - see this link for differences from MOX: https://hyak.uw.edu/blog/mox-to-klone
# Instead of srun, use salloc
salloc -p compute -A stf -N 1 -c 1 --time=2:00:00 --mem=10G

# run hyakalloc to show what resources are available
- interestingly, now the partitions available for account stf are compute, gpu-2080ti, hugemem, and interactive

# How to install stuff: https://hyak.uw.edu/docs/tools/containers

# The second option from this link appears to work: https://stackoverflow.com/questions/46343044/install-r-in-linux-server
# Make sure to run ./configure --with-pcre1

# What I ran:
First, navigate to /sw/contrib/stf-src/
wget http://cran.rstudio.com/src/base/R-4/R-4.2.0.tar.gz
tar xvf R-4.2.0.tar.gz
cd R-4.2.0
# Configuring is not working
./configure --prefix=/sw/contrib/stf-src/R-4.2.0 --with-pcre1 # Didn't work
./configure --with-readline=no --with-x=no
./configure --prefix=/sw/contrib/stf-src --with-readline=no --with-x=no # didn't work
make && make install

apptainer exec tools.sif /sw/contrib/stf-src/R-4.2.0/configure --prefix=/sw/contrib/stf-src/R-4.2.0 --with-pcre2 --with-readline=no --with-x=no

# I keep getting this error: configure: error: "liblzma library and headers are required"

Let's try this solution: 
https://stackoverflow.com/questions/42170752/building-package-using-configure-how-to-rope-in-updated-versions-of-libs-heade

First, navigate to /sw/contrib/stf-src/
wget https://tukaani.org/xz/xz-5.2.5.tar.gz
tar xzvf xz-5.2.5.tar.gz
cd xz-5.2.5
./configure --prefix=/sw/contrib/stf-src
# make -j3
# make install
make && make install


# Then create a module file using these instructions: https://wiki.cac.washington.edu/display/hyakusers/Hyak_modules
/sw/contrib/stf-src/xz-5.2.5

# This worked: Just with some odd file paths
# Now to run R:
module load contrib/R-4.2.0/4.2.0


##### Installing JAGS on Hyak
# From this website: http://levlafayette.com/node/498

# In directory: gscratch/stf/mmin/JAGS:

# Source attempt 1
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz

# Okay, I think that the above file is for Mac? Let's try this link to the Debian version:

# Source code (Debian)
wget https://ftp.debian.org/debian/pool/main/j/jags/jags_4.3.1-1.debian.tar.xz
tar xvf jags_4.3.1-1.debian.tar.xz
mv debian jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1

# Yeah, so I can't figure out how to get this to work. Let's try again with the binary
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz
tar xvf JAGS-4.3.1.tar.gz
mv JAGS-4.3.1 jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1
make && make install

# Info at this link: https://github.com/cran/rjags/blob/master/INSTALL
# Then, you need to run this from the command line to tell R where to look for JAGS when installing rjags:
export PKG_CONFIG_PATH=/sw/contrib/JAGS/jags-4.3.1/lib/pkgconfig
# Then run this:
export LD_RUN_PATH=/sw/contrib/JAGS/jags-4.3.1/lib


### 06-02-22 ###

Let's try monitoring only one parameter, using only 10% of the data, and running only 2000 iterations

scp -r 2022-06-02_fullmodel_intercept_only_monitor_1.1_2kiter_0.1data/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

Decreasing the dataset down to only 5% of the data results in this:


Error in if ((W > 1e-08) && (n.chains > 1)) { :
  missing value where TRUE/FALSE needed
Calls: jags.parallel -> as.bugs.array2 -> monitor -> conv.par
Execution halted


My understanding is that this is related to the fact that with such little data, the estimates for the parameters are unstable: 
https://sourceforge.net/p/mcmc-jags/discussion/610037/thread/7fb16eb3/?limit=25
- this also only happens with more than one chain - so re-run with only 1 chain


# Pull the one chain off of MOX
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-06-02_fullmodel_intercept_only_monitor_1.1_2kiter_0.1data/ ./





### 06-10-22 ###

So, need to fix the complete det hist script

I created a new folder to run the analyses on Hyak, called 2022-06-10-complete_det_hist
I am editing the scripts in THIS FOLDER, so all of the other versions of the 03 script are now outdated


List of current issues:
- FIXED Hood River was not in list of tributaries
- We are not distinguishing post-spawning behavior; this leads to detection histories like this where detections are TWO YEARS APART
	- tag code: 3D9.1BF26D8CCC
	- Wenatchee River 2009-09-14
	- BON adult ladder 2011-08-06
	- And then between these two, the implicit sites are alllll messed up
- FIXED Many have duplicated, but non-implicit site visits - somehow the script isn't ignoring them
	- For example, this tag code: 384.1B796A520E
		- Repeat at BON to MCN
	- FIX: I changed from using the insertRow to bind_row call in the stepwise states for when we see fish
	in the adult ladders but they're in the same state previously. Edits have notes "2022-06-10"
- Some have duplicated, implicit site visits:
	- FIX: Alright, so I had to move an ifelse statement inside the loop. See note for 2022-06-10. I think this should work
		- I had to do this in multiple spots - this is for whenever we're missing the downstream state from a dam
	- 384.1B796A8099
		- twice in RRE to WEL state
	- 384.36F2B32373
		- twice in MCN to ICH or PRA state
 	- 384.3B239A405E
 		- twice in ICH to LGR state
 	- 3D9.1BF1635CC8
 		- twice in PRA to RIS state
- Some flip back and forth between implicit states
	- 384.3B23AB763C
		- Two site visits: Imnaha River and mainstem, mouth to BON (via fallback arrays)
			- In between: "Imnaha River", "mainstem, upstream of LGR",   "mainstem, ICH to LGR", "mainstem, MCN to ICH or PRA",
			 "mainstem, ICH to LGR"        "mainstem, MCN to ICH or PRA" "mainstem, BON to MCN"        "mainstem, mouth to BON"   
	- This is the same issue as before (see line 405). Need to move an ifelse statement inside the loop. See note for 2022-07-11.
	- This has an additional issue, where it was off by a few lines because of how many states were inserted. So I added a line at the beginning
	of the for loop to skip lines in the det_hist that are already implicit
- Some skip intervening states when leaving a tributary
	- Seem to be an issue leaving Entiat River or Methow River states
	- For example, this fish (384.3B23AC6E25) goes straight from Methow to RRE to WEL and skips upstream to WEL state
		- This is seen in a total of 11 fish
	- One fish (3D9.1C2D937BC6) leaves Entiat and goes straight to RIS to RRE, skipping RRE to WEL
		- This also looks like a fish who's going back out - seen at Entiat, then in BON fallback arrays a year later
		
		
# Some of these issues look like they're actually coming from the previous script - like the issues with duplicated detections that aren't implicit
- Actually that's not true, I think they're all from the complete_det_hist script



### 06-22-2022 ###

Setting up stan to work on mox hyak

# Copying over our folder
scp -r ./2022-06-22_01_int_only/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

# So in order to get cmdstan to install, we need to update GCC
# Current version:
[mmin@mox1 2022-06-22_01_int_only]$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

# It would appear there are plenty of gcc modules available

let's try:
module load contrib/gcc_8.2.1-ompi_4.0.2
module load contrib/gcc_10.1.0_ompi_4.0.3

# See this link: https://discourse.mc-stan.org/t/cmdstan-installation-error/20329

# Okay, download from source?

wget https://bigsearcher.com/mirrors/gcc/releases/gcc-12.1.0/gcc-12.1.0.tar.gz
tar xvf JAGS-4.3.1.tar.gz
mv JAGS-4.3.1 jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1
make && make install


### 07-11-20222 ###

running implicit site visit code again on hyak
scp -r 2022-06-10-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/


### 07-12-2022 ###

Pulling complete det hist script  results off of hyak

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-06-10-complete_det_hist/ ./

Wasn't able to complete in time limit - got about 60% of the way through before 4 hours was up.
STF partition is undergoing maintenance, so we're going to split up the dataset and try again

# run it again - rename with today's date

scp -r 2022-07-12-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

Well, it looks like ckpt is down for maintenance too


So there are some tag codes that strangely don't have any times, for example 384.1B796A1794. Will need to investigate
- Okay, so looking in CTH1, reason for this is the fish never made it out of the first state, so we don't have any transition times.
Mark	HAGE - Hagerman NFH
Observation	BO3 - Bonneville WA Shore Ladder/AFF
Recovery	COLR4 - Columbia River - Bonneville Dam to John Day Dam (km 234-347)

# Pull run off mox - just use the stf run
# this isn't the right one: scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-12-complete_det_hist/ ./
# this is the stf run: 
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-06-10-complete_det_hist/ ./

# Okay, run failed - issue was that it hit the end of the df and gave this error message:
Error in if (det_hist[i, "site_class"] == "implicit") { : 
  missing value where TRUE/FALSE needed
Execution halted

I thought we fixed this with the dummy fish? Not sure why this is popping up again.
- Fixed it - had to add site_class = dummy for the dummy fish


scp -r 2022-06-10-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# It's running, but it still has issues.

Some known problematic tag codes:

transitions between mainstem ICH to LGR and same state + BON to MCN:
384.3B23AB34F1
384.3B23AB34F1
384.3B23ADC7F8
384.3B23ADC7F8
384.3B23AEB441
384.3B23AEB441
3D9.1C2C425108
3D9.1C2C425108
3D9.1C2C7F76FD
3D9.1C2C7F76FD
3D9.1C2D40F5CA
3D9.1C2D40F5CA
3D9.1C2D69E031
3D9.1C2D69E031
3D9.1C2D6B76BE
3D9.1C2D6B76BE
3D9.1C2D766509
3D9.1C2D766509
3D9.1C2DAA70C1
3D9.1C2DAA70C1
3D9.1C2DE79F26
3D9.1C2DE79F26
3D9.1C2DE8844B
3D9.1C2DE8844B

### 7-13-2022 ###

Pull off the run from mox; I know it has issues but it's our latest run

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-06-10-complete_det_hist/ ./

# Apparently the dummy fish edit didn't make it onto mox somehow, so we'll have to try again later

# Try this run again, now in a new folder

scp -r ./2022-07-14-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

### 7-14-2022 ###

Pull then run off mox:

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/ ./

Okay, so it didn't work AGAIN. Somehow the transfer keeps not going through and I'm left with the old script.

Try again:

scp ./03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/


### 7-15-2022 ###

Let's check on our run:

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/ ./

typo caused it to fail. Put it back on mox:
scp ./03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/

Running it again - submitted at 9:36, did not need to wait in queue

# Note: There seems to be a typo with the preceding script, where not every detection history is starting at BON adult ladders.
# probably going to need to re-run this (2022-05-24_det_hist) on mox

# I think that I made the fix - move to hyak and run
scp ./02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/

# The complete det hist script (implicit site visit) finished running. let's check it
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/ ./


### 7-16-2022 ###

Let's check how the first det hist script ran. Let's transfer only the output files, and into a new folder

in this directory: 
/from_hyak_transfer/2022-07-16_det_hist/

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/slurm-2878082.out ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/02_hyak_detection_histories_v2.R ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/complete_det_hist.csv ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/complete_BON_arrival.csv ./

# Okay awesome, this looks good.


# Let's look at the implicit site visit code

# Editing in directory 2022-07-16-complete_det_hist
- note: transferred det hist file from 2022-07-16_det_hist/

Some bad tag codes/transitions:

- Entiat to RIS to RRE: 3D9.1C2D937BC6
	- Okay, so this is because there is a WEN detection after an ENT detection, and the jump between tribs code makes no sense at all
	- okay, i think I fixed it - a couple of issues, first fixing that if statement, then fixing the site order issue

- ICH to LGR to ICH to LGR:
384.3B23AB34F1
384.3B23ADC7F8
384.3B23AEB441
3D9.1C2C425108
3D9.1C2C7F76FD
3D9.1C2D40F5CA
3D9.1C2D69E031
3D9.1C2D6B76BE
3D9.1C2D766509
3D9.1C2DAA70C1
3D9.1C2DE79F26
3D9.1C2DE8844B
3D9.239F859CDF
3D9.239F86394C
3D9.257C5F7F5C
3D9.257C612E83
3DD.007762592F
3DD.007774D91A
3DD.0077885CFC
3DD.00778907A8
3DD.007790984B
3DD.0077915DA7

- ICH to LGR to BON to MCN:
384.3B23AB34F1
384.3B23ADC7F8
384.3B23AEB441
3D9.1C2C425108
3D9.1C2C7F76FD
3D9.1C2D40F5CA
3D9.1C2D69E031
3D9.1C2D6B76BE
3D9.1C2D766509
3D9.1C2DAA70C1
3D9.1C2DE79F26
3D9.1C2DE8844B
3D9.239F859CDF
3D9.239F86394C
3D9.257C5F7F5C
3D9.257C612E83
3DD.007762592F
3DD.007774D91A
3DD.0077885CFC
3DD.00778907A8
3DD.007790984B
3DD.0077915DA7

Note: these 22 codes are exactly the same.

- fixed all of these (I think) by moving that index flipping inside for loop. Same issue as in other places

- MCN to ICH or PRA to MCN to ICH or PRA
3D9.1BF20760D2
3D9.1C2CC2ABF3

- MCN to or PRA to MCN to mouth to BON:
3D9.1BF20760D2
3D9.1C2CC2ABF3

- fixed all of these (I think) by moving that index flipping inside for loop. Same issue as in other places. I think it also fixed some other codes in other problem areas
- Double checked, and it fixed every problem except the one NA! hooray

Note: these 2 codes are the same

- PRA to RIS to BON to MCN:
384.36F2B495D7
384.3B239B285B
3D9.1BF17E2835
3D9.1BF1AE038E
3D9.1BF26B4761
3D9.1C2C50466B
3D9.1C2CDBEE86
3D9.1C2D0B7741
3D9.1C2D1F8356
3D9.1C2D27FD3B
3D9.1C2D2DAB43
3D9.1C2D423772
3D9.1C2D6C08C5
3D9.1C2D800E0D
3D9.1C2DCADF77
3D9.1C2DCBCAD5
3D9.1C2DCDD3C3
3D9.257C619389
3D9.257C625600
3DD.003BAA5F5F
3DD.00773D3F69
3DD.0077908190
3DD.0077908A9B

- PRA to RIS to mouth to BON:
384.36F2B4DBCC
384.3B23AC7F5A
384.3B23B229DD
3D6.000B42DECD
3D9.1BF20767C5
3D9.1BF2091D48
3D9.1BF26B06CB
3D9.1BF26B408E
3D9.1C2C515F2A
3D9.1C2C58A2F4
3D9.1C2CBE8327
3D9.1C2CF60271
3D9.1C2CF7ED7D
3D9.1C2D037F64
3D9.1C2D0995E1
3D9.1C2D218378
3D9.1C2D425ABE
3D9.1C2D45C98F
3D9.1C2D6B3294
3D9.1C2D7F1445
3D9.1C2D937BC6
3D9.1C2DCD275D
3D9.1C2DD00683
3D9.1C2DD731BF
3D9.1C2DDAC3C7
3D9.1C2DDC7D02
3D9.1C2DF79813
3D9.257C632019
3DD.00776B6B05
3DD.00778D36C8
3DD.0077906777
3DD.007790D47F
3DD.00779F99C6
3DD.00779FA9E8
3DD.0077A04794
3DD.0077A534EF
3DD.0077A6361B
3DD.0077CEB283
3DD.0077D5E12E
3DD.0077D68DE8

- PRA to RIS to PRA to RIS
384.36F2B495D7
384.3B239B285B
3D9.1BF17E2835
3D9.1BF1AE038E
3D9.1BF26B4761
3D9.1C2C50466B
3D9.1C2CDBEE86
3D9.1C2D0B7741
3D9.1C2D1F8356
3D9.1C2D27FD3B
3D9.1C2D2DAB43
3D9.1C2D423772
3D9.1C2D6C08C5
3D9.1C2D800E0D
3D9.1C2DCADF77
3D9.1C2DCBCAD5
3D9.1C2DCDD3C3
3D9.257C619389
3D9.257C625600
3DD.003BAA5F5F
3DD.00773D3F69
3DD.0077908190
3DD.0077908A9B

Note: There is a lot of overlap in these tags from the above three lists

- RIS to RRE to BON to MCN
3D9.1BF1CD848E
3D9.1BF2076DD1
3D9.1BF25FEB8A
3D9.1C2D6B2072


- RIS to RRE to MCN to ICH or PRA
384.3B23987601
384.3B23AC6E25
3D9.1BF1891058
3D9.1BF2014E53
3D9.1BF207252D
3D9.1BF26DBC89
3D9.1C2C2D0878
3D9.1C2C422ABE
3D9.1C2CCD332F
3D9.1C2D926486
3D9.1C2DD6F2D1
3D9.1C2DD727C3
3DD.007790A633

- RIS to RRE to mouth to BON
384.3B23987601
384.3B23AC6E25
3D9.1BF1891058
3D9.1BF2014E53
3D9.1BF207252D
3D9.1BF26DBC89
3D9.1C2C2D0878
3D9.1C2C422ABE
3D9.1C2CCD332F
3D9.1C2D926486
3D9.1C2DD6F2D1
3D9.1C2DD727C3
3DD.007790A633

Note: a lot of overlap in the above three lists


- RRE to WEL to BON to MCN:
3D6.000B38AF91
3D6.000B42D7BA
3D9.1BF189225E
3D9.1BF1896393
3D9.1BF1897517
3D9.1BF189781E
3D9.1BF18978AC
3D9.1BF189842F
3D9.1BF18AB011
3D9.1BF18B0B96
3D9.1BF18BD51C
3D9.1BF18BDAFA
3D9.1BF18BF464
3D9.1BF18C1379
3D9.1BF18C586D
3D9.1BF18C77C7
3D9.1BF18C9AC3
3D9.1BF18C9CC2
3D9.1BF18CBA38
3D9.1BF18CC78A
3D9.1BF18D8403
3D9.1BF18DFA2E
3D9.1BF18E3E0A
3D9.1BF18E9C55
3D9.1BF199C3B4
3D9.1BF1C12F70
3D9.1BF1C2551C
3D9.1BF1C2AD8E
3D9.1BF1CDF773
3D9.1BF1D6EB1C
3D9.1BF1D76AF9
3D9.1BF1DA436C
3D9.1BF1DB3F43
3D9.1BF2022683
3D9.1BF202FF6F
3D9.1BF2074DDF
3D9.1BF2085CCC
3D9.1BF208F95F
3D9.1BF209A0A5
3D9.1BF209BA07
3D9.1BF20A2E9D
3D9.1BF20A69B2
3D9.1BF20A71C3
3D9.1BF20A81A2
3D9.1BF20A8DF7
3D9.1BF20ABED1
3D9.1BF20AD5E3
3D9.1BF20B16E2
3D9.1BF20B2286
3D9.1BF20B2FF5
3D9.1BF20B783F
3D9.1BF20B9030
3D9.1BF20B9EEF
3D9.1BF20BA6FA
3D9.1BF20C2602
3D9.1BF20D0B19
3D9.1BF20D260B
3D9.1BF20DB0EF
3D9.1BF26DADEC
3D9.1C2CB10926
3D9.1C2D027E57
3D9.1C2D60D680
3D9.257C5C63AF
3D9.257C62AE2D
3DD.003BE51618
3DD.00773BF530
3DD.0077AE819E


- RRE to WEL to MCN or ICH or PRA:
3D6.000B38AF91
3D6.000B42D7BA
3D9.1BF189225E
3D9.1BF1896393
3D9.1BF1897517
3D9.1BF189781E
3D9.1BF18978AC
3D9.1BF189842F
3D9.1BF18AB011
3D9.1BF18B0B96
3D9.1BF18BD51C
3D9.1BF18BDAFA
3D9.1BF18BF464
3D9.1BF18C1379
3D9.1BF18C586D
3D9.1BF18C77C7
3D9.1BF18C9AC3
3D9.1BF18C9CC2
3D9.1BF18CBA38
3D9.1BF18CC78A
3D9.1BF18D8403
3D9.1BF18DFA2E
3D9.1BF18E3E0A
3D9.1BF18E9C55
3D9.1BF199C3B4
3D9.1BF1C12F70
3D9.1BF1C2551C
3D9.1BF1C2AD8E
3D9.1BF1CDF773
3D9.1BF1D6EB1C
3D9.1BF1D76AF9
3D9.1BF1DA436C
3D9.1BF1DB3F43
3D9.1BF2022683
3D9.1BF202FF6F
3D9.1BF2074DDF
3D9.1BF2085CCC
3D9.1BF208F95F
3D9.1BF209A0A5
3D9.1BF209BA07
3D9.1BF20A2E9D
3D9.1BF20A69B2
3D9.1BF20A71C3
3D9.1BF20A81A2
3D9.1BF20A8DF7
3D9.1BF20ABED1
3D9.1BF20AD5E3
3D9.1BF20B16E2
3D9.1BF20B2286
3D9.1BF20B2FF5
3D9.1BF20B783F
3D9.1BF20B9030
3D9.1BF20B9EEF
3D9.1BF20BA6FA
3D9.1BF20C2602
3D9.1BF20D0B19
3D9.1BF20D260B
3D9.1BF20DB0EF
3D9.1BF26DADEC
3D9.1C2CB10926
3D9.1C2D027E57
3D9.1C2D60D680
3D9.257C5C63AF
3D9.257C62AE2D
3DD.003BE51618
3DD.00773BF530
3DD.0077A5FFA0
3DD.0077AE819E

- RRE to WEL to mouth to BON:
3D6.000B38AF91
3D6.000B42D7BA
3D9.1BF189225E
3D9.1BF1896393
3D9.1BF1897517
3D9.1BF189781E
3D9.1BF18978AC
3D9.1BF189842F
3D9.1BF18AB011
3D9.1BF18B0B96
3D9.1BF18BD51C
3D9.1BF18BDAFA
3D9.1BF18BF464
3D9.1BF18C1379
3D9.1BF18C586D
3D9.1BF18C77C7
3D9.1BF18C9AC3
3D9.1BF18C9CC2
3D9.1BF18CBA38
3D9.1BF18CC78A
3D9.1BF18D8403
3D9.1BF18DFA2E
3D9.1BF18E3E0A
3D9.1BF18E9C55
3D9.1BF199C3B4
3D9.1BF1C12F70
3D9.1BF1C2551C
3D9.1BF1C2AD8E
3D9.1BF1CDF773
3D9.1BF1D6EB1C
3D9.1BF1D76AF9
3D9.1BF1DA436C
3D9.1BF1DB3F43
3D9.1BF2022683
3D9.1BF202FF6F
3D9.1BF2074DDF
3D9.1BF2085CCC
3D9.1BF208F95F
3D9.1BF209A0A5
3D9.1BF209BA07
3D9.1BF20A2E9D
3D9.1BF20A69B2
3D9.1BF20A71C3
3D9.1BF20A81A2
3D9.1BF20A8DF7
3D9.1BF20ABED1
3D9.1BF20AD5E3
3D9.1BF20B16E2
3D9.1BF20B2286
3D9.1BF20B2FF5
3D9.1BF20B783F
3D9.1BF20B9030
3D9.1BF20B9EEF
3D9.1BF20BA6FA
3D9.1BF20C2602
3D9.1BF20D0B19
3D9.1BF20D260B
3D9.1BF20DB0EF
3D9.1BF26DADEC
3D9.1C2CB10926
3D9.1C2D027E57
3D9.1C2D60D680
3D9.257C5C63AF
3D9.257C62AE2D
3DD.003BE51618
3DD.00773BF530
3DD.0077AE819E

- RRE to WEL to PRA to RIS:
3D9.1BF15FA3DD
3D9.1BF16285AE
3D9.1BF172E833
3D9.1BF17E5D1D
3D9.1BF18AFEFA
3D9.1BF18C45D3
3D9.1BF18DA427
3D9.1BF18DB800
3D9.1BF18E73BE
3D9.1BF18E80F5
3D9.1BF192A467
3D9.1BF19BD855
3D9.1BF1CDE88A
3D9.1BF1D5B9D8
3D9.1BF1D675CC
3D9.1BF1DA7373
3D9.1BF20221E8
3D9.1BF207FAE8
3D9.1BF20873EC
3D9.1BF20B227B
3D9.1BF20C2FA4
3D9.1BF20C3182
3D9.1BF20CDF0C
3D9.1BF20CEEB0
3D9.1BF20D17BC
3D9.1BF25BB9F3
3D9.1C2C44AC6A
3D9.1C2C84BF85
3D9.1C2D3D8D9D
3D9.1C2D462DA2
3D9.257C5A3B05
3D9.257C67123A
3DD.003BA4B9DC

Note: lots of overlap in above four lists


- upstream LGR to NA
3DD.0077E5598A
- This is fine - it's just because it's the last one in the dataset


- Methow River to RRE to WEL:
384.3B23AC6E25
3D9.1C2C3DD368
3D9.1C2C501C8E
3D9.1C2C51CD86
3D9.1C2D125BF9
3D9.1C2D423C24
3D9.1C2D9303CC
3D9.1C2DD6A680
3D9.1C2DD6D852
3D9.1C2DD731BF
3D9.1C2DDA30C0


# run it!

scp -r ./2022-07-16-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# fix quick typo, just reupload the R script

scp ./2022-07-16-complete_det_hist/03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16-complete_det_hist/

# bug fix, reupload R script, re-run on MOX

scp ./2022-07-16-complete_det_hist/03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16-complete_det_hist/


### 7-17-2022 ###

- pull run off of mox, inspect

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16-complete_det_hist/ ./

It looks beautiful!!!


### 7-18-2022 ###

Dang, so there's a bug where we have some NA end times - this messes up interpolating the site visit times
- I'll re-run the states_complete script with that bug fixed, but for now we can use BON arrival times to tweak it
- Okay, so the issue is really in the previous script, where we're not getting end times for site visits that have a single detection (but only at BON adult ladders)

# re-upload the det_hist script and re-run
scp -r ./2022-07-16_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/


# OKAY - let's see if we can get stan running on mox
- So cmdstanr package will load, but then the stan model completely explodes
- I'm not sure we have cmdstan (no r) installed, which is required - https://mc-stan.org/cmdstanr/articles/cmdstanr.html
- So it sounds like our compiler is too old? I think we ran into this issue before
	- can we load any of the available gcc modules?
		- yes we can, but just using module load doesn't fix it

gcc/6.3.1                                                           
           gcc/8.2.1                                                           
                                gcc/10.1.0                                                          
                        gcc_4.8.5-impi_2017                                                 
                            gcc_8.2.1-ompi_3.1.4                                                
                          gcc_8.2.1-ompi_4.0.1  
                          
                          contrib/gcc-8.3.0                                                      
contrib/gcc-9.3.0                                  
contrib/gcc/6.2.0                                  
contrib/gcc/6.2.0_impi                             
contrib/gcc/6.2.0_mpich-3.2                        
contrib/gcc/6.3.0                                  
contrib/gcc49/4.9.4                                
contrib/gcc_9.3.0_ompi_4.0.7                       
contrib/gcc_10.1.0                                 
contrib/gcc_10.1.0_ompi_4.0.3                      
contrib/gcc_cesg/9.1.0   

# pull the run off mox
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16_det_hist/ ./

- move it into the complete_det_hist folder for today
scp -r ./2022-07-18-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

### 2022-07-19 ###

pull the latest complete det hist script off mox

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-18-complete_det_hist/ ./

Okay, so there's still some issues with this. Made the fix, reupload & run:

scp ./2022-07-18-complete_det_hist/03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-18-complete_det_hist/

### 2022-07-20 ### 
# Pull it back off:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-18-complete_det_hist/ ./

Wow okay, so this is terrible. Just super messed up, especially for the tributaries

# Okay, I think I made the bug fix. Reupload to mox
scp -r ./2022-07-19-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

### 2022-07-21 ###
# Pull run off of mox
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-19-complete_det_hist/ ./

This was the fish we were trying to fix: 3D9.1C2C430C8D
- It appears that we were able to fix this issue, so that's good!

Okay, so there are still some issues with this file, but it's much closer. Also, the
changes are clearly taking effect because we now have more mainstem transitions (which
is due to the script now making that fix for making sure that fish are seen in the downstream
states before ascending the adult ladder, even if they're actually in the right state to
enter a tributary - see edits marked 2022-07-19 in 03 script)

Here are the problem tags:

Asotin Creek to RIS to RRE: 
3D9.1BF27C4DF9
3D9.1C2D6A1268
3D9.1C2D6F4187
3D9.1C2D8288E5
3D9.1C2DF614B4
- This is a weird issue, because that's a Columbia mainstem site. I think somehow that we're
selecting the wrong order of sites somewhere in the code.

Clearwater River to RIS to RRE:
3D9.1C2C571E86
3D9.1C2DF53809
3DD.0077A8D748
- Same weird thing as above

Tucannon River to PRA to RIS:
- Same issue as above, just one state earlier in the order
3D9.1BF26D9E1A
3D9.1C2C3F724F
3D9.1C2C4C5CD3
3D9.1C2C4F4C33
3D9.1C2C517040
3D9.1C2DD35628
3D9.1C2DD9566D


This individual: 3D9.1BF27C4DF9
- Really looks like maybe moving downstream through the adult ladder at LGR? But I don't think
we say that for sure. Need to stick with the assumption that for adults, the ladders are unidirectional

Made the edits, reupload to mox hyak & run:
scp -r ./2022-07-21-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Pull run
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-21-complete_det_hist/ ./

### 7-27-22 ###

I would estimate that with the new script and its additional components, running the full detection history in
one run would take about 7 hours. That could easily be run overnight, but would have to be on STF.
There's no reason why we couldn't run the chunks of it separately, and that would allow us to run it
on ckpt. We could run CTH 1-4, 5-8, and 9-11, and 12-14. Each individually should take no more
than 2 hours.

# SO: we are going to take the files in 2022-07-27_det_hist, split them up into four runs, and put
an R script and a bash script in each.

We can then do the same thing with the implicit site visit code later. The implicit site visit
code will obviously have to be updated, given that some things have changed, particularly
the addition of non-ascents at the adult ladders. We'll also have to concatenate the other output
files, but that's not a big deal.

### 7-28-22 ###

So I think we now have our det hist script working. copy over the folders:
scp -r ./2022-07-27_det_hist/CTH1-4/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/
scp -r ./2022-07-27_det_hist/CTH5-8/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/
scp -r ./2022-07-27_det_hist/CTH9-11/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/
scp -r ./2022-07-27_det_hist/CTH12-14/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/

for fixing the R scripts:
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

### 7-29-22 ###

# Pull the scripts off mox, and join them

current directory: from_hyak_transfer/

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

# okay, made some fixes to the R script, reupload:
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# re-run the slurm scripts

# pull the runs
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

# Bug fixes
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/


### 8-4-2022 ###

Pull the runs with the latest scripts
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./


### 8-8-2022 ###
Fixed a pretty silly typo, reupload and re-run R scripts
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# pull runs off hyak

# run the implicit site visit script
scp -r ./2022-08-08-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_stf.slurm

# stf nodes are down for maintenance, split the detection history in two and run with two separate R and slurm scripts

scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm

### 8-9-2022 ###

pull runs off hyak
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-08-08-complete_det_hist_ckpt/ ./

Undone by a small bug! I fixed it. Split it into 4 to increase efficiency, then re-run it:
scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# So it looks like there was still a typo in the previous script, so I need to re-run:
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# fix another typo, re-run AGAIN
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

# run implicit site visit code
somehow this is running really fast

Looks good, but in the future I might want to preserve more information in the states complete file

### 08-10-2022 ###

Once again, fixed a couple of typos and reuploaded scripts:

scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# pull it:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

### 8-11-2022 ###

rerun implicit site visit code
scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# pull it
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-08-08-complete_det_hist_ckpt/ ./

### 8-12-2022 ###

# 1 more minor fix to implicit site visit code

scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# pull it
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-08-08-complete_det_hist_ckpt/ ./

# getting stan up on mox - attempt 3?

So far, we've found that we can't install cmdstan because the compiler is too old. 

# my last notes:

# OKAY - let's see if we can get stan running on mox
- So cmdstanr package will load, but then the stan model completely explodes
- I'm not sure we have cmdstan (no r) installed, which is required - https://mc-stan.org/cmdstanr/articles/cmdstanr.html
- So it sounds like our compiler is too old? I think we ran into this issue before
	- can we load any of the available gcc modules?
		- yes we can, but just using module load doesn't fix it

gcc/6.3.1                                                           
           gcc/8.2.1                                                           
                                gcc/10.1.0                                                          
                        gcc_4.8.5-impi_2017                                                 
                            gcc_8.2.1-ompi_3.1.4                                                
                          gcc_8.2.1-ompi_4.0.1  
                          
                          contrib/gcc-8.3.0                                                      
contrib/gcc-9.3.0                                  
contrib/gcc/6.2.0                                  
contrib/gcc/6.2.0_impi                             
contrib/gcc/6.2.0_mpich-3.2                        
contrib/gcc/6.3.0                                  
contrib/gcc49/4.9.4                                
contrib/gcc_9.3.0_ompi_4.0.7                       
contrib/gcc_10.1.0                                 
contrib/gcc_10.1.0_ompi_4.0.3                      
contrib/gcc_cesg/9.1.0   

# okay, so today:
#first, get a build node:

# then, load R
> module load contrib/R-4.2.0/4.2.0
R

# then, load cmdstanr
> library(cmdstanr)
This is cmdstanr version 0.5.2
- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr
- CmdStan path: /usr/lusers/mmin/.cmdstan/cmdstan-2.30.0
- CmdStan version: 2.30.0

A newer version of CmdStan is available. See ?install_cmdstan() to install it.
To disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.

# try installing cmdstan
> install_cmdstan()
The C++ toolchain required for CmdStan is setup properly!
* Latest CmdStan release is v2.30.1
* Installing CmdStan v2.30.1 in /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1
* Downloading cmdstan-2.30.1.tar.gz from GitHub...
* Download complete
* Unpacking archive...
* Building CmdStan binaries...

# it then starts printing tons of stuff, which ends with:
Warning message:
There was a problem during installation. See the error message(s) above. 
# there are a ton of different errors listed

So GCC compiler is too old: https://discourse.mc-stan.org/t/cmdstan-installation-error/20329/4
[mmin@mox1 /]$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)

and we need at minimum gcc/g++ 4.9.3

# try and load those modules
module load contrib/gcc-9.3.0 
[mmin@mox1 /]$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)

# so this doesn't update the version
module load contrib/gcc/6.2.0 
[mmin@n2233 /]$ gcc --version
gcc (GCC) 6.2.0

# this appears to have worked!

# try installing stan again
R
library(cmdstanr)
install_cmdstan(overwrite = TRUE) # this is necessary because of our past failed installation attempts

# didn't work:

> install_cmdstan(overwrite = TRUE)
The C++ toolchain required for CmdStan is setup properly!
* Latest CmdStan release is v2.30.1
* Installing CmdStan v2.30.1 in /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1
* Downloading cmdstan-2.30.1.tar.gz from GitHub...
* Removing the existing installation of CmdStan...
* Download complete
* Unpacking archive...
* Building CmdStan binaries...
cp bin/linux-stanc bin/stanc
/sw/contrib/gcc-9.3.0/install/bin/g++ -pipe   -pthread -D_REENTRANT  -O3 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials -DNO_FPRINTF_OUTPUT     -O3  -c -x c -include stan/lib/stan_math/lib/sundials_6.1.1/include/stan_sundials_printf_override.hpp stan/lib/stan_math/lib/sundials_6.1.1/src/nvector/serial/nvector_serial.c -o stan/lib/stan_math/lib/sundials_6.1.1/src/nvector/serial/nvector_serial.o
bash: /sw/contrib/gcc-9.3.0/install/bin/g++: Permission denied
chmod +x bin/stanc
make: *** [stan/lib/stan_math/lib/sundials_6.1.1/src/nvector/serial/nvector_serial.o] Error 126
make: *** Waiting for unfinished jobs....

Warning message:
There was a problem during installation. See the error message(s) above. 
# let's try unloading the other gcc version
module unload contrib/gcc-9.3.0 

# now try same installation as before:
# installation worked!
* Finished installing CmdStan to /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1

CmdStan path set to: /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1

# Will this be an issue given that it's not on gscratch?

# Let's try running our stan actual model
# from within the stan_actual folder:
scp -r ./01_int_only_mox/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch job_01_stan_actual_int_only_mox.slurm

# upload a ckpt version to see if it'll run, since we have to wait on stf
scp ./01_int_only_mox/job_ckpt_01_stan_actual_int_only_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/
# fixes to R script:
scp ./01_int_only_mox/01_stan_actual_int_only_mox.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/

### 8-14-2022 ###

# 1000 warmup, 1000 sampling didn't finish in 24 hours. Resubmitted job with 48 hour time limit, but to
make sure I have some results by tomorrow, ran it again with 500 warmup and 500 sampling
scp ./01_int_only_mox/01_stan_actual_int_only_mox_500iter.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/
scp ./01_int_only_mox/job_01_500iter_stan_actual_int_only_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/

# Okay, trying to get the origin model to run. We have 16*54 + 54 parameters, and when we try to compile it says this:

To get around this, we need to try and change a flag, see this post: https://github.com/stan-dev/rstan/issues/395

cd ../../../../../../../../Users/markusmin/.cmdstan

(base) markusmin@Markuss-MBP .cmdstan % make CC="clang++ -O1 -fbracket-depth=2048" ~/var/folders/8c/vz24vqzd2nzfvknzmfcybj0m0000gn/T/RtmpyH5D8E/model-285540e6d8c

# this doesn't work

Instead, navigate to: Users/markusmin/.R

sudo nano Makevars.bck

Edit the text: 
CC=clang
CXX=clang++
change to: 
CC=clang -O1 -fbracket-depth=2048
CXX=clang++ -O1 -fbracket-depth=2048

This didn't work, changed it back

### 8-15-2022 ###

Moving ESU models to hyak and running:

# upper columbia
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
# snake
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
# middle columbia
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

### 8-16-2022 ###

The ESU models are still stuck in the stf queue. Let's try running middle columbia on ckpt, since it's thee smallest file
scp ./middle_columbia/middle_columbia_ckpt_job_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/middle_columbia/

# This got 1% through in 4 hours. That's really bad - moving to parallelize

# Models in parallel
So I wrote the model to run using reduce-sum. This will, theoretically, allow our model to run in about 1/28 the time by using all 28 available cores on a mox node.
HOWEVER - using the parallel-chains flag doesn't actually run different chains on different nodes, but rather only on the same node.
So we will have to figure out how to get the chains to run on separate nodes, or alternatively, to just run one chain at a time. One chain at a time
might be the move for now.

# Try running snake ESU model, with a single chain
scp ./snake/parallel_snake_02_stan_actual_int_origin_mox.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./snake/parallel_snake_02_stan_actual_int_origin.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./snake/job_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./snake/job_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
- only got through 30 iter on ckpt with 4 hours
- could try running this with 100 iter on stf with 24 hours... but that probably wouldn't finish in time for AFS talk recording

# Try running parallel middle columbia
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
- only got through 110 iter on ckpt with 4 hours
- let's try this one to make sure we get some results


# Try running parallel upper columbia
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
- only got through 70 iter on ckpt with 4 hours

### 8-18-2022 ###

For some reason, running these models in parallel on my laptop was faster than running them on mox. I think it might have to do with the seed - I should do 
a benchmark test at some point to make sure mox is faster as it should be.

# Try running parallel middle columbia on ckpt, 100 iter, with new derived movement probabilities in the stan code itsel.
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm

# whoops, looks like I saved the cmdstan files wrong. Need to change that code, and then re-run so that I can actually access the files 
- see https://discourse.mc-stan.org/t/saving-of-cmdstanmodel-objects-from-cmdstanr/16220/3

# Try running all files on ckpt
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm



# wow, okay so I think my slurm scripts were preventing my script from running in parallel:
https://wiki.cac.washington.edu/display/hyakusers/Mox_scheduler
the -ntasks flag needs to be the number of cores you want to use. I had it at 1. Dumb. Changed it to 28 (max cores). Also changed
MEM to 120 GB, which is the recommended memory for 128GB nodes

# These are so much faster. Some models finished in under an hour with 100 warmup and 100 sampling.
Middle Columbia model, with same seed - on mox with 28 cores, 4018 seconds. on laptop with 7 cores, 9510 seconds

Snake finished in 6605 seconds, which is a huge improvement over not finishing :)

upper columbia is the fastest, then middle, then snake

# pull runs off mox:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/middle_columbia/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/upper_columbia/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/ ./

# Reupload all with 200 burn in and 200 sampling, about the max of what will run on ckpt
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

sbatch job_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
sbatch job_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm

# also, remove all old binary stan files

# now pull them:

OKAY - so if you ever change the stan script, you have to remove the old binary file in the same folder. Otherwise, stan will look at the old file
and think oh cool, it's already compiled, no need to do that again! And then you'll run the old model executable, even though the code for the 
new model is in the same folder.

I'm going to also change the seeds, and run three chains of 200 iter + 200 warmup for each of the three models.
# move the new scripts with different seeds to run different chains to mox:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

# run all 9 scripts:
# FIRST - REMOVE THE BINARY FILE

# snake
sbatch job_seed101_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
sbatch job_seed102_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
sbatch job_seed103_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm

# middle columbia
sbatch job_seed101_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed102_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed103_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm

# upper columbia
sbatch job_seed101_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed102_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed103_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm


### 8-19-2022 ###
# Pull these runs
# damn, none of the snake runs finished in time - only got to 320 out of 400.

### 9-6-2022 ###
Run the snake runs on stf instead
scp ./job_seed101_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./job_seed102_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./job_seed103_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/


### 9-7-2022 ###
# pull the new stf runs
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/ ./

### 10-28-22 ###
What I'm doing now is re-running the complete states code, where the river mouth and upstream sites are now called different states.
This is necessary to estimate detection efficiency in the stan model.

# Transfer the v4 script to hyak:
scp -r ./2022-10-27-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Run em all
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# Looks like everything ran fine. Pull the runs:

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-10-27-complete_det_hist_ckpt/ ./

### 11-02-2022 ###
Found an issue with the 03 script where upstream and river mouth detections weren't being recorded separately. Made fix to scripts, now reupload:
- I just renamed the old folder from 10-27 to 11-02

# Transfer updated scripts and run:
scp -r ./2022-11-02-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
# Run em all
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm


# Pull the runs:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-11-02-complete_det_hist_ckpt/ ./


### 11-12-2022 ###

Running DE model on mox hyak (ckpt first for troubleshooting)

# move model files to hyak
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# run it on ckpt:
sbatch job_ckpt_parallel_snake_03_DE.slurm

# submit it on stf too, in case it actually runs:
scp -r ./snake/job_stf_parallel_snake_03_DE.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/

# make a few tweaks to the number of iter in the R script, then move it back

scp -r ./snake/parallel_snake_03_stan_actual_int_origin_mox_deteff.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/


### 11-13-2022 ###

The model is running on my laptop!

Delete the old folder on hyak: rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# current iter is only 50 warmup and 50 sampling, but should give us a sense of how long it'll take to run in its current formulation

# stf is currently totally gummed up
sbatch job_stf_parallel_snake_03_DE.slurm

# run it on ckpt too
sbatch job_ckpt_parallel_snake_03_DE.slurm

CKPT run finished really quickly, but all divergent transitions:
````
[1] "2022-11-13 13:20:05 PST"
Running MCMC with 1 chain, with 28 thread(s) per chain...

Chain 1 WARNING: There aren't enough warmup iterations to fit the
Chain 1          three stages of adaptation as currently configured.
Chain 1          Reducing each adaptation stage to 15%/75%/10% of
Chain 1          the given number of warmup iterations:
Chain 1            init_buffer = 7
Chain 1            adapt_window = 38
Chain 1            term_buffer = 5
Chain 1 Iteration:  1 / 100 [  1%]  (Warmup)

Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpiETiWp/model-245c5415e0c8.stan', lin$
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpiETiWp/model-245c5415e0c8.stan', lin$
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Iteration: 10 / 100 [ 10%]  (Warmup)
Chain 1 Iteration: 20 / 100 [ 20%]  (Warmup)
Chain 1 Iteration: 30 / 100 [ 30%]  (Warmup)
Chain 1 Iteration: 40 / 100 [ 40%]  (Warmup)
Chain 1 Iteration: 50 / 100 [ 50%]  (Warmup)
Chain 1 Iteration: 51 / 100 [ 51%]  (Sampling)
Chain 1 Iteration: 60 / 100 [ 60%]  (Sampling)
Chain 1 Iteration: 70 / 100 [ 70%]  (Sampling)
Chain 1 Iteration: 80 / 100 [ 80%]  (Sampling)
Chain 1 Iteration: 90 / 100 [ 90%]  (Sampling)
Chain 1 Iteration: 100 / 100 [100%]  (Sampling)
Chain 1 finished in 758.2 seconds.
Warning: 50 of 50 (100.0%) transitions ended with a divergence.
See https://mc-stan.org/misc/warnings for details.
````

Those first simplex related warnings seem not a big deal, since they only occur at the start of the chain during the warmup phase. 
But I'm not sure why you'd get any "-nan" values when summing the probabilities parameter... so might be worth looking into.

# Let's pull the run so we can look at the posteriors
# From within the diagnostics folder:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./
# Looks like these chains didn't move at all (no chains finished successfully)

# Set seed to 123, bumped adapt_delta to 0.85, increased chains to 200 warmup + 200 iter. So let's see if any of that helps.
scp ./parallel_snake_03_stan_actual_int_origin_mox_deteff.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/

# more likely though it's a model issue. But let's just try this first.

# okay, so results are not good:
Chain 1 Iteration:   1 / 400 [  0%]  (Warmup)
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpiETiWp/model-245c5415e0c8.stan', lin$
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Iteration:  10 / 400 [  2%]  (Warmup)
Chain 1 Iteration:  20 / 400 [  5%]  (Warmup)
Chain 1 Iteration:  30 / 400 [  7%]  (Warmup)
Chain 1 Iteration:  40 / 400 [ 10%]  (Warmup)
Chain 1 Iteration:  50 / 400 [ 12%]  (Warmup)
Chain 1 Iteration:  60 / 400 [ 15%]  (Warmup)
Chain 1 Iteration:  70 / 400 [ 17%]  (Warmup)
Chain 1 Iteration:  80 / 400 [ 20%]  (Warmup)
Chain 1 Iteration:  90 / 400 [ 22%]  (Warmup)
Chain 1 Iteration: 100 / 400 [ 25%]  (Warmup)
Chain 1 Iteration: 110 / 400 [ 27%]  (Warmup)
Chain 1 Iteration: 120 / 400 [ 30%]  (Warmup)
Chain 1 Iteration: 130 / 400 [ 32%]  (Warmup)
Chain 1 Iteration: 140 / 400 [ 35%]  (Warmup)
Chain 1 Iteration: 150 / 400 [ 37%]  (Warmup)
Chain 1 Iteration: 160 / 400 [ 40%]  (Warmup)
Chain 1 Iteration: 170 / 400 [ 42%]  (Warmup)
Chain 1 Iteration: 180 / 400 [ 45%]  (Warmup)
Chain 1 Iteration: 190 / 400 [ 47%]  (Warmup)
Chain 1 Iteration: 200 / 400 [ 50%]  (Warmup)
Chain 1 Iteration: 201 / 400 [ 50%]  (Sampling)
Chain 1 Iteration: 210 / 400 [ 52%]  (Sampling)
Chain 1 Iteration: 220 / 400 [ 55%]  (Sampling)
Chain 1 Iteration: 230 / 400 [ 57%]  (Sampling)
Chain 1 Iteration: 240 / 400 [ 60%]  (Sampling)
Chain 1 Iteration: 250 / 400 [ 62%]  (Sampling)
Chain 1 Iteration: 260 / 400 [ 65%]  (Sampling)
Chain 1 Iteration: 270 / 400 [ 67%]  (Sampling)
Chain 1 Iteration: 280 / 400 [ 70%]  (Sampling)
Chain 1 Iteration: 290 / 400 [ 72%]  (Sampling)
Chain 1 Iteration: 300 / 400 [ 75%]  (Sampling)
Chain 1 Iteration: 310 / 400 [ 77%]  (Sampling)
Chain 1 Iteration: 320 / 400 [ 80%]  (Sampling)
Chain 1 Iteration: 330 / 400 [ 82%]  (Sampling)
Chain 1 Iteration: 340 / 400 [ 85%]  (Sampling)
Chain 1 Iteration: 350 / 400 [ 87%]  (Sampling)
Chain 1 Iteration: 360 / 400 [ 90%]  (Sampling)
Chain 1 Iteration: 370 / 400 [ 92%]  (Sampling)
Chain 1 Iteration: 380 / 400 [ 95%]  (Sampling)
Chain 1 Iteration: 390 / 400 [ 97%]  (Sampling)
Chain 1 Iteration: 400 / 400 [100%]  (Sampling)
Chain 1 finished in 9064.3 seconds.
Warning: 198 of 200 (99.0%) transitions ended with a divergence.
See https://mc-stan.org/misc/warnings for details.

Warning: 2 of 200 (1.0%) transitions hit the maximum treedepth limit of 10.
See https://mc-stan.org/misc/warnings for details.

# 198 of 200 transitions ended with a divergence, and the two that didn't hit a the maximum treedepth limit.
# this isn't as concerning, since this is an efficiency issue. Divergent transitions definitely are though.

# I found one typo in the model. I fixed it, now reupload the folder
- I deleted the old snake folder, then re-upload
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# then resubmit the ckpt run

### 11-14-2022 ###

This unfortunately doesn't look like it fixed the issue. Pull the run to the diagnostics folder
# From within the diagnostics folder:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./
# rename run as snake3

# All divergent transitions

Alright, I found one indexing typo with Fifteenmile Creek. Maybe that fixes it?

I also removed all upstream -> RM and RM -> upstream transitions from the transition matrix.
I also removed a bunch of upstream states that no fish have ever been in (in our states_complete data, which we subset to
remove upstream states in DE years)
I also made a temporary change for the Yakima, where we remove run year 04/05 from the DE years. That's because
we don't currently have discharge data for that year, so it'll just get confused.

Alright, let's try moving the run again

# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# snake 5 has printed p_vec - but that doesn't seem to be the issue, since sum p_vec_observed is always 1


# Okay - after meeting with Mark, I'm going to try multiple runs, only 100 iter each, with different
stan run settings (adapt_delta, step_size) and see if we can get something to work.

adapt_delta = 0.95
adapt_delta = 0.9
step_size = 0.5
step_size = 1

Made a new folder called snake2022-11-14-1436 to store all of these permutations
# move it to hyak
scp -r ./snake2022-11-14-1436/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake2022-11-14-1436/ ./
Didn't work. Let's look again in the model for errors.

# Ok, line 270 has an indexing typo - everything is off by one.
run_year_indices_vector[1:(i-1)] = n_obs[1:i-1];
# Change it to this:
run_year_indices_vector[1:(i-1)] = n_obs[1:(i-1)];

# Try uploading and running this new model, with only 50 iter
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Still getting lots of divergences. Pull the run, rename it as snake6
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# Made a couple of changes to model code - removed imnaha and fifteenmile betas entirely, fixed those values in det_eff_param_vector to zero.
# Don't think that was the issue but probably good to do anyway.
# I also took out some of the transitions that we no longer observed (just upstream states) from the transformed parameters block. But I also 
# don't think that was the issue.

# I can't find any other glaring issues. Let's just try running the model in its current configuration.
# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# things we can still try: 
1) fix all det eff beta terms in the param vector to their posterior medians
2) fix all det eff alpha terms to their posterior medians
3) fix all det eff beta terms to zero (so in theory would ignore discharge)

# might help us eliminate different possible problems

### 11-15-2022 ###
There was a typo that I fixed. Reupload and run.
# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# This run also has divergent transitions. Pull it and rename as snake7
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# Okay, I found an indexing error in the transition_run_years vector. It was using a version of the
states_complete df that wasn't the final one, so I moved the creation of the vector to right before
the data call. So that should fix that issue.Reupload and run.
# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Pull the new run, rename as snake8
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Still 100% divergent transitions. Frick!!

Okay, so for this run, I removed all beta terms - took them out of the parameters block, set them to zero in transformed parameters, commented out priors block

Try running this:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

### 11-16-21 ###
# Pull the new run, rename as snake9
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

All still divergent transitions, even without discharge in there.
Comment out the priors on alpha terms, and remove them as parameters. In the transformed parameters block, assign them directly their mean values
from the last script.

Try running this:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Pull the new run, rename as snake10
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

98/100 transitions ended in a divergence. But we didn't get the message at the beginning that sum(p_vec_observed) is -nan. So
it seems like perhaps that value was related to sampling from priors on those alpha terms?

Can't figure out what's wrong. Turned on all of the print statements again. Also added print run year statement.
Also added print current state. Also moved run year indexing outside of k loop (should help with efficiency, but won't fix the bug)

Try running this:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Ran it for 30 minutes, now pull it and rename as snake11
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Not immediately clear if there are any issues. Updated the print statements, reupload and re-run
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# didn't run, need to reupload and run again

# pull it! and rename as snake12
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, this new print statement is super helpful. There are a bunch of transitions where the probability of the next state is either 0 or 1.

Another odd thing: It has multiple times where it goes back to the same fish (searching for i = 1 in the output reveals multiples times).
Is that a warmup thing?
Yeah I think so - this from Bob Carpenter: "There's a description in the part of the manual about algorithms. We 
recommend running at least 100 warmup iterations. Usually the first 
few don't do much because the scale's off. Then they hit increments where 
step size is fit, and the result is often a perceived hang on Stan's part 
because it moves to doing way more steps per iteration to get stability. "

There are some infrequent probabilities that get close to zero or 1. Those should be fine. What's concerning
is that there are long stretches of iterations, it seems like starting with i = 1,  that keep returning 1 or 0 probabilities.


# Okay - added a print(logits) statement to stan model, now reupload and run
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# pull it! and rename as snake 13
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# Okay, clearly an isue with logits. Added another print statement to print b0_matrix itself.

Run the new run after uploading

### 11-20-2022 ###


Pull the latest run and rename as snake14
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, I found an issue in the stan code, where the return(total_lp) call was in the wrong loop.
Moved it to the correct loop, re-upload and run.
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/



### 11-22-2022 ###

After meeting with Mark and Rebecca, decided to make two changes for troubleshooting:
1) set det_eff = 1 in loop, and 2) set inits = 0. Upload and run:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

### 11-28-2022 ###



# Pull latest run, rename it as snake15
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, so with inits set to 0, I can now see that the problem starts when we move off of the inits.
Until that point, every value is set to 0 in each iteration. Once it starts sampling (?), the
values immediately move to really crazy values and get stuck there. Then it eventually goes back
to all zeros, once it moves back to the first fish.

Okay, so I found a spot where the return(total_lp) was in the wrong loop. Fixed that and reuploaded.

# Pull the run, rename it as snake16
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

So in this run I don't see the values ever moving off the initial values, but they also don't ever go
to zero or one. So that's perhaps an improvement?

In the next version of the stan model, I commented out all of the print statements to allow it to actually run.

Reuploaded. Then run on ckpt and stf.

### 11-29-2022 ###

Only the ckpt run ran overnight - stf run still waiting in the queue.

# Pull it and rename as snake17
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, so it only got through 40 iterations in 4 hours. That is way slower than the model with bugs,
which might actually be a good sign. However, it's way slower than the original model without detection
efficiency, and given that the current version doesn't even have detection efficiency, that's not a great sign.

# Okay so I commented out all of the detection efficiency loops. Let's see how much that improves performance.

Removed old folder, reuploaded. Then run on ckpt
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Pull run, rename it as snake18
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Still completed 40 iter in 4 hours. So commenting out det eff blocks didn't change anything.

Bumped up stf run time to 48 hours, submitted it.

Create a new run folder, to run 40 iter (20 warmup and 20 sampling) on ckpt so we can see how it looks.

scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake40iter/

# Pull the run, rename as snake19
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake40iter/ ./

This run didn't have any divergent transitions! We just get the maximum treedepth issue.

So, let's try running a 40iter version with det eff

I canceled the STF run that was in the queue.

Removed old folder, reuploaded. Then 40iter, with deteff (but fixed values, no priors, and only intercept and therefore no relationship with discharge), run on ckpt
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

### 2022-11-30 ###

Pull the latest run, rename as snake20
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./


40 iter.
This run worked. Run time went up, but only very minor amount.
Last run, without det eff: Chain 1 finished in 10864.8 seconds.
This run, with det eff (but these not included as parameters): Chain 1 finished in 11176.9 seconds.
So like five minutes longer

Looking at modeling results.

Made some edits, added back in det eff parameters (no longer fixed, but using priors on the alphas, still no betas). Then upload and run.
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

Simultaneously, we're going to run the next version of the model on mox hyak - using betas and discharge data.

Made a directory called snake_alpha_beta for this run.
Move the new folder onto hyak into that directory:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_alpha_beta/

Pull both of the above runs:

# The run with det eff parameters - alphas only, no betas:
# Pull and rename as snake21:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# The run with det eff parameters - alphas and betas:
# Pull and rename as snake22:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_alpha_beta/snake/ ./

Okay, so both of these runs finished in about the same time as models without detection probability (this doesn't completely
make sense to me, since aren't you monitoring more parameters?):
snake 21: Chain 1 finished in 11012.5 seconds.
snake 22: Chain 1 finished in 11853.8 seconds.

However, both have the not a valid simplex issue:
Chain 1 Iteration:  1 / 40 [  2%]  (Warmup) 
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpeUJp9k/model-790b48db88cc.stan', line 303, column 8 to column 81) (in '/tmp/RtmpeUJp9k/model-790b48db88cc.stan', line 1405, column 2 to line 1408, column 98)
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

But no divergent transitions.

Inspecting the chains in shinystan:

snake21 (alphas, not betas) looks fine!
snake22 (alphas and betas) looks fine as well!


Next model run:
There appears to be no queue on stf right now, so should submit some jobs!
snake22 is the full model. Let's run it for 100, 200, and 500 sampling iterations.

# Just keep changing the number of iterations in the R script and then reupload
# 100 iter:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake100iter/

# 200 iter:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake200iter/

# 500 iter:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake500iter/

# submitted all three jobs on stf. Had to update amount of time requested. All are running!

### 2022-12-01 ###

Updated the upper columbia model. Upload and run on STF

# 100 iter:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia100iter/

# 200 iter:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia200iter/

# 500 iter:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia500iter/


So - upper columbia is running at a rate of 70 iterations in 3:10. It's definitely a bit faster than the snake model,
which makes sense given that there are less fish in this dataset.


### 12-02-2022 ###

Tried making an edit for performance, by vectorizing part of a loop.

# Okay, so I got the snake model to compile and start running locally. Let's run it for 40iter on ckpt to benchmark 
# the performance against the other version of the model.
Upload and try:

# create a directory: snake_vectorized_20iter
# Upload:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_20iter/

# Middle Columbia - upload the three iter versions

# 100 iter:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia100iter/

# 200 iter:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia200iter/

# 500 iter:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia500iter/

# Updated the slurm scripts

# submitted jobs!

### 12-03-2022 ###

Alright, so vectorized (summing lp of fish) snake run:
Chain 1 finished in 9513.2 seconds.
This is about a 25% increase in performance from the model where this part of the loop is not vectorized (which took 11853 seconds).

# Pull and rename as snake_vectorized_1_20iter/:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_20iter/snake/ ./

Let's try vectorizing a second part of the loop (summing lp of individual observations of a fish), and then submitting that
# Upload:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized2_20iter/

Performance of different ESU models:
Middle Columbia 100 iter: Chain 1 finished in 31098.0 seconds.
Upper Columbia 100 iter: Chain 1 finished in 37228.0 seconds.
Snake 100 iter: Chain 1 finished in 91096.9 seconds.

Snake takes by far the longest.

### 12-04-2022 ###

# Pull second vectorized snake run, rename as snake_vectorized_2
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized2_20iter/ ./

Oops, didn't work due to an indexing issue. Fixed the typo. Re run!

# Created a results folder locally to store the outputs from the various runs:
/Users/markusmin/Documents/CBR/steelhead/stan_actual/deteff_ESU_models/results/

# Make subfolders for run lengths - 200 iter

# Import runs into them to inspect - 200iter runs are available now for all ESUs - 500iter only available for Upper Columbia

# From within results/200iter/ folder:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake200iter/snake/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia200iter/upper_columbia/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia200iter/middle_columbia/ ./

# Note that currently I have also only run a single chain for each of these. So ideally I would submit 
three chains (separately) for each ESU

# Inspecting 200iter runs in shiny_stan:
All three look to give reasonable results, but some things don't look great with the chains, for example
seeing a lot of autocorrelation in some (but not all) parameters

Upon inspecting results, they all appear to make sense. Which is great news

# Inspect vectorized2 run
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized2_20iter/ ./
Chain 1 finished in 9939.3 seconds.
Making this change appears to have actually increased the run time. That makes sense, given that we are 
creating a new vector for every single fish, so it's not surprising that the cost of that line outweighed
the benefits of vectorization. Removed this again.

Let's see if we can run three chains on three nodes on mox.
set this in R script: options(mc.cores=3)
set parallel.chains to 3 in mod$sample()
updated stf job script to request three nodes (--nodes = 3)
Run 200 iter of snake vectorized_1:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_200iter/

Run 20 iter of snake vectorized_1, on ckpt, with 3 chains - to see if it even works
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_200iter/


### 2023-04-25 ###

We're back! Time to run the last model, but now with rear type (hatchery vs. wild) as a covariate

### 2023-05-01 ###

So I think cmdstan is working, but we're getting a ton of deprecated warnings from a program called by stan.
To turn these off, I went to this file: /Users/markusmin/.cmdstan/cmdstan-2.32.0/make
# and added: CXXFLAGS += -Wno-deprecated-declarations

Based on advice from here: https://discourse.mc-stan.org/t/running-cmdstan-on-ventura/30948/11





