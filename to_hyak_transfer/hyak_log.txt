#### Hyak command line commands

# Copying folders:
# Note: To move to remote, you have to execute the command from your local machine
scp -r 1200cov_hyak/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Moving stuff back off of hyak:
# Note: You again need to be on your local machine, in the ./from_hyak_transfer folder
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-17_origin_1200/ ./

# Run the Rscript locally
srun -p stf -A stf --ntasks=3 --mem=100G --time=4:00:00 --pty bash –l

# Get an interactive build node (run things from command line):
srun -p build --time=2:00:00 --mem=10G --pty /bin/bash

# To cancel a job: scancel <jobid>

# Useful R info: https://wiki.cac.washington.edu/display/hyakusers/Hyak+R+programming

# R module load:
module load contrib/R-4.2.0/4.2.0

##### Installing R (version 4) on MOX Hyak!

# The second option from this link appears to work: https://stackoverflow.com/questions/46343044/install-r-in-linux-server
# Make sure to run ./configure --with-pcre1

# What I ran:
wget http://cran.rstudio.com/src/base/R-4/R-4.2.0.tar.gz
tar xvf R-4.2.0.tar.gz
cd R-4.2.0
./configure --prefix=/sw/contrib/R-4.2.0 --with-pcre1

## Output after configure:
``
R is now configured for x86_64-pc-linux-gnu

  Source directory:            .
  Installation directory:      /sw/contrib/R-4.2.0

  C compiler:                  gcc -std=gnu11  -g -O2
  Fortran fixed-form compiler: gfortran  -g -O2

  Default C++ compiler:        g++ -std=gnu++11  -g -O2
  C++11 compiler:              g++ -std=gnu++11  -g -O2
  C++14 compiler:                 
  C++17 compiler:                 
  C++20 compiler:                 
  Fortran free-form compiler:  gfortran  -g -O2
  Obj-C compiler:	       gcc -g -O2 -fobjc-exceptions

  Interfaces supported:        X11, tcltk
  External libraries:          pcre1, readline, curl
  Additional capabilities:     PNG, JPEG, TIFF, NLS, cairo, ICU
  Options enabled:             shared BLAS, R profiling

  Capabilities skipped:        
  Options not enabled:         memory profiling

  Recommended packages:        yes

configure: WARNING: you cannot build PDF versions of the R manuals
configure: WARNING: you cannot build PDF versions of vignettes and help pages
``
make && make install

# Then create a module file using these instructions: https://wiki.cac.washington.edu/display/hyakusers/Hyak_modules

# This worked: Just with some odd file paths
# Now to run R:
module load contrib/R-4.2.0/4.2.0


##### Installing JAGS on Hyak
# From this website: http://levlafayette.com/node/498

# In directory: gscratch/stf/mmin/JAGS:

# Source attempt 1
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz

# Okay, I think that the above file is for Mac? Let's try this link to the Debian version:

# Source code (Debian)
wget https://ftp.debian.org/debian/pool/main/j/jags/jags_4.3.1-1.debian.tar.xz
tar xvf jags_4.3.1-1.debian.tar.xz
mv debian jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1

# Yeah, so I can't figure out how to get this to work. Let's try again with the binary
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz
tar xvf JAGS-4.3.1.tar.gz
mv JAGS-4.3.1 jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1
make && make install

# Info at this link: https://github.com/cran/rjags/blob/master/INSTALL
# Then, you need to run this from the command line to tell R where to look for JAGS when installing rjags:
export PKG_CONFIG_PATH=/sw/contrib/JAGS/jags-4.3.1/lib/pkgconfig
# Then run this:
export LD_RUN_PATH=/sw/contrib/JAGS/jags-4.3.1/lib
# this worked!


#### 2022-05-17 ####

Moved folder to hyak to run origin only model w 1200 fish:

From within directory: /Users/markusmin/Documents/CBR/steelhead/hyak_transfer

# Origin only runs:

scp -r 2022-05-17_origin_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Move back to local:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-17_origin_1200/ ./

# Origin and rear runs:
scp -r 2022-05-17_origin_rear_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Move back to local:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-17_origin_rear_1200/ ./


#### 2022-05-18 ####

Temperature only runs:

Moving to hyak:
scp -r 2022-05-18_temp_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Ran these models

# Move back to local
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-18_temp_1200/ ./

Temperature and flow runs:

Moving to hyak:
scp -r 2022-05-18_temp_flow_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

# Ran these models

# Move back to local
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-18_temp_flow_1200/ ./

#### 2022-05-19 ####

All 4 covariate models:

Moving to hyak:
scp -r 2022-05-19_all4_1200 mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

#### 2022-05-20 ####

Moving all 4 cov model off of hyak
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-05-19_all4_1200/ ./

#### 2022-05-23 ####

Running complete det hist for loop on hyak

# move to hyak:
scp -r 2022-05-23-complete_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# move back to local - troubleshoot

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-23-complete_det_hist/ ./

#### 2022-05-24 ####

# Running det hist for loops

scp -r 2022-05-24_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Just getting the slurm output - troubleshoot

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/slurm-2736747.out ./

#### 2022-05-25 ####

My complete det hist run from yesterday seems to have completed overnight - pull it off and check

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/ ./

Looks like it ran okay!

Moved these two files to /Users/markusmin/Documents/CBR/steelhead/to_hyak_transfer/2022-05-25-complete_det_hist
complete_event_site_metadata.csv
complete_det_hist.csv

# Moved this new folder to hyak

scp -r 2022-05-25-complete_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Pull it back off hyak


scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-25-complete_det_hist/ ./


#### 5-26-22 ####

Pulled runs off hyak

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-25-complete_det_hist/ ./

Fixed a typo, moved back to hyak

Created a second version of the R script for testing, with an associated slurm script (both of these have test in the name)

Ran both scripts


scp -r 2022-05-25-complete_det_hist mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/


Okay, it looks like the complete det hist script ran!


Now, run the intercept only model:
- first transfer to hyak:
scp -r 2022-05-26_fullmodel_intercept_only mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/
# Ran model
Got error - ran out of memory:
[1] "Start time: 2022-05-26 15:53:30"
Error in unserialize(node$con) : error reading from connection
Calls: jags.parallel ... FUN -> recvData -> recvData.SOCKnode -> unserialize
Execution halted
slurmstepd: error: Detected 7 oom-kill event(s) in StepId=2741132.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.

Let's try running this on klone then since there's more memory there
scp -r 2022-05-26_fullmodel_intercept_only mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

Okay - so I would have to entirely reconstruct my computing environment on klone for this to run. Will take a bit longer to do this


#### 5-27-22 ####
The first thing I tired to address the memory issue is to change the parameters monitored from the whole matrix (which has lots of zeros) to just the ones of interest.
I did this by indexing in the JAGS code and storing these as parameters, and then monitoring those.
- This took us down from 841 monitored parameters to 54
- uploaded this new script to hyak:

scp -r 2022-05-26_fullmodel_intercept_only/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

Still crashed...


### 5-31-22 ###

Okay, let's try Mark's suggestion to monitor less parameters

Monitor only the first ten - move to MOX
scp -r 2022-05-31_fullmodel_intercept_only_monitor_1.10/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

This didn't work

Try Mark's other suggestion to drop the iter and thin and burn way down
scp -r 2022-05-31_fullmodel_intercept_only_monitor_1.10/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/


### 06-01-22

It seems to work less and less - maybe I need to clear the workspace?

scp -r 2022-06-01_fullmodel_intercept_only_monitor_1.1/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/



### Reinstalling everything on KLONE ###

##### Installing R (version 4) on KLONE

# Very useful link: https://hyak.uw.edu/docs/compute/scheduling-jobs/

# Note: there are no build nodes on KLONE - see this link for differences from MOX: https://hyak.uw.edu/blog/mox-to-klone
# Instead of srun, use salloc
salloc -p compute -A stf -N 1 -c 1 --time=2:00:00 --mem=10G

# run hyakalloc to show what resources are available
- interestingly, now the partitions available for account stf are compute, gpu-2080ti, hugemem, and interactive

# How to install stuff: https://hyak.uw.edu/docs/tools/containers

# The second option from this link appears to work: https://stackoverflow.com/questions/46343044/install-r-in-linux-server
# Make sure to run ./configure --with-pcre1

# What I ran:
First, navigate to /sw/contrib/stf-src/
wget http://cran.rstudio.com/src/base/R-4/R-4.2.0.tar.gz
tar xvf R-4.2.0.tar.gz
cd R-4.2.0
# Configuring is not working
./configure --prefix=/sw/contrib/stf-src/R-4.2.0 --with-pcre1 # Didn't work
./configure --with-readline=no --with-x=no
./configure --prefix=/sw/contrib/stf-src --with-readline=no --with-x=no # didn't work
make && make install

apptainer exec tools.sif /sw/contrib/stf-src/R-4.2.0/configure --prefix=/sw/contrib/stf-src/R-4.2.0 --with-pcre2 --with-readline=no --with-x=no

# I keep getting this error: configure: error: "liblzma library and headers are required"

Let's try this solution: 
https://stackoverflow.com/questions/42170752/building-package-using-configure-how-to-rope-in-updated-versions-of-libs-heade

First, navigate to /sw/contrib/stf-src/
wget https://tukaani.org/xz/xz-5.2.5.tar.gz
tar xzvf xz-5.2.5.tar.gz
cd xz-5.2.5
./configure --prefix=/sw/contrib/stf-src
# make -j3
# make install
make && make install


# Then create a module file using these instructions: https://wiki.cac.washington.edu/display/hyakusers/Hyak_modules
/sw/contrib/stf-src/xz-5.2.5


##### Installing JAGS on Hyak
# From this website: http://levlafayette.com/node/498

# In directory: gscratch/stf/mmin/JAGS:

# Source attempt 1
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz

# Okay, I think that the above file is for Mac? Let's try this link to the Debian version:

# Source code (Debian)
wget https://ftp.debian.org/debian/pool/main/j/jags/jags_4.3.1-1.debian.tar.xz
tar xvf jags_4.3.1-1.debian.tar.xz
mv debian jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1

# Yeah, so I can't figure out how to get this to work. Let's try again with the binary
wget https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Source/JAGS-4.3.1.tar.gz
tar xvf JAGS-4.3.1.tar.gz
mv JAGS-4.3.1 jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1
make && make install

# Info at this link: https://github.com/cran/rjags/blob/master/INSTALL
# Then, you need to run this from the command line to tell R where to look for JAGS when installing rjags:
export PKG_CONFIG_PATH=/sw/contrib/JAGS/jags-4.3.1/lib/pkgconfig
# Then run this:
export LD_RUN_PATH=/sw/contrib/JAGS/jags-4.3.1/lib


### 06-02-22 ###

Let's try monitoring only one parameter, using only 10% of the data, and running only 2000 iterations

scp -r 2022-06-02_fullmodel_intercept_only_monitor_1.1_2kiter_0.1data/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/

Decreasing the dataset down to only 5% of the data results in this:


Error in if ((W > 1e-08) && (n.chains > 1)) { :
  missing value where TRUE/FALSE needed
Calls: jags.parallel -> as.bugs.array2 -> monitor -> conv.par
Execution halted


My understanding is that this is related to the fact that with such little data, the estimates for the parameters are unstable: 
https://sourceforge.net/p/mcmc-jags/discussion/610037/thread/7fb16eb3/?limit=25
- this also only happens with more than one chain - so re-run with only 1 chain


# Pull the one chain off of MOX
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/JAGS_runs/2022-06-02_fullmodel_intercept_only_monitor_1.1_2kiter_0.1data/ ./





### 06-10-22 ###

So, need to fix the complete det hist script

I created a new folder to run the analyses on Hyak, called 2022-06-10-complete_det_hist
I am editing the scripts in THIS FOLDER, so all of the other versions of the 03 script are now outdated


List of current issues:
- FIXED Hood River was not in list of tributaries
- We are not distinguishing post-spawning behavior; this leads to detection histories like this where detections are TWO YEARS APART
	- tag code: 3D9.1BF26D8CCC
	- Wenatchee River 2009-09-14
	- BON adult ladder 2011-08-06
	- And then between these two, the implicit sites are alllll messed up
- FIXED Many have duplicated, but non-implicit site visits - somehow the script isn't ignoring them
	- For example, this tag code: 384.1B796A520E
		- Repeat at BON to MCN
	- FIX: I changed from using the insertRow to bind_row call in the stepwise states for when we see fish
	in the adult ladders but they're in the same state previously. Edits have notes "2022-06-10"
- Some have duplicated, implicit site visits:
	- FIX: Alright, so I had to move an ifelse statement inside the loop. See note for 2022-06-10. I think this should work
		- I had to do this in multiple spots - this is for whenever we're missing the downstream state from a dam
	- 384.1B796A8099
		- twice in RRE to WEL state
	- 384.36F2B32373
		- twice in MCN to ICH or PRA state
 	- 384.3B239A405E
 		- twice in ICH to LGR state
 	- 3D9.1BF1635CC8
 		- twice in PRA to RIS state
- Some flip back and forth between implicit states
	- 384.3B23AB763C
		- Two site visits: Imnaha River and mainstem, mouth to BON (via fallback arrays)
			- In between: "Imnaha River", "mainstem, upstream of LGR",   "mainstem, ICH to LGR", "mainstem, MCN to ICH or PRA",
			 "mainstem, ICH to LGR"        "mainstem, MCN to ICH or PRA" "mainstem, BON to MCN"        "mainstem, mouth to BON"   
	- This is the same issue as before (see line 405). Need to move an ifelse statement inside the loop. See note for 2022-07-11.
	- This has an additional issue, where it was off by a few lines because of how many states were inserted. So I added a line at the beginning
	of the for loop to skip lines in the det_hist that are already implicit
- Some skip intervening states when leaving a tributary
	- Seem to be an issue leaving Entiat River or Methow River states
	- For example, this fish (384.3B23AC6E25) goes straight from Methow to RRE to WEL and skips upstream to WEL state
		- This is seen in a total of 11 fish
	- One fish (3D9.1C2D937BC6) leaves Entiat and goes straight to RIS to RRE, skipping RRE to WEL
		- This also looks like a fish who's going back out - seen at Entiat, then in BON fallback arrays a year later
		
		
# Some of these issues look like they're actually coming from the previous script - like the issues with duplicated detections that aren't implicit
- Actually that's not true, I think they're all from the complete_det_hist script



### 06-22-2022 ###

Setting up stan to work on mox hyak

# Copying over our folder
scp -r ./2022-06-22_01_int_only/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

# So in order to get cmdstan to install, we need to update GCC
# Current version:
[mmin@mox1 2022-06-22_01_int_only]$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

# It would appear there are plenty of gcc modules available

let's try:
module load contrib/gcc_8.2.1-ompi_4.0.2
module load contrib/gcc_10.1.0_ompi_4.0.3

# See this link: https://discourse.mc-stan.org/t/cmdstan-installation-error/20329

# Okay, download from source?

wget https://bigsearcher.com/mirrors/gcc/releases/gcc-12.1.0/gcc-12.1.0.tar.gz
tar xvf JAGS-4.3.1.tar.gz
mv JAGS-4.3.1 jags-4.3.1
cd jags-4.3.1
./configure --prefix=/sw/contrib/JAGS/jags-4.3.1
make && make install


### 07-11-20222 ###

running implicit site visit code again on hyak
scp -r 2022-06-10-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/


### 07-12-2022 ###

Pulling complete det hist script  results off of hyak

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-06-10-complete_det_hist/ ./

Wasn't able to complete in time limit - got about 60% of the way through before 4 hours was up.
STF partition is undergoing maintenance, so we're going to split up the dataset and try again

# run it again - rename with today's date

scp -r 2022-07-12-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

Well, it looks like ckpt is down for maintenance too


So there are some tag codes that strangely don't have any times, for example 384.1B796A1794. Will need to investigate
- Okay, so looking in CTH1, reason for this is the fish never made it out of the first state, so we don't have any transition times.
Mark	HAGE - Hagerman NFH
Observation	BO3 - Bonneville WA Shore Ladder/AFF
Recovery	COLR4 - Columbia River - Bonneville Dam to John Day Dam (km 234-347)

# Pull run off mox - just use the stf run
# this isn't the right one: scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-12-complete_det_hist/ ./
# this is the stf run: 
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-06-10-complete_det_hist/ ./

# Okay, run failed - issue was that it hit the end of the df and gave this error message:
Error in if (det_hist[i, "site_class"] == "implicit") { : 
  missing value where TRUE/FALSE needed
Execution halted

I thought we fixed this with the dummy fish? Not sure why this is popping up again.
- Fixed it - had to add site_class = dummy for the dummy fish


scp -r 2022-06-10-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# It's running, but it still has issues.

Some known problematic tag codes:

transitions between mainstem ICH to LGR and same state + BON to MCN:
384.3B23AB34F1
384.3B23AB34F1
384.3B23ADC7F8
384.3B23ADC7F8
384.3B23AEB441
384.3B23AEB441
3D9.1C2C425108
3D9.1C2C425108
3D9.1C2C7F76FD
3D9.1C2C7F76FD
3D9.1C2D40F5CA
3D9.1C2D40F5CA
3D9.1C2D69E031
3D9.1C2D69E031
3D9.1C2D6B76BE
3D9.1C2D6B76BE
3D9.1C2D766509
3D9.1C2D766509
3D9.1C2DAA70C1
3D9.1C2DAA70C1
3D9.1C2DE79F26
3D9.1C2DE79F26
3D9.1C2DE8844B
3D9.1C2DE8844B

### 7-13-2022 ###

Pull off the run from mox; I know it has issues but it's our latest run

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-06-10-complete_det_hist/ ./

# Apparently the dummy fish edit didn't make it onto mox somehow, so we'll have to try again later

# Try this run again, now in a new folder

scp -r ./2022-07-14-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

### 7-14-2022 ###

Pull then run off mox:

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/ ./

Okay, so it didn't work AGAIN. Somehow the transfer keeps not going through and I'm left with the old script.

Try again:

scp ./03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/


### 7-15-2022 ###

Let's check on our run:

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/ ./

typo caused it to fail. Put it back on mox:
scp ./03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/

Running it again - submitted at 9:36, did not need to wait in queue

# Note: There seems to be a typo with the preceding script, where not every detection history is starting at BON adult ladders.
# probably going to need to re-run this (2022-05-24_det_hist) on mox

# I think that I made the fix - move to hyak and run
scp ./02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/

# The complete det hist script (implicit site visit) finished running. let's check it
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-14-complete_det_hist/ ./


### 7-16-2022 ###

Let's check how the first det hist script ran. Let's transfer only the output files, and into a new folder

in this directory: 
/from_hyak_transfer/2022-07-16_det_hist/

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/slurm-2878082.out ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/02_hyak_detection_histories_v2.R ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/complete_det_hist.csv ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-05-24_det_hist/complete_BON_arrival.csv ./

# Okay awesome, this looks good.


# Let's look at the implicit site visit code

# Editing in directory 2022-07-16-complete_det_hist
- note: transferred det hist file from 2022-07-16_det_hist/

Some bad tag codes/transitions:

- Entiat to RIS to RRE: 3D9.1C2D937BC6
	- Okay, so this is because there is a WEN detection after an ENT detection, and the jump between tribs code makes no sense at all
	- okay, i think I fixed it - a couple of issues, first fixing that if statement, then fixing the site order issue

- ICH to LGR to ICH to LGR:
384.3B23AB34F1
384.3B23ADC7F8
384.3B23AEB441
3D9.1C2C425108
3D9.1C2C7F76FD
3D9.1C2D40F5CA
3D9.1C2D69E031
3D9.1C2D6B76BE
3D9.1C2D766509
3D9.1C2DAA70C1
3D9.1C2DE79F26
3D9.1C2DE8844B
3D9.239F859CDF
3D9.239F86394C
3D9.257C5F7F5C
3D9.257C612E83
3DD.007762592F
3DD.007774D91A
3DD.0077885CFC
3DD.00778907A8
3DD.007790984B
3DD.0077915DA7

- ICH to LGR to BON to MCN:
384.3B23AB34F1
384.3B23ADC7F8
384.3B23AEB441
3D9.1C2C425108
3D9.1C2C7F76FD
3D9.1C2D40F5CA
3D9.1C2D69E031
3D9.1C2D6B76BE
3D9.1C2D766509
3D9.1C2DAA70C1
3D9.1C2DE79F26
3D9.1C2DE8844B
3D9.239F859CDF
3D9.239F86394C
3D9.257C5F7F5C
3D9.257C612E83
3DD.007762592F
3DD.007774D91A
3DD.0077885CFC
3DD.00778907A8
3DD.007790984B
3DD.0077915DA7

Note: these 22 codes are exactly the same.

- fixed all of these (I think) by moving that index flipping inside for loop. Same issue as in other places

- MCN to ICH or PRA to MCN to ICH or PRA
3D9.1BF20760D2
3D9.1C2CC2ABF3

- MCN to or PRA to MCN to mouth to BON:
3D9.1BF20760D2
3D9.1C2CC2ABF3

- fixed all of these (I think) by moving that index flipping inside for loop. Same issue as in other places. I think it also fixed some other codes in other problem areas
- Double checked, and it fixed every problem except the one NA! hooray

Note: these 2 codes are the same

- PRA to RIS to BON to MCN:
384.36F2B495D7
384.3B239B285B
3D9.1BF17E2835
3D9.1BF1AE038E
3D9.1BF26B4761
3D9.1C2C50466B
3D9.1C2CDBEE86
3D9.1C2D0B7741
3D9.1C2D1F8356
3D9.1C2D27FD3B
3D9.1C2D2DAB43
3D9.1C2D423772
3D9.1C2D6C08C5
3D9.1C2D800E0D
3D9.1C2DCADF77
3D9.1C2DCBCAD5
3D9.1C2DCDD3C3
3D9.257C619389
3D9.257C625600
3DD.003BAA5F5F
3DD.00773D3F69
3DD.0077908190
3DD.0077908A9B

- PRA to RIS to mouth to BON:
384.36F2B4DBCC
384.3B23AC7F5A
384.3B23B229DD
3D6.000B42DECD
3D9.1BF20767C5
3D9.1BF2091D48
3D9.1BF26B06CB
3D9.1BF26B408E
3D9.1C2C515F2A
3D9.1C2C58A2F4
3D9.1C2CBE8327
3D9.1C2CF60271
3D9.1C2CF7ED7D
3D9.1C2D037F64
3D9.1C2D0995E1
3D9.1C2D218378
3D9.1C2D425ABE
3D9.1C2D45C98F
3D9.1C2D6B3294
3D9.1C2D7F1445
3D9.1C2D937BC6
3D9.1C2DCD275D
3D9.1C2DD00683
3D9.1C2DD731BF
3D9.1C2DDAC3C7
3D9.1C2DDC7D02
3D9.1C2DF79813
3D9.257C632019
3DD.00776B6B05
3DD.00778D36C8
3DD.0077906777
3DD.007790D47F
3DD.00779F99C6
3DD.00779FA9E8
3DD.0077A04794
3DD.0077A534EF
3DD.0077A6361B
3DD.0077CEB283
3DD.0077D5E12E
3DD.0077D68DE8

- PRA to RIS to PRA to RIS
384.36F2B495D7
384.3B239B285B
3D9.1BF17E2835
3D9.1BF1AE038E
3D9.1BF26B4761
3D9.1C2C50466B
3D9.1C2CDBEE86
3D9.1C2D0B7741
3D9.1C2D1F8356
3D9.1C2D27FD3B
3D9.1C2D2DAB43
3D9.1C2D423772
3D9.1C2D6C08C5
3D9.1C2D800E0D
3D9.1C2DCADF77
3D9.1C2DCBCAD5
3D9.1C2DCDD3C3
3D9.257C619389
3D9.257C625600
3DD.003BAA5F5F
3DD.00773D3F69
3DD.0077908190
3DD.0077908A9B

Note: There is a lot of overlap in these tags from the above three lists

- RIS to RRE to BON to MCN
3D9.1BF1CD848E
3D9.1BF2076DD1
3D9.1BF25FEB8A
3D9.1C2D6B2072


- RIS to RRE to MCN to ICH or PRA
384.3B23987601
384.3B23AC6E25
3D9.1BF1891058
3D9.1BF2014E53
3D9.1BF207252D
3D9.1BF26DBC89
3D9.1C2C2D0878
3D9.1C2C422ABE
3D9.1C2CCD332F
3D9.1C2D926486
3D9.1C2DD6F2D1
3D9.1C2DD727C3
3DD.007790A633

- RIS to RRE to mouth to BON
384.3B23987601
384.3B23AC6E25
3D9.1BF1891058
3D9.1BF2014E53
3D9.1BF207252D
3D9.1BF26DBC89
3D9.1C2C2D0878
3D9.1C2C422ABE
3D9.1C2CCD332F
3D9.1C2D926486
3D9.1C2DD6F2D1
3D9.1C2DD727C3
3DD.007790A633

Note: a lot of overlap in the above three lists


- RRE to WEL to BON to MCN:
3D6.000B38AF91
3D6.000B42D7BA
3D9.1BF189225E
3D9.1BF1896393
3D9.1BF1897517
3D9.1BF189781E
3D9.1BF18978AC
3D9.1BF189842F
3D9.1BF18AB011
3D9.1BF18B0B96
3D9.1BF18BD51C
3D9.1BF18BDAFA
3D9.1BF18BF464
3D9.1BF18C1379
3D9.1BF18C586D
3D9.1BF18C77C7
3D9.1BF18C9AC3
3D9.1BF18C9CC2
3D9.1BF18CBA38
3D9.1BF18CC78A
3D9.1BF18D8403
3D9.1BF18DFA2E
3D9.1BF18E3E0A
3D9.1BF18E9C55
3D9.1BF199C3B4
3D9.1BF1C12F70
3D9.1BF1C2551C
3D9.1BF1C2AD8E
3D9.1BF1CDF773
3D9.1BF1D6EB1C
3D9.1BF1D76AF9
3D9.1BF1DA436C
3D9.1BF1DB3F43
3D9.1BF2022683
3D9.1BF202FF6F
3D9.1BF2074DDF
3D9.1BF2085CCC
3D9.1BF208F95F
3D9.1BF209A0A5
3D9.1BF209BA07
3D9.1BF20A2E9D
3D9.1BF20A69B2
3D9.1BF20A71C3
3D9.1BF20A81A2
3D9.1BF20A8DF7
3D9.1BF20ABED1
3D9.1BF20AD5E3
3D9.1BF20B16E2
3D9.1BF20B2286
3D9.1BF20B2FF5
3D9.1BF20B783F
3D9.1BF20B9030
3D9.1BF20B9EEF
3D9.1BF20BA6FA
3D9.1BF20C2602
3D9.1BF20D0B19
3D9.1BF20D260B
3D9.1BF20DB0EF
3D9.1BF26DADEC
3D9.1C2CB10926
3D9.1C2D027E57
3D9.1C2D60D680
3D9.257C5C63AF
3D9.257C62AE2D
3DD.003BE51618
3DD.00773BF530
3DD.0077AE819E


- RRE to WEL to MCN or ICH or PRA:
3D6.000B38AF91
3D6.000B42D7BA
3D9.1BF189225E
3D9.1BF1896393
3D9.1BF1897517
3D9.1BF189781E
3D9.1BF18978AC
3D9.1BF189842F
3D9.1BF18AB011
3D9.1BF18B0B96
3D9.1BF18BD51C
3D9.1BF18BDAFA
3D9.1BF18BF464
3D9.1BF18C1379
3D9.1BF18C586D
3D9.1BF18C77C7
3D9.1BF18C9AC3
3D9.1BF18C9CC2
3D9.1BF18CBA38
3D9.1BF18CC78A
3D9.1BF18D8403
3D9.1BF18DFA2E
3D9.1BF18E3E0A
3D9.1BF18E9C55
3D9.1BF199C3B4
3D9.1BF1C12F70
3D9.1BF1C2551C
3D9.1BF1C2AD8E
3D9.1BF1CDF773
3D9.1BF1D6EB1C
3D9.1BF1D76AF9
3D9.1BF1DA436C
3D9.1BF1DB3F43
3D9.1BF2022683
3D9.1BF202FF6F
3D9.1BF2074DDF
3D9.1BF2085CCC
3D9.1BF208F95F
3D9.1BF209A0A5
3D9.1BF209BA07
3D9.1BF20A2E9D
3D9.1BF20A69B2
3D9.1BF20A71C3
3D9.1BF20A81A2
3D9.1BF20A8DF7
3D9.1BF20ABED1
3D9.1BF20AD5E3
3D9.1BF20B16E2
3D9.1BF20B2286
3D9.1BF20B2FF5
3D9.1BF20B783F
3D9.1BF20B9030
3D9.1BF20B9EEF
3D9.1BF20BA6FA
3D9.1BF20C2602
3D9.1BF20D0B19
3D9.1BF20D260B
3D9.1BF20DB0EF
3D9.1BF26DADEC
3D9.1C2CB10926
3D9.1C2D027E57
3D9.1C2D60D680
3D9.257C5C63AF
3D9.257C62AE2D
3DD.003BE51618
3DD.00773BF530
3DD.0077A5FFA0
3DD.0077AE819E

- RRE to WEL to mouth to BON:
3D6.000B38AF91
3D6.000B42D7BA
3D9.1BF189225E
3D9.1BF1896393
3D9.1BF1897517
3D9.1BF189781E
3D9.1BF18978AC
3D9.1BF189842F
3D9.1BF18AB011
3D9.1BF18B0B96
3D9.1BF18BD51C
3D9.1BF18BDAFA
3D9.1BF18BF464
3D9.1BF18C1379
3D9.1BF18C586D
3D9.1BF18C77C7
3D9.1BF18C9AC3
3D9.1BF18C9CC2
3D9.1BF18CBA38
3D9.1BF18CC78A
3D9.1BF18D8403
3D9.1BF18DFA2E
3D9.1BF18E3E0A
3D9.1BF18E9C55
3D9.1BF199C3B4
3D9.1BF1C12F70
3D9.1BF1C2551C
3D9.1BF1C2AD8E
3D9.1BF1CDF773
3D9.1BF1D6EB1C
3D9.1BF1D76AF9
3D9.1BF1DA436C
3D9.1BF1DB3F43
3D9.1BF2022683
3D9.1BF202FF6F
3D9.1BF2074DDF
3D9.1BF2085CCC
3D9.1BF208F95F
3D9.1BF209A0A5
3D9.1BF209BA07
3D9.1BF20A2E9D
3D9.1BF20A69B2
3D9.1BF20A71C3
3D9.1BF20A81A2
3D9.1BF20A8DF7
3D9.1BF20ABED1
3D9.1BF20AD5E3
3D9.1BF20B16E2
3D9.1BF20B2286
3D9.1BF20B2FF5
3D9.1BF20B783F
3D9.1BF20B9030
3D9.1BF20B9EEF
3D9.1BF20BA6FA
3D9.1BF20C2602
3D9.1BF20D0B19
3D9.1BF20D260B
3D9.1BF20DB0EF
3D9.1BF26DADEC
3D9.1C2CB10926
3D9.1C2D027E57
3D9.1C2D60D680
3D9.257C5C63AF
3D9.257C62AE2D
3DD.003BE51618
3DD.00773BF530
3DD.0077AE819E

- RRE to WEL to PRA to RIS:
3D9.1BF15FA3DD
3D9.1BF16285AE
3D9.1BF172E833
3D9.1BF17E5D1D
3D9.1BF18AFEFA
3D9.1BF18C45D3
3D9.1BF18DA427
3D9.1BF18DB800
3D9.1BF18E73BE
3D9.1BF18E80F5
3D9.1BF192A467
3D9.1BF19BD855
3D9.1BF1CDE88A
3D9.1BF1D5B9D8
3D9.1BF1D675CC
3D9.1BF1DA7373
3D9.1BF20221E8
3D9.1BF207FAE8
3D9.1BF20873EC
3D9.1BF20B227B
3D9.1BF20C2FA4
3D9.1BF20C3182
3D9.1BF20CDF0C
3D9.1BF20CEEB0
3D9.1BF20D17BC
3D9.1BF25BB9F3
3D9.1C2C44AC6A
3D9.1C2C84BF85
3D9.1C2D3D8D9D
3D9.1C2D462DA2
3D9.257C5A3B05
3D9.257C67123A
3DD.003BA4B9DC

Note: lots of overlap in above four lists


- upstream LGR to NA
3DD.0077E5598A
- This is fine - it's just because it's the last one in the dataset


- Methow River to RRE to WEL:
384.3B23AC6E25
3D9.1C2C3DD368
3D9.1C2C501C8E
3D9.1C2C51CD86
3D9.1C2D125BF9
3D9.1C2D423C24
3D9.1C2D9303CC
3D9.1C2DD6A680
3D9.1C2DD6D852
3D9.1C2DD731BF
3D9.1C2DDA30C0


# run it!

scp -r ./2022-07-16-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# fix quick typo, just reupload the R script

scp ./2022-07-16-complete_det_hist/03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16-complete_det_hist/

# bug fix, reupload R script, re-run on MOX

scp ./2022-07-16-complete_det_hist/03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16-complete_det_hist/


### 7-17-2022 ###

- pull run off of mox, inspect

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16-complete_det_hist/ ./

It looks beautiful!!!


### 7-18-2022 ###

Dang, so there's a bug where we have some NA end times - this messes up interpolating the site visit times
- I'll re-run the states_complete script with that bug fixed, but for now we can use BON arrival times to tweak it
- Okay, so the issue is really in the previous script, where we're not getting end times for site visits that have a single detection (but only at BON adult ladders)

# re-upload the det_hist script and re-run
scp -r ./2022-07-16_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/


# OKAY - let's see if we can get stan running on mox
- So cmdstanr package will load, but then the stan model completely explodes
- I'm not sure we have cmdstan (no r) installed, which is required - https://mc-stan.org/cmdstanr/articles/cmdstanr.html
- So it sounds like our compiler is too old? I think we ran into this issue before
	- can we load any of the available gcc modules?
		- yes we can, but just using module load doesn't fix it

gcc/6.3.1                                                           
           gcc/8.2.1                                                           
                                gcc/10.1.0                                                          
                        gcc_4.8.5-impi_2017                                                 
                            gcc_8.2.1-ompi_3.1.4                                                
                          gcc_8.2.1-ompi_4.0.1  
                          
                          contrib/gcc-8.3.0                                                      
contrib/gcc-9.3.0                                  
contrib/gcc/6.2.0                                  
contrib/gcc/6.2.0_impi                             
contrib/gcc/6.2.0_mpich-3.2                        
contrib/gcc/6.3.0                                  
contrib/gcc49/4.9.4                                
contrib/gcc_9.3.0_ompi_4.0.7                       
contrib/gcc_10.1.0                                 
contrib/gcc_10.1.0_ompi_4.0.3                      
contrib/gcc_cesg/9.1.0   

# pull the run off mox
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-16_det_hist/ ./

- move it into the complete_det_hist folder for today
scp -r ./2022-07-18-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

### 2022-07-19 ###

pull the latest complete det hist script off mox

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-18-complete_det_hist/ ./

Okay, so there's still some issues with this. Made the fix, reupload & run:

scp ./2022-07-18-complete_det_hist/03_hyak_complete_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-18-complete_det_hist/

### 2022-07-20 ### 
# Pull it back off:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-18-complete_det_hist/ ./

Wow okay, so this is terrible. Just super messed up, especially for the tributaries

# Okay, I think I made the bug fix. Reupload to mox
scp -r ./2022-07-19-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

### 2022-07-21 ###
# Pull run off of mox
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-19-complete_det_hist/ ./

This was the fish we were trying to fix: 3D9.1C2C430C8D
- It appears that we were able to fix this issue, so that's good!

Okay, so there are still some issues with this file, but it's much closer. Also, the
changes are clearly taking effect because we now have more mainstem transitions (which
is due to the script now making that fix for making sure that fish are seen in the downstream
states before ascending the adult ladder, even if they're actually in the right state to
enter a tributary - see edits marked 2022-07-19 in 03 script)

Here are the problem tags:

Asotin Creek to RIS to RRE: 
3D9.1BF27C4DF9
3D9.1C2D6A1268
3D9.1C2D6F4187
3D9.1C2D8288E5
3D9.1C2DF614B4
- This is a weird issue, because that's a Columbia mainstem site. I think somehow that we're
selecting the wrong order of sites somewhere in the code.

Clearwater River to RIS to RRE:
3D9.1C2C571E86
3D9.1C2DF53809
3DD.0077A8D748
- Same weird thing as above

Tucannon River to PRA to RIS:
- Same issue as above, just one state earlier in the order
3D9.1BF26D9E1A
3D9.1C2C3F724F
3D9.1C2C4C5CD3
3D9.1C2C4F4C33
3D9.1C2C517040
3D9.1C2DD35628
3D9.1C2DD9566D


This individual: 3D9.1BF27C4DF9
- Really looks like maybe moving downstream through the adult ladder at LGR? But I don't think
we say that for sure. Need to stick with the assumption that for adults, the ladders are unidirectional

Made the edits, reupload to mox hyak & run:
scp -r ./2022-07-21-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Pull run
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-21-complete_det_hist/ ./

### 7-27-22 ###

I would estimate that with the new script and its additional components, running the full detection history in
one run would take about 7 hours. That could easily be run overnight, but would have to be on STF.
There's no reason why we couldn't run the chunks of it separately, and that would allow us to run it
on ckpt. We could run CTH 1-4, 5-8, and 9-11, and 12-14. Each individually should take no more
than 2 hours.

# SO: we are going to take the files in 2022-07-27_det_hist, split them up into four runs, and put
an R script and a bash script in each.

We can then do the same thing with the implicit site visit code later. The implicit site visit
code will obviously have to be updated, given that some things have changed, particularly
the addition of non-ascents at the adult ladders. We'll also have to concatenate the other output
files, but that's not a big deal.

### 7-28-22 ###

So I think we now have our det hist script working. copy over the folders:
scp -r ./2022-07-27_det_hist/CTH1-4/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/
scp -r ./2022-07-27_det_hist/CTH5-8/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/
scp -r ./2022-07-27_det_hist/CTH9-11/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/
scp -r ./2022-07-27_det_hist/CTH12-14/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/

for fixing the R scripts:
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

### 7-29-22 ###

# Pull the scripts off mox, and join them

current directory: from_hyak_transfer/

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

# okay, made some fixes to the R script, reupload:
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# re-run the slurm scripts

# pull the runs
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

# Bug fixes
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/


### 8-4-2022 ###

Pull the runs with the latest scripts
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./


### 8-8-2022 ###
Fixed a pretty silly typo, reupload and re-run R scripts
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# pull runs off hyak

# run the implicit site visit script
scp -r ./2022-08-08-complete_det_hist/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_stf.slurm

# stf nodes are down for maintenance, split the detection history in two and run with two separate R and slurm scripts

scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm

### 8-9-2022 ###

pull runs off hyak
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-08-08-complete_det_hist_ckpt/ ./

Undone by a small bug! I fixed it. Split it into 4 to increase efficiency, then re-run it:
scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# So it looks like there was still a typo in the previous script, so I need to re-run:
scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# fix another typo, re-run AGAIN
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

# run implicit site visit code
somehow this is running really fast

Looks good, but in the future I might want to preserve more information in the states complete file

### 08-10-2022 ###

Once again, fixed a couple of typos and reuploaded scripts:

scp ./2022-07-27_det_hist/CTH1-4/CTH1-4_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH1-4/
scp ./2022-07-27_det_hist/CTH5-8/CTH5-8_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH5-8/
scp ./2022-07-27_det_hist/CTH9-11/CTH9-11_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH9-11/
scp ./2022-07-27_det_hist/CTH12-14/CTH12-14_02_hyak_detection_histories_v2.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/CTH12-14/

# pull it:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-07-27_det_hist/ ./

### 8-11-2022 ###

rerun implicit site visit code
scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# pull it
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-08-08-complete_det_hist_ckpt/ ./

### 8-12-2022 ###

# 1 more minor fix to implicit site visit code

scp -r ./2022-08-08-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# pull it
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-08-08-complete_det_hist_ckpt/ ./

# getting stan up on mox - attempt 3?

So far, we've found that we can't install cmdstan because the compiler is too old. 

# my last notes:

# OKAY - let's see if we can get stan running on mox
- So cmdstanr package will load, but then the stan model completely explodes
- I'm not sure we have cmdstan (no r) installed, which is required - https://mc-stan.org/cmdstanr/articles/cmdstanr.html
- So it sounds like our compiler is too old? I think we ran into this issue before
	- can we load any of the available gcc modules?
		- yes we can, but just using module load doesn't fix it

gcc/6.3.1                                                           
           gcc/8.2.1                                                           
                                gcc/10.1.0                                                          
                        gcc_4.8.5-impi_2017                                                 
                            gcc_8.2.1-ompi_3.1.4                                                
                          gcc_8.2.1-ompi_4.0.1  
                          
                          contrib/gcc-8.3.0                                                      
contrib/gcc-9.3.0                                  
contrib/gcc/6.2.0                                  
contrib/gcc/6.2.0_impi                             
contrib/gcc/6.2.0_mpich-3.2                        
contrib/gcc/6.3.0                                  
contrib/gcc49/4.9.4                                
contrib/gcc_9.3.0_ompi_4.0.7                       
contrib/gcc_10.1.0                                 
contrib/gcc_10.1.0_ompi_4.0.3                      
contrib/gcc_cesg/9.1.0   

# okay, so today:
#first, get a build node:

# then, load R
> module load contrib/R-4.2.0/4.2.0
R

# then, load cmdstanr
> library(cmdstanr)
This is cmdstanr version 0.5.2
- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr
- CmdStan path: /usr/lusers/mmin/.cmdstan/cmdstan-2.30.0
- CmdStan version: 2.30.0

A newer version of CmdStan is available. See ?install_cmdstan() to install it.
To disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.

# try installing cmdstan
> install_cmdstan()
The C++ toolchain required for CmdStan is setup properly!
* Latest CmdStan release is v2.30.1
* Installing CmdStan v2.30.1 in /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1
* Downloading cmdstan-2.30.1.tar.gz from GitHub...
* Download complete
* Unpacking archive...
* Building CmdStan binaries...

# it then starts printing tons of stuff, which ends with:
Warning message:
There was a problem during installation. See the error message(s) above. 
# there are a ton of different errors listed

So GCC compiler is too old: https://discourse.mc-stan.org/t/cmdstan-installation-error/20329/4
[mmin@mox1 /]$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)

and we need at minimum gcc/g++ 4.9.3

# try and load those modules
module load contrib/gcc-9.3.0 
[mmin@mox1 /]$ gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)

# so this doesn't update the version
module load contrib/gcc/6.2.0 
[mmin@n2233 /]$ gcc --version
gcc (GCC) 6.2.0

# this appears to have worked!

# try installing stan again
R
library(cmdstanr)
install_cmdstan(overwrite = TRUE) # this is necessary because of our past failed installation attempts

# didn't work:

> install_cmdstan(overwrite = TRUE)
The C++ toolchain required for CmdStan is setup properly!
* Latest CmdStan release is v2.30.1
* Installing CmdStan v2.30.1 in /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1
* Downloading cmdstan-2.30.1.tar.gz from GitHub...
* Removing the existing installation of CmdStan...
* Download complete
* Unpacking archive...
* Building CmdStan binaries...
cp bin/linux-stanc bin/stanc
/sw/contrib/gcc-9.3.0/install/bin/g++ -pipe   -pthread -D_REENTRANT  -O3 -I stan/lib/stan_math/lib/sundials_6.1.1/include -I stan/lib/stan_math/lib/sundials_6.1.1/src/sundials -DNO_FPRINTF_OUTPUT     -O3  -c -x c -include stan/lib/stan_math/lib/sundials_6.1.1/include/stan_sundials_printf_override.hpp stan/lib/stan_math/lib/sundials_6.1.1/src/nvector/serial/nvector_serial.c -o stan/lib/stan_math/lib/sundials_6.1.1/src/nvector/serial/nvector_serial.o
bash: /sw/contrib/gcc-9.3.0/install/bin/g++: Permission denied
chmod +x bin/stanc
make: *** [stan/lib/stan_math/lib/sundials_6.1.1/src/nvector/serial/nvector_serial.o] Error 126
make: *** Waiting for unfinished jobs....

Warning message:
There was a problem during installation. See the error message(s) above. 
# let's try unloading the other gcc version
module unload contrib/gcc-9.3.0 

# now try same installation as before:
# installation worked!
* Finished installing CmdStan to /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1

CmdStan path set to: /usr/lusers/mmin/.cmdstan/cmdstan-2.30.1

# Will this be an issue given that it's not on gscratch?

# Let's try running our stan actual model
# from within the stan_actual folder:
scp -r ./01_int_only_mox/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
sbatch job_01_stan_actual_int_only_mox.slurm

# upload a ckpt version to see if it'll run, since we have to wait on stf
scp ./01_int_only_mox/job_ckpt_01_stan_actual_int_only_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/
# fixes to R script:
scp ./01_int_only_mox/01_stan_actual_int_only_mox.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/

### 8-14-2022 ###

# 1000 warmup, 1000 sampling didn't finish in 24 hours. Resubmitted job with 48 hour time limit, but to
make sure I have some results by tomorrow, ran it again with 500 warmup and 500 sampling
scp ./01_int_only_mox/01_stan_actual_int_only_mox_500iter.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/
scp ./01_int_only_mox/job_01_500iter_stan_actual_int_only_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/01_int_only_mox/

# Okay, trying to get the origin model to run. We have 16*54 + 54 parameters, and when we try to compile it says this:

To get around this, we need to try and change a flag, see this post: https://github.com/stan-dev/rstan/issues/395

cd ../../../../../../../../Users/markusmin/.cmdstan

(base) markusmin@Markuss-MBP .cmdstan % make CC="clang++ -O1 -fbracket-depth=2048" ~/var/folders/8c/vz24vqzd2nzfvknzmfcybj0m0000gn/T/RtmpyH5D8E/model-285540e6d8c

# this doesn't work

Instead, navigate to: Users/markusmin/.R

sudo nano Makevars.bck

Edit the text: 
CC=clang
CXX=clang++
change to: 
CC=clang -O1 -fbracket-depth=2048
CXX=clang++ -O1 -fbracket-depth=2048

This didn't work, changed it back

### 8-15-2022 ###

Moving ESU models to hyak and running:

# upper columbia
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
# snake
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
# middle columbia
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

### 8-16-2022 ###

The ESU models are still stuck in the stf queue. Let's try running middle columbia on ckpt, since it's thee smallest file
scp ./middle_columbia/middle_columbia_ckpt_job_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/middle_columbia/

# This got 1% through in 4 hours. That's really bad - moving to parallelize

# Models in parallel
So I wrote the model to run using reduce-sum. This will, theoretically, allow our model to run in about 1/28 the time by using all 28 available cores on a mox node.
HOWEVER - using the parallel-chains flag doesn't actually run different chains on different nodes, but rather only on the same node.
So we will have to figure out how to get the chains to run on separate nodes, or alternatively, to just run one chain at a time. One chain at a time
might be the move for now.

# Try running snake ESU model, with a single chain
scp ./snake/parallel_snake_02_stan_actual_int_origin_mox.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./snake/parallel_snake_02_stan_actual_int_origin.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./snake/job_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./snake/job_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
- only got through 30 iter on ckpt with 4 hours
- could try running this with 100 iter on stf with 24 hours... but that probably wouldn't finish in time for AFS talk recording

# Try running parallel middle columbia
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
- only got through 110 iter on ckpt with 4 hours
- let's try this one to make sure we get some results


# Try running parallel upper columbia
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
- only got through 70 iter on ckpt with 4 hours

### 8-18-2022 ###

For some reason, running these models in parallel on my laptop was faster than running them on mox. I think it might have to do with the seed - I should do 
a benchmark test at some point to make sure mox is faster as it should be.

# Try running parallel middle columbia on ckpt, 100 iter, with new derived movement probabilities in the stan code itsel.
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm

# whoops, looks like I saved the cmdstan files wrong. Need to change that code, and then re-run so that I can actually access the files 
- see https://discourse.mc-stan.org/t/saving-of-cmdstanmodel-objects-from-cmdstanr/16220/3

# Try running all files on ckpt
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
sbatch job_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm



# wow, okay so I think my slurm scripts were preventing my script from running in parallel:
https://wiki.cac.washington.edu/display/hyakusers/Mox_scheduler
the -ntasks flag needs to be the number of cores you want to use. I had it at 1. Dumb. Changed it to 28 (max cores). Also changed
MEM to 120 GB, which is the recommended memory for 128GB nodes

# These are so much faster. Some models finished in under an hour with 100 warmup and 100 sampling.
Middle Columbia model, with same seed - on mox with 28 cores, 4018 seconds. on laptop with 7 cores, 9510 seconds

Snake finished in 6605 seconds, which is a huge improvement over not finishing :)

upper columbia is the fastest, then middle, then snake

# pull runs off mox:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/middle_columbia/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/upper_columbia/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/ ./

# Reupload all with 200 burn in and 200 sampling, about the max of what will run on ckpt
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

sbatch job_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
sbatch job_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm

# also, remove all old binary stan files

# now pull them:

OKAY - so if you ever change the stan script, you have to remove the old binary file in the same folder. Otherwise, stan will look at the old file
and think oh cool, it's already compiled, no need to do that again! And then you'll run the old model executable, even though the code for the 
new model is in the same folder.

I'm going to also change the seeds, and run three chains of 200 iter + 200 warmup for each of the three models.
# move the new scripts with different seeds to run different chains to mox:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

# run all 9 scripts:
# FIRST - REMOVE THE BINARY FILE

# snake
sbatch job_seed101_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
sbatch job_seed102_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm
sbatch job_seed103_ckpt_parallel_snake_02_stan_actual_int_origin_mox.slurm

# middle columbia
sbatch job_seed101_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed102_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed103_ckpt_parallel_middle_columbia_02_stan_actual_int_origin_mox.slurm

# upper columbia
sbatch job_seed101_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed102_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm
sbatch job_seed103_ckpt_parallel_upper_columbia_02_stan_actual_int_origin_mox.slurm


### 8-19-2022 ###
# Pull these runs
# damn, none of the snake runs finished in time - only got to 320 out of 400.

### 9-6-2022 ###
Run the snake runs on stf instead
scp ./job_seed101_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./job_seed102_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/
scp ./job_seed103_stf_parallel_snake_02_stan_actual_int_origin_mox.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/


### 9-7-2022 ###
# pull the new stf runs
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/snake/ ./

### 10-28-22 ###
What I'm doing now is re-running the complete states code, where the river mouth and upstream sites are now called different states.
This is necessary to estimate detection efficiency in the stan model.

# Transfer the v4 script to hyak:
scp -r ./2022-10-27-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/

# Run em all
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm

# Looks like everything ran fine. Pull the runs:

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-10-27-complete_det_hist_ckpt/ ./

### 11-02-2022 ###
Found an issue with the 03 script where upstream and river mouth detections weren't being recorded separately. Made fix to scripts, now reupload:
- I just renamed the old folder from 10-27 to 11-02

# Transfer updated scripts and run:
scp -r ./2022-11-02-complete_det_hist_ckpt/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/
# Run em all
sbatch complete_det_hist_ckpt_part1.slurm
sbatch complete_det_hist_ckpt_part2.slurm
sbatch complete_det_hist_ckpt_part3.slurm
sbatch complete_det_hist_ckpt_part4.slurm


# Pull the runs:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/2022-11-02-complete_det_hist_ckpt/ ./


### 11-12-2022 ###

Running DE model on mox hyak (ckpt first for troubleshooting)

# move model files to hyak
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# run it on ckpt:
sbatch job_ckpt_parallel_snake_03_DE.slurm

# submit it on stf too, in case it actually runs:
scp -r ./snake/job_stf_parallel_snake_03_DE.slurm mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/

# make a few tweaks to the number of iter in the R script, then move it back

scp -r ./snake/parallel_snake_03_stan_actual_int_origin_mox_deteff.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/


### 11-13-2022 ###

The model is running on my laptop!

Delete the old folder on hyak: rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# current iter is only 50 warmup and 50 sampling, but should give us a sense of how long it'll take to run in its current formulation

# stf is currently totally gummed up
sbatch job_stf_parallel_snake_03_DE.slurm

# run it on ckpt too
sbatch job_ckpt_parallel_snake_03_DE.slurm

CKPT run finished really quickly, but all divergent transitions:
````
[1] "2022-11-13 13:20:05 PST"
Running MCMC with 1 chain, with 28 thread(s) per chain...

Chain 1 WARNING: There aren't enough warmup iterations to fit the
Chain 1          three stages of adaptation as currently configured.
Chain 1          Reducing each adaptation stage to 15%/75%/10% of
Chain 1          the given number of warmup iterations:
Chain 1            init_buffer = 7
Chain 1            adapt_window = 38
Chain 1            term_buffer = 5
Chain 1 Iteration:  1 / 100 [  1%]  (Warmup)

Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpiETiWp/model-245c5415e0c8.stan', lin$
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpiETiWp/model-245c5415e0c8.stan', lin$
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Iteration: 10 / 100 [ 10%]  (Warmup)
Chain 1 Iteration: 20 / 100 [ 20%]  (Warmup)
Chain 1 Iteration: 30 / 100 [ 30%]  (Warmup)
Chain 1 Iteration: 40 / 100 [ 40%]  (Warmup)
Chain 1 Iteration: 50 / 100 [ 50%]  (Warmup)
Chain 1 Iteration: 51 / 100 [ 51%]  (Sampling)
Chain 1 Iteration: 60 / 100 [ 60%]  (Sampling)
Chain 1 Iteration: 70 / 100 [ 70%]  (Sampling)
Chain 1 Iteration: 80 / 100 [ 80%]  (Sampling)
Chain 1 Iteration: 90 / 100 [ 90%]  (Sampling)
Chain 1 Iteration: 100 / 100 [100%]  (Sampling)
Chain 1 finished in 758.2 seconds.
Warning: 50 of 50 (100.0%) transitions ended with a divergence.
See https://mc-stan.org/misc/warnings for details.
````

Those first simplex related warnings seem not a big deal, since they only occur at the start of the chain during the warmup phase. 
But I'm not sure why you'd get any "-nan" values when summing the probabilities parameter... so might be worth looking into.

# Let's pull the run so we can look at the posteriors
# From within the diagnostics folder:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./
# Looks like these chains didn't move at all (no chains finished successfully)

# Set seed to 123, bumped adapt_delta to 0.85, increased chains to 200 warmup + 200 iter. So let's see if any of that helps.
scp ./parallel_snake_03_stan_actual_int_origin_mox_deteff.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/

# more likely though it's a model issue. But let's just try this first.

# okay, so results are not good:
Chain 1 Iteration:   1 / 400 [  0%]  (Warmup)
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpiETiWp/model-245c5415e0c8.stan', lin$
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Iteration:  10 / 400 [  2%]  (Warmup)
Chain 1 Iteration:  20 / 400 [  5%]  (Warmup)
Chain 1 Iteration:  30 / 400 [  7%]  (Warmup)
Chain 1 Iteration:  40 / 400 [ 10%]  (Warmup)
Chain 1 Iteration:  50 / 400 [ 12%]  (Warmup)
Chain 1 Iteration:  60 / 400 [ 15%]  (Warmup)
Chain 1 Iteration:  70 / 400 [ 17%]  (Warmup)
Chain 1 Iteration:  80 / 400 [ 20%]  (Warmup)
Chain 1 Iteration:  90 / 400 [ 22%]  (Warmup)
Chain 1 Iteration: 100 / 400 [ 25%]  (Warmup)
Chain 1 Iteration: 110 / 400 [ 27%]  (Warmup)
Chain 1 Iteration: 120 / 400 [ 30%]  (Warmup)
Chain 1 Iteration: 130 / 400 [ 32%]  (Warmup)
Chain 1 Iteration: 140 / 400 [ 35%]  (Warmup)
Chain 1 Iteration: 150 / 400 [ 37%]  (Warmup)
Chain 1 Iteration: 160 / 400 [ 40%]  (Warmup)
Chain 1 Iteration: 170 / 400 [ 42%]  (Warmup)
Chain 1 Iteration: 180 / 400 [ 45%]  (Warmup)
Chain 1 Iteration: 190 / 400 [ 47%]  (Warmup)
Chain 1 Iteration: 200 / 400 [ 50%]  (Warmup)
Chain 1 Iteration: 201 / 400 [ 50%]  (Sampling)
Chain 1 Iteration: 210 / 400 [ 52%]  (Sampling)
Chain 1 Iteration: 220 / 400 [ 55%]  (Sampling)
Chain 1 Iteration: 230 / 400 [ 57%]  (Sampling)
Chain 1 Iteration: 240 / 400 [ 60%]  (Sampling)
Chain 1 Iteration: 250 / 400 [ 62%]  (Sampling)
Chain 1 Iteration: 260 / 400 [ 65%]  (Sampling)
Chain 1 Iteration: 270 / 400 [ 67%]  (Sampling)
Chain 1 Iteration: 280 / 400 [ 70%]  (Sampling)
Chain 1 Iteration: 290 / 400 [ 72%]  (Sampling)
Chain 1 Iteration: 300 / 400 [ 75%]  (Sampling)
Chain 1 Iteration: 310 / 400 [ 77%]  (Sampling)
Chain 1 Iteration: 320 / 400 [ 80%]  (Sampling)
Chain 1 Iteration: 330 / 400 [ 82%]  (Sampling)
Chain 1 Iteration: 340 / 400 [ 85%]  (Sampling)
Chain 1 Iteration: 350 / 400 [ 87%]  (Sampling)
Chain 1 Iteration: 360 / 400 [ 90%]  (Sampling)
Chain 1 Iteration: 370 / 400 [ 92%]  (Sampling)
Chain 1 Iteration: 380 / 400 [ 95%]  (Sampling)
Chain 1 Iteration: 390 / 400 [ 97%]  (Sampling)
Chain 1 Iteration: 400 / 400 [100%]  (Sampling)
Chain 1 finished in 9064.3 seconds.
Warning: 198 of 200 (99.0%) transitions ended with a divergence.
See https://mc-stan.org/misc/warnings for details.

Warning: 2 of 200 (1.0%) transitions hit the maximum treedepth limit of 10.
See https://mc-stan.org/misc/warnings for details.

# 198 of 200 transitions ended with a divergence, and the two that didn't hit a the maximum treedepth limit.
# this isn't as concerning, since this is an efficiency issue. Divergent transitions definitely are though.

# I found one typo in the model. I fixed it, now reupload the folder
- I deleted the old snake folder, then re-upload
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# then resubmit the ckpt run

### 11-14-2022 ###

This unfortunately doesn't look like it fixed the issue. Pull the run to the diagnostics folder
# From within the diagnostics folder:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./
# rename run as snake3

# All divergent transitions

Alright, I found one indexing typo with Fifteenmile Creek. Maybe that fixes it?

I also removed all upstream -> RM and RM -> upstream transitions from the transition matrix.
I also removed a bunch of upstream states that no fish have ever been in (in our states_complete data, which we subset to
remove upstream states in DE years)
I also made a temporary change for the Yakima, where we remove run year 04/05 from the DE years. That's because
we don't currently have discharge data for that year, so it'll just get confused.

Alright, let's try moving the run again

# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# snake 5 has printed p_vec - but that doesn't seem to be the issue, since sum p_vec_observed is always 1


# Okay - after meeting with Mark, I'm going to try multiple runs, only 100 iter each, with different
stan run settings (adapt_delta, step_size) and see if we can get something to work.

adapt_delta = 0.95
adapt_delta = 0.9
step_size = 0.5
step_size = 1

Made a new folder called snake2022-11-14-1436 to store all of these permutations
# move it to hyak
scp -r ./snake2022-11-14-1436/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake2022-11-14-1436/ ./
Didn't work. Let's look again in the model for errors.

# Ok, line 270 has an indexing typo - everything is off by one.
run_year_indices_vector[1:(i-1)] = n_obs[1:i-1];
# Change it to this:
run_year_indices_vector[1:(i-1)] = n_obs[1:(i-1)];

# Try uploading and running this new model, with only 50 iter
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Still getting lots of divergences. Pull the run, rename it as snake6
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# Made a couple of changes to model code - removed imnaha and fifteenmile betas entirely, fixed those values in det_eff_param_vector to zero.
# Don't think that was the issue but probably good to do anyway.
# I also took out some of the transitions that we no longer observed (just upstream states) from the transformed parameters block. But I also 
# don't think that was the issue.

# I can't find any other glaring issues. Let's just try running the model in its current configuration.
# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# things we can still try: 
1) fix all det eff beta terms in the param vector to their posterior medians
2) fix all det eff alpha terms to their posterior medians
3) fix all det eff beta terms to zero (so in theory would ignore discharge)

# might help us eliminate different possible problems

### 11-15-2022 ###
There was a typo that I fixed. Reupload and run.
# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# This run also has divergent transitions. Pull it and rename as snake7
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# Okay, I found an indexing error in the transition_run_years vector. It was using a version of the
states_complete df that wasn't the final one, so I moved the creation of the vector to right before
the data call. So that should fix that issue.Reupload and run.
# First remove old run
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Pull the new run, rename as snake8
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Still 100% divergent transitions. Frick!!

Okay, so for this run, I removed all beta terms - took them out of the parameters block, set them to zero in transformed parameters, commented out priors block

Try running this:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

### 11-16-21 ###
# Pull the new run, rename as snake9
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

All still divergent transitions, even without discharge in there.
Comment out the priors on alpha terms, and remove them as parameters. In the transformed parameters block, assign them directly their mean values
from the last script.

Try running this:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Pull the new run, rename as snake10
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

98/100 transitions ended in a divergence. But we didn't get the message at the beginning that sum(p_vec_observed) is -nan. So
it seems like perhaps that value was related to sampling from priors on those alpha terms?

Can't figure out what's wrong. Turned on all of the print statements again. Also added print run year statement.
Also added print current state. Also moved run year indexing outside of k loop (should help with efficiency, but won't fix the bug)

Try running this:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Ran it for 30 minutes, now pull it and rename as snake11
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Not immediately clear if there are any issues. Updated the print statements, reupload and re-run
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# didn't run, need to reupload and run again

# pull it! and rename as snake12
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, this new print statement is super helpful. There are a bunch of transitions where the probability of the next state is either 0 or 1.

Another odd thing: It has multiple times where it goes back to the same fish (searching for i = 1 in the output reveals multiples times).
Is that a warmup thing?
Yeah I think so - this from Bob Carpenter: "There's a description in the part of the manual about algorithms. We 
recommend running at least 100 warmup iterations. Usually the first 
few don't do much because the scale's off. Then they hit increments where 
step size is fit, and the result is often a perceived hang on Stan's part 
because it moves to doing way more steps per iteration to get stability. "

There are some infrequent probabilities that get close to zero or 1. Those should be fine. What's concerning
is that there are long stretches of iterations, it seems like starting with i = 1,  that keep returning 1 or 0 probabilities.


# Okay - added a print(logits) statement to stan model, now reupload and run
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# pull it! and rename as snake 13
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# Okay, clearly an isue with logits. Added another print statement to print b0_matrix itself.

Run the new run after uploading

### 11-20-2022 ###


Pull the latest run and rename as snake14
# pull it: (from within local diagnostics folder:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, I found an issue in the stan code, where the return(total_lp) call was in the wrong loop.
Moved it to the correct loop, re-upload and run.
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/



### 11-22-2022 ###

After meeting with Mark and Rebecca, decided to make two changes for troubleshooting:
1) set det_eff = 1 in loop, and 2) set inits = 0. Upload and run:
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

### 11-28-2022 ###



# Pull latest run, rename it as snake15
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, so with inits set to 0, I can now see that the problem starts when we move off of the inits.
Until that point, every value is set to 0 in each iteration. Once it starts sampling (?), the
values immediately move to really crazy values and get stuck there. Then it eventually goes back
to all zeros, once it moves back to the first fish.

Okay, so I found a spot where the return(total_lp) was in the wrong loop. Fixed that and reuploaded.

# Pull the run, rename it as snake16
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

So in this run I don't see the values ever moving off the initial values, but they also don't ever go
to zero or one. So that's perhaps an improvement?

In the next version of the stan model, I commented out all of the print statements to allow it to actually run.

Reuploaded. Then run on ckpt and stf.

### 11-29-2022 ###

Only the ckpt run ran overnight - stf run still waiting in the queue.

# Pull it and rename as snake17
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Okay, so it only got through 40 iterations in 4 hours. That is way slower than the model with bugs,
which might actually be a good sign. However, it's way slower than the original model without detection
efficiency, and given that the current version doesn't even have detection efficiency, that's not a great sign.

# Okay so I commented out all of the detection efficiency loops. Let's see how much that improves performance.

Removed old folder, reuploaded. Then run on ckpt
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

# Pull run, rename it as snake18
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

Still completed 40 iter in 4 hours. So commenting out det eff blocks didn't change anything.

Bumped up stf run time to 48 hours, submitted it.

Create a new run folder, to run 40 iter (20 warmup and 20 sampling) on ckpt so we can see how it looks.

scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake40iter/

# Pull the run, rename as snake19
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake40iter/ ./

This run didn't have any divergent transitions! We just get the maximum treedepth issue.

So, let's try running a 40iter version with det eff

I canceled the STF run that was in the queue.

Removed old folder, reuploaded. Then 40iter, with deteff (but fixed values, no priors, and only intercept and therefore no relationship with discharge), run on ckpt
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

### 2022-11-30 ###

Pull the latest run, rename as snake20
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./


40 iter.
This run worked. Run time went up, but only very minor amount.
Last run, without det eff: Chain 1 finished in 10864.8 seconds.
This run, with det eff (but these not included as parameters): Chain 1 finished in 11176.9 seconds.
So like five minutes longer

Looking at modeling results.

Made some edits, added back in det eff parameters (no longer fixed, but using priors on the alphas, still no betas). Then upload and run.
# First remove old run from mox
rm -r snake/

Move the new folder onto hyak:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/

Simultaneously, we're going to run the next version of the model on mox hyak - using betas and discharge data.

Made a directory called snake_alpha_beta for this run.
Move the new folder onto hyak into that directory:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_alpha_beta/

Pull both of the above runs:

# The run with det eff parameters - alphas only, no betas:
# Pull and rename as snake21:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake/ ./

# The run with det eff parameters - alphas and betas:
# Pull and rename as snake22:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_alpha_beta/snake/ ./

Okay, so both of these runs finished in about the same time as models without detection probability (this doesn't completely
make sense to me, since aren't you monitoring more parameters?):
snake 21: Chain 1 finished in 11012.5 seconds.
snake 22: Chain 1 finished in 11853.8 seconds.

However, both have the not a valid simplex issue:
Chain 1 Iteration:  1 / 40 [  2%]  (Warmup) 
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/RtmpeUJp9k/model-790b48db88cc.stan', line 303, column 8 to column 81) (in '/tmp/RtmpeUJp9k/model-790b48db88cc.stan', line 1405, column 2 to line 1408, column 98)
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.

But no divergent transitions.

Inspecting the chains in shinystan:

snake21 (alphas, not betas) looks fine!
snake22 (alphas and betas) looks fine as well!


Next model run:
There appears to be no queue on stf right now, so should submit some jobs!
snake22 is the full model. Let's run it for 100, 200, and 500 sampling iterations.

# Just keep changing the number of iterations in the R script and then reupload
# 100 iter:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake100iter/

# 200 iter:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake200iter/

# 500 iter:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake500iter/

# submitted all three jobs on stf. Had to update amount of time requested. All are running!

### 2022-12-01 ###

Updated the upper columbia model. Upload and run on STF

# 100 iter:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia100iter/

# 200 iter:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia200iter/

# 500 iter:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia500iter/


So - upper columbia is running at a rate of 70 iterations in 3:10. It's definitely a bit faster than the snake model,
which makes sense given that there are less fish in this dataset.


### 12-02-2022 ###

Tried making an edit for performance, by vectorizing part of a loop.

# Okay, so I got the snake model to compile and start running locally. Let's run it for 40iter on ckpt to benchmark 
# the performance against the other version of the model.
Upload and try:

# create a directory: snake_vectorized_20iter
# Upload:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_20iter/

# Middle Columbia - upload the three iter versions

# 100 iter:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia100iter/

# 200 iter:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia200iter/

# 500 iter:
scp -r ./middle_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia500iter/

# Updated the slurm scripts

# submitted jobs!

### 12-03-2022 ###

Alright, so vectorized (summing lp of fish) snake run:
Chain 1 finished in 9513.2 seconds.
This is about a 25% increase in performance from the model where this part of the loop is not vectorized (which took 11853 seconds).

# Pull and rename as snake_vectorized_1_20iter/:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_20iter/snake/ ./

Let's try vectorizing a second part of the loop (summing lp of individual observations of a fish), and then submitting that
# Upload:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized2_20iter/

Performance of different ESU models:
Middle Columbia 100 iter: Chain 1 finished in 31098.0 seconds.
Upper Columbia 100 iter: Chain 1 finished in 37228.0 seconds.
Snake 100 iter: Chain 1 finished in 91096.9 seconds.

Snake takes by far the longest.

### 12-04-2022 ###

# Pull second vectorized snake run, rename as snake_vectorized_2
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized2_20iter/ ./

Oops, didn't work due to an indexing issue. Fixed the typo. Re run!

# Created a results folder locally to store the outputs from the various runs:
/Users/markusmin/Documents/CBR/steelhead/stan_actual/deteff_ESU_models/results/

# Make subfolders for run lengths - 200 iter

# Import runs into them to inspect - 200iter runs are available now for all ESUs - 500iter only available for Upper Columbia

# From within results/200iter/ folder:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake200iter/snake/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/upper_columbia200iter/upper_columbia/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/middle_columbia200iter/middle_columbia/ ./

# Note that currently I have also only run a single chain for each of these. So ideally I would submit 
three chains (separately) for each ESU

# Inspecting 200iter runs in shiny_stan:
All three look to give reasonable results, but some things don't look great with the chains, for example
seeing a lot of autocorrelation in some (but not all) parameters

Upon inspecting results, they all appear to make sense. Which is great news

# Inspect vectorized2 run
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized2_20iter/ ./
Chain 1 finished in 9939.3 seconds.
Making this change appears to have actually increased the run time. That makes sense, given that we are 
creating a new vector for every single fish, so it's not surprising that the cost of that line outweighed
the benefits of vectorization. Removed this again.

Let's see if we can run three chains on three nodes on mox.
set this in R script: options(mc.cores=3)
set parallel.chains to 3 in mod$sample()
updated stf job script to request three nodes (--nodes = 3)
Run 200 iter of snake vectorized_1:
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_200iter/

Run 20 iter of snake vectorized_1, on ckpt, with 3 chains - to see if it even works
scp -r ./snake/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/deteff_ESU_models/snake_vectorized_200iter/


### 2023-04-25 ###

We're back! Time to run the last model, but now with rear type (hatchery vs. wild) as a covariate

### 2023-05-01 ###

So I think cmdstan is working, but we're getting a ton of deprecated warnings from a program called by stan.
To turn these off, I went to this file: /Users/markusmin/.cmdstan/cmdstan-2.32.0/make
# and added: CXXFLAGS += -Wno-deprecated-declarations

Based on advice from here: https://discourse.mc-stan.org/t/running-cmdstan-on-ventura/30948/11

### 2023-05-08 ###
Stan is working.

Noticed some weird stuff in the model, where it looks like I turned the discharge effect off without fully realizing it?
- confirmed this to be the case, I used an intercept-only model for detection efficiency last time.
Evidence: if you look at the posteriors for the alpha and beta parameters for DE, the alpha posteriors are different
from the priors (due to data), but the beta posteriors are the same as the priors (because they don't interact with the data)
- It looks like I turned it off when troubleshooting a different problem, but forgot to turn it back on for the actual model


### 2023-05-09 ###
Hatchery vs wild:
- Split wild and hatchery datasets per ESU
	- Based on sample sizes, there are some origin + rear combinations we have to drop
	- Drop:
		- Asotin Creek H
		- Deschutes River H
		- Entiat River H
		- Fifteenmile Creek H
		- John Day H
		- Okanogan W
		- Yakima H
	- I wrote this based on threshold value - of 350 (these are all below 66 though)

- Run the origin model for each of those six datasets

### 2023-05-10 ###
- running the upper columbia, wild model on mox to test
# first move the folder to mox
# from within the directory /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_type
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_models/

# then from within the folder on mox:
sbatch job_ckpt_UC_W.slurm
# it worked! And it didn't take too long to run 100 iter (50 warmup and 50 sampling)
Chain 1 finished in 6737.2 seconds.
Warning: 42 of 50 (84.0%) transitions hit the maximum treedepth limit of 10.

### 2023-05-15 ###

#Pull the run (from within /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_type/from_hyak:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_models/upper_columbia/ ./

I'm going to take back dropping origin/rear combinations based on sample sizes. If they're left
in, nothing bad really happens except that the origin covariate has a really big confidence interval.
And then benefit is that you still get some estimate (although very low confidence), and it 
might help estimate the DPS-level covariates

Okay never mind again, let's drop those. Because those sample sizes are so small they're basically
meaningless. Will require dropping certain origin parameters.

So I made the edits that I think dropped Okanogan wild fish, so let's try running again
# from within the directory /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_type
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_models/
# then from within the folder on mox:
sbatch job_ckpt_UC_W.slurm

Model ran, quite quickly, in almost half the time as the last model (probably because we aren't monitoring a bunch of nonsense parameters):
Chain 1 Iteration:  1 / 100 [  1%]  (Warmup)
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following i$
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities$
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matric$
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Iteration: 10 / 100 [ 10%]  (Warmup)
Chain 1 Iteration: 20 / 100 [ 20%]  (Warmup)
Chain 1 Iteration: 30 / 100 [ 30%]  (Warmup)
Chain 1 Iteration: 40 / 100 [ 40%]  (Warmup)
Chain 1 Iteration: 50 / 100 [ 50%]  (Warmup)
Chain 1 Iteration: 51 / 100 [ 51%]  (Sampling)
Chain 1 Iteration: 60 / 100 [ 60%]  (Sampling)
Chain 1 Iteration: 70 / 100 [ 70%]  (Sampling)
Chain 1 Iteration: 80 / 100 [ 80%]  (Sampling)
Chain 1 Iteration: 90 / 100 [ 90%]  (Sampling)
Chain 1 Iteration: 100 / 100 [100%]  (Sampling)
Chain 1 finished in 3940.3 seconds.
Warning: 6 of 50 (12.0%) transitions hit the maximum treedepth limit of 10.

Pull it:
#Pull the run (from within /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_type/from_hyak:)
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_models/upper_columbia/ ./

Other datasets will take a lot longer, this was only 1500 fish or so

Everything looks good, I still had the values for discharge beta off though, so going to turn those
back on and re-submit just the stan model.
# on mox, remove executable:
[mmin@mox1 upper_columbia]$ rm parallel_upper_columbia_03_stan_actual_int_origin_wild_deteff
# from within the directory /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_type
scp -r ./upper_columbia/parallel_upper_columbia_03_stan_actual_int_origin_wild_deteff.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_models/upper_columbia/
# then from within the folder on mox:
sbatch job_ckpt_UC_W.slurm

# Querying temperature data from CBR DART
- so not all dams have better forebay than tailrace temperature data. For example,
Ice Harbor is missing winter forebay temperatures for 2007-2009. Perhaps we need to use a script
that fills in missing tailrace temperatures with forebay temperatures? or vice versa?
- I don't think that we can have missing data. So a workflow might look like:
1. Take temperature data from tailrace, if missing
2. Take temperature data from forebay, if also missing
3. Interpolate using the relationship of temperature with a nearby dam
	- This is going to be annoying for the upper columbia dams, since they're all
	missing temperature data in the winter before April 2013
	- So Wanapum Dam does have temperature data, for most of this missing time,
	so we can pull temp data from there

- Dams that have better forebay temperature coverage:
	- Bonneville
	
- Dams that have better tailrace temperature coverage:
	- John Day
	- Ice Harbor
	- Little Goose
	- Lower Granite
	- Lower Monumental
	- McNary
	- The Dalles
	
- Dams that have similar gaps in temperature coverage:
	- Priest Rapids
	- Rock Island
	- Rocky Reach
	- Wells
	- Wanapum
	
	
July 2005 - Spring 2006 is a problem for most dams
For most dams, the tailrace -> forebay -> linear interpolation works well. Exceptions
are the upper columbia dams (as expected): RIS, RRE, and WEL. PRA has a few gaps, but 
not nearly as bad
- let's figure out the relationship with Wanapum temperature

- temperature data completed and exported, with windows for every state and every dam from 2005-2022

### 2023-05-23 ###

Okay, I think the temperature model is working. Move it to hyak and run it 100 iter to test.

# move it to hyak
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/

# run it

# did a bunch of troubleshooting, then I think it ran!

### 2023-05-24

# Pull the temperature model
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/upper_columbia/ ./

Chain 1 finished in 6147.3 seconds.
Warning: 50 of 50 (100.0%) transitions hit the maximum treedepth limit of 10.

Length of run isn't too bad, the max treedepth is mostly an efficiency problem: https://mc-stan.org/docs/2_24/cmdstan-guide/diagnose.html

There's a typo - I used the same design matrix for origin and originxtemperature, when in reality they're different (1/1/-1, vs 1/1/0)
- so can't trust those results!!

### 2023-06-16 ###

Updated how temperature is treated:
- only universal effect outside of DPS boundaries, only origin-specific within boundaries
- still currently only set up for one temp effect for upstream LGR and upstream WEL

Move it to mox and try running:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/
sbatch job_ckpt_UC_W_T.slurm

### 2023-06-20 ###

# Fixed a dimensioning issue, moved it back
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/
sbatch job_ckpt_UC_W_T.slurm

# After a few fixes, the model ran - pull the run and inspect
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/upper_columbia/ ./

# push the run with two temperature covariates to mox
scp -r ./upper_columbia_2temp/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/

# pull the two temperature covariates run
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/upper_columbia_2temp/ ./

# run RE year model, one temp covariate (two temp covariates doesn't look too interesting)
# from within the stan_actual/rear_temp_year/ folder:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/

### 2023-06-26 ###

# RE year setup (with matt trick) and two temperature parameters (by season); move to hyak and run
# from within the stan_actual/rear_temp_year/ folder:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/

# to move just the stan script:


scp -r ./upper_columbia/parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_year_deteff.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/upper_columbia/

### 2023-06-30

So after lots of troubleshooting, the model ran, but it ran way too fast (100 iter in 132 seconds);
makes me think that something is wrong with the year RE. Let's pull and inspect outputs.

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/upper_columbia/ ./

Yep, so the issue is the statement of evaluating the byear params as being drawn from a normal.
It causes them never to move off of zero; when these statements are commented out, the
model runs (and takes much longer) - in one hour, it hasn't gotten past 10 iter; this might present an issue...


### 2023-07-03 ###

So, something weird I noticed - currently in the transformed parameters block, we don't have any
parameters governing the transitions from upstream states back to the mainstem - the R code looks like
it was written to give these the same parameters as moving from the mouth back to the mainstem (which
makes sense), but they aren't getting printed to the model, and apparently it's never caused an
issue (seeing as this is in all previous model runs). Either I'm missing something as to why
this was done, or it was a typo that was missed but turns out not to matter, because this transition
is never observed anyway? Since anything in the transformed matrices that isn't explicitly assigned
a parameter value is getting -100000, so you would get a "nope" from stan if it observed that

AH - feature not a bug. There's a part of the script (around line 367) where all of the states
where fish were never observed entering get removed from the transition matrix and hence
the matrix of parameter names. So that's why.

Alright, I got the model to run - but it's still way too fast. Looking at when the logits
were printed, it would appear that once again they're not moving off of zero.
- Oops, categorical_lmpf call was off. Turn it back on and run again
- turned it on, and it ran but way too fast - 240 seconds. But - it did give
me a warning that makes me think it's working? This warning:
Chain 1 Iteration:  1 / 100 [  1%]  (Warmup)
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in '/tmp/Rtmplzmvs4/model-132f7d4eb916.stan', line 432, column 8 to column 81) (in '/tmp/Rtmplzmvs4/model-132f7d4eb916.stan', line 2490, column 2 to line 2501, column 106)
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1
Chain 1 Iteration: 10 / 100 [ 10%]  (Warmup)
Chain 1 Iteration: 20 / 100 [ 20%]  (Warmup)
Chain 1 Iteration: 30 / 100 [ 30%]  (Warmup)
Chain 1 Iteration: 40 / 100 [ 40%]  (Warmup)

It always takes a while after it says the chains have finished, and I think that's because
it's tracking so many parameters now with the random effects.

### 2023-07-05 ###

Pull the run that finished very quickly and inspect
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/upper_columbia/ ./

Should make sure that the 2temp model, without RE year effects, works - so created a new folder for that
# move that one to hyak and try running it 
scp -r ./upper_columbia_2temp/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_models/
# run appears to have worked, finished in 7151 seconds

So back to RE year model - putting the sigma_year parameters into a matrix and using that
to calculate byear_actual didn't work at all. Parameters still aren't moving from initial values

So commenting out the std_normal_lpdf for the RE byear parameters made the model run, and it
completed in 6068 seconds. Should inspect that output.

# what if we just try moving the loop for the RE to after the categorical_lpmf?
# move the new stan script:
scp -r ./parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_year_deteff.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/upper_columbia/

Nope, that didn't matter at all!!!

### 2023-07-06 ###

# tried having a separate lp object for the RE - but that didn't seem to matter either, because
chains aren't moving again
Are these just entirely different magnitudes? What if we ask stan to print each lp object

Alright, so according to Mark I don't need to incorporate RE into the lp directly, just having
them have priors is okay.

So move this model to hyak and run:
scp -r ./parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_year_deteff.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_year_models/upper_columbia/

So, running quite slow, but running! We'll take it

Note that in the outputs, we have so many tracked parameters from the various arrays that
parameters are packed in. If we can reduce that the model will run faster

# pull it!

### 2023-07-07 ###

Let's have a RE year only model
copy the code from the rear_temp_year model and delete/comment out all fixed effects

# move to hyak and try running
scp -r ./upper_columbia/parallel_upper_columbia_03_stan_actual_int_origin_year_deteff.stan mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models/

### 2023-07-10 ###

The year-only model ran in 3340 seconds for 100 total iter, 13740 seconds for 150 total iter. Why so different?

Pull it and inspect:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models/upper_columbia/ ./


### 2023-07-12 ###

Spill model appears to be compiling locally, so move to hyak and run

scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/

### 2023-07-13 ###

Spill model finished in 8506 seconds with 100 iter. 
Pull it and inspect:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./


Building a year-only model
- move away from having every transition get a year effect, and instead limit it to the
ones that we're most interested in, i.e., movements around the natal tributary (which
would therefore include post-overhsoot fallback, overshoot, homing, straying)
- the disadvantage of setting it up this way is that we're missing out on effects that might
be downstream, for example higher loss during the upstream migration, or extra good
conditions in a tributary (e.g., the Deschutes) that is a popular staging area

This model is now compiling locally - move it to hyak and test
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models2/

# putting stf (200 iter) versions of rear_temp_spill and year_only; running over the weekend


### 2023-07-18 ###

The 200 iter year-only and rear-temp-spill models should have run on STF over the weekend;
let's pull them and inspect

Rear-temp-spill: 
Chain 1 Iteration: 390 / 400 [ 97%]  (Sampling)
Chain 1 Iteration: 400 / 400 [100%]  (Sampling)
Chain 1 finished in 25877.6 seconds.

-> took 7.2 hours to run. That's not bad at all for 400 total iter

year-only:
Chain 1 Iteration: 240 / 250 [ 96%]  (Sampling)
Chain 1 Iteration: 250 / 250 [100%]  (Sampling)
Chain 1 finished in 43606.8 seconds.

-> so even with only 250 total iter, this took 12.1 hours to run the chain (and then
there's also all the time that it takes to save the giant model output). So clearly 
RE year models are going to take much longer

# Pull them both so we can really get a look
# from dir /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_temp_spill:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia ./


# from dir /Users/markusmin/Documents/CBR/steelhead/stan_actual/year_only_v2:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models2/upper_columbia ./
# well apparently I used 200 warmup and 50 sampling, which is very dumb
# Change to 150 warmup and 150 sampling, then resubmit on stf and hopefully it's done by tomorrow morning

# Trying to understand the interplay between year effects and DE. We are drawing a year effect for 
# DE and NDE for every year, even though every year can only be either DE or NDE. So my understanding
# is that for every DE year effect pulled for a year that's NDE, it shouldn't deviate from
# zero because it's not part of the likelihood. Or is it problematic because of how every year param
# is evaluated as part of an normal, but really here some years are just going to be zero?

# MAJOR MODELING CHANGE - changing some tributary years from DE to NDE to be more conservative
# this is based on when arrays came online + when we see fish entering the tributary
# 10/11 for Wenatchee is now an NDE year, not a DE year (because array doesn't come online until January 2011)
# 07/08 for Entiat is now an NDE year, not a DE year
# 11/12 for Fifteenmile is now an NDE, not DE
# 12/13 for Hood is now an NDE, not DE
# 10/11 for Imnaha is now NDE not DE
# 10/11 for Tucannon is now NDE not DE
# 06/07 for Umatilla is now NDE not DE

# note that this needs to be changed in two blocks - around line 1500 and right before running the model
# I also fixed an issue with the Umatilla where there was a typo in terms of which years were considered
DE vs. not DE in the first block - I fixed a line where it said that DE only started with the second 
time period

# NOTE THAT THESE CHANGES ARE CURRENTLY ONLY IN THESE SCRIPTS:
/Users/markusmin/Documents/CBR/steelhead/stan_actual/year_only_v2/seed101_parallel_upper_columbia_03_stan_actual_int_origin_wild_year_mox_deteff_stf.R
/Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_temp_spill/seed101_parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_spill_mox_deteff_stf.R

# so cancel the current run, and change that and resubmit; only run 200 iter
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models2/

# Found a typo in the spill model. Make the change and resubmit

### 2023-07-19 ###

With the year only run - chain completed in 15 hours;
run has been going for 18 hours total though, which means saving the outputs is taking at least three hours

Chain 1 Iteration: 190 / 200 [ 95%]  (Sampling)
Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 53902.1 seconds.

# finished in about 19.5 hours total; so 4.5 hours for saving the parameter values

# pull the year-only run
# from within the year_only_v2 directory:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/year_only_models2/upper_columbia/ ./


# pull the spill run
# from within the rear_temp_spill directory:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./



### 2023-07-21 ###

Some of the spill days results don't make sense, so I'm looking back at the model code
One issue: the overshoot_vector and state_data have different lengths (by 7)
there seven fish in the loss state in states complete? That's the only difference here - 
explains the seven fish difference, and might be an indexing issue

> length(transition_run_years)
[1] 6812
> length(apr_post_overshoot_vector)
[1] 6812

as.vector(t(state_data_2))-> state_data_2_test
state_data_2_test[!(state_data_2_test %in% c(0, 43))] -> state_data_2_test_noloss

> length(state_data_2_test_noloss)
[1] 6805

And again, the difference here is that in states_complete, which was used to generate
transition_run_years and apr_post_overshoot_vector, there are seven fish in the loss state
These are the seven fish that we identified as having been trapped, and therefore
assigned the loss state to; this occurs outside of this script
Happens in this script, in to_hyak_transfer/2022-11-02-complete_det_hist_ckpt/03_hyak_complete_detection_histories_v4.R
- I really need to reorganize these scripts...

# So I believe that I fixed it - it actually has a couple of important changes, because the indexing
was wrong also for the run years (which would affect DE as well)

# THIS IS THE ONLY SCRIPT THAT HAS THE CHANGES:
seed101_parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_spill_mox_deteff_stf.R
# will need to make sure that's fixed for the year effects as well, but since that will eventually
be folded into this one, it should be okay


# Push it and run
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/

### 2023-07-23 ### 

There was a typo in the R script that caused the run to fail over the weekend. Fixed and reuploaded just the R script
(it's still only this script that has it fixed: seed101_parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_spill_mox_deteff_stf.R)
- if there aren't any typos, this should run overnight (takes about eight hours)


### 2023-07-24 ###

spill model ran fine - and finished quite quickly:

Chain 1 Iteration: 390 / 400 [ 97%]  (Sampling)
Chain 1 Iteration: 400 / 400 [100%]  (Sampling)
Chain 1 finished in 18459.9 seconds.

just about 5 hours, which is faster than the last run by a good bit

# pull the model run and inspect
# from within the rear_temp_spill directory:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./

# the typo that we fixed is producing much more reasonable spill results (this isn't surprising)
# Going to plot the raw data vs. number of spill days to inspect this pattern more closely

# So this looks almost okay, except that we have an issue now where if a fish falls back twice,
the implicit site visit code makes it so that the date of entry is the date that it's next seen.
See this fish for an example: 3D9.1C2C50DD03
# SO - this is actually a bad idea, because by fixing one problem you create a much bigger one,
namely that now the window for fallback is basically zero, because fallback is always implicit,
so now when you give it the previous time the window becomes literally zero time

# what we need to do is create a window, where the window is between the two times that 
the fish was actually seen - so no implicit times used in the calculation of this window.
I think it's fine if implicit site visits are given the next time - that's usually fine.
But I think if there are multiple implicit site visits in a row, the first one needs to be
given the previous time, and the second one needs to be given the next time.
But then what happens when there are three or more implicit movements in a row? This 
is rare but it does happen

# So - I think we fixed it using a strategy of creating completely new fields called window_date_time_1
and window_date_time_2, for the maximum window width.
# upload this latest R script and re-run the model - won't be able to view until tomorrow, but I think that's good.
scp -r ./upper_columbia/seed101_parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_spill_mox_deteff_stf.R mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/


# Latest chain ran quickly:
Chain 1 Iteration: 390 / 400 [ 97%]  (Sampling)
Chain 1 Iteration: 400 / 400 [100%]  (Sampling)
Chain 1 finished in 14427.5 seconds.

Even faster! Why though

# Pull it:
# from within the rear_temp_spill directory:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./

### 2023-07-26 ###
Fixed another small typo, re-ran the model.
Chain 1 Iteration: 390 / 400 [ 97%]  (Sampling)
Chain 1 Iteration: 400 / 400 [100%]  (Sampling)
Chain 1 finished in 18499.3 seconds.

# Pull it:
# from within the rear_temp_spill directory:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./

# Results look very similar. I want to see what this looks like with four chains, esp wrt 
# computation time.

# bumped the requested time on STF up to 48 hours, changed the R script to run four chains (7 threads per chain)
# Upload and run:
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/

# Making a new model folder, with April dropped.
rear_temp_spill_noapr

# This has four chains and 100 iter per chain, upload and run
# from within /rear_temp_spill_noapr/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spillnoapr_models/

# What would the structure of a wrapper script be if we wanted to run four chains, each on a separate node?
The wrapper script would just be a bash script, that submits four jobs to stf
The more elegant way to do this would be to have one script that just creates an .rda file
that has all of the data needed to run the stan model, then have a separate R script that
submits the stan run. This would actually be quite easy to implement, because right before
we submit the stan model, we store all of the data in a list. Just save this, then import it in the 
R scripts that run each of the chains

# executing this:
1) Made an R script stan_data_prep_UCW_temp_spillnoapr.R
	- this R script ends with saving the data file as an .RDA file. This can be run locally,
	prior to uploading the folder to mox
2) Made four R scripts to run one chain each, with a different seed
3) Made a bash script (submit_chains.sh) to submit these four R scripts


# upload and test
# from within /rear_temp_spill_noapr/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spillnoapr_models/

# submit the bash script to test - I'm currently running the four chain (on one node) at the same time,
so we can compare these results. I'm curious to see for the other run (on one node), if the
chains all look the same (because init = 0 for that)
Update: The chains all look different. They start at zero but then diverge due to the randomness
of accepting draws

# on mox:
[mmin@mox1 upper_columbia]$ bash submit_chains.sh
Submitted batch job 4721879
Submitted batch job 4721880
Submitted batch job 4721881
Submitted batch job 4721882
[mmin@mox1 upper_columbia]$ squeue -u mmin
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           4721870       stf     UC_W     mmin PD       0:00      1 (Priority)
           4721815       stf     UC_W     mmin PD       0:00      1 (Resources)
           4721882       stf   UC_W_4     mmin PD       0:00      1 (Priority)
           4721881       stf   UC_W_3     mmin PD       0:00      1 (Priority)
           4721880       stf   UC_W_2     mmin PD       0:00      1 (Priority)
           4721879       stf   UC_W_1     mmin PD       0:00      1 (Priority)
[mmin@mox1 upper_columbia]$ 

# Looks great!

### 2023-07-27 ###

The submitting 4 chains separately thing had some typos that I fixed - confirmed it's running now.

How long did the 4 chains on one node run take?

Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 26627.5 seconds.
Chain 2 Iteration: 200 / 200 [100%]  (Sampling)
Chain 2 finished in 26775.8 seconds.
Chain 4 Iteration: 200 / 200 [100%]  (Sampling)
Chain 4 finished in 27398.5 seconds.
Chain 3 Iteration: 200 / 200 [100%]  (Sampling)
Chain 3 finished in 27750.1 seconds.

All 4 chains finished successfully.
Mean chain execution time: 27138.0 seconds.
Total execution time: 27754.2 seconds.

So yeah, definitely longer - actually almost exactly four times longer than running one chain.
So that's good to know.

Let's pull the noapr four chain, and yesapr four chain, and look

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_models/upper_columbia/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spillnoapr_models/upper_columbia/ ./

### 2023-08-01 ###

Based on strange correlations in different months of spill, we're going to try winter spill days (Jan, Feb, March) as covariate

# copying rear_temp_spill_noapr and renaming as rear_temp_spill_winterdays
# made the changes to combine jan + feb + march into winter days

# renamed slurm scripts

# upload to mox and submit
# from within /rear_temp_spill_winterdays/
scp -r ./upper_columbia/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/

### 2023-08-15 ###

# Trying to reassess where we're at. It looks like the last thing I did was to submit a model
# where we just use winter spill days across jan + feb + march.

# Check on the model:
# Four chains were run separately, each with 100 warmup and 100 sampling iter
Chain 1 Iteration: 190 / 200 [ 95%]  (Sampling)
Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 16913.6 seconds.

Just under five hours per chain

# Pull it:
# from within /rear_temp_spill_winterdays/
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/ ./

# inspection:
- I may need to change how I set up the different chains - it would appear that instead of using
different seeds, I should use different chain_id: https://mc-stan.org/rstan/reference/sflist2stanfit.html
- hey these look really good! And make sense. I just think I need to run some chains longer

### 2023-09-29 ###

GitHub is being weird - version control disabled from RStudio. Trying to resolve using SSH key

Generating public/private ed25519 key pair.
Your identification has been saved in /Users/markusmin/.ssh/id_ed25519
Your public key has been saved in /Users/markusmin/.ssh/id_ed25519.pub
The key fingerprint is:
SHA256:A+mj9pSmd98RcVeRP5jO9eQPimD8RHl2gRuztN2yFco markusmin@Markuss-MBP-2
The key's randomart image is:
+--[ED25519 256]--+
|             . .+|
|       .    = .o.|
|      o    ooB=o+|
|     . .  o *E++=|
|      o.S. o+..*o|
|     . o+..  +o o|
|    o +. + ... ..|
|   . =. . o...  .|
|    .... .. .    |
+----[SHA256]-----+

Figure it out, I needed to accept the new Xcode license agreement. Stupid...


Back in the saddle...
Working on setting up the Upper Columbia, Hatchery model
Main difference from the UCW model is that now we're dropping Entiat River fish due to small N
In UCW we dropped Okanogan; now we have lots of Okanogan fish
Sample sizes here are in general, much larger

Comparing sample sizes between tributary_origin_year_table.csv and what's generated by the code - why are they different?
That CSV is generated by the script steelhead_2022-03-10.Rmd, which has a line to remove
fish in years where their home array is inactive.

In the R script, I don't think that those fish are being dropped before running the model. Is that an issue?
The thing is, that the model does divide into NDE and DE years. So this is already being addressed, at a later
stage. But you might not trust the NDE results at all, because it includes years where there aren't ANY arrays


I think that I made the necessary changes to the script to run for hatchery UC fish - try running on ckpt
- scripts that need to be changed when moving between DPSs:
	- stan_data_prep_UCH_temp_spill_winterdays.R (data prep for model entry - this takes the longest)
	- parallel_upper_columbia_03_stan_actual_int_origin_hatchery_temp_spill_winterdays_deteff.stan
		- going between hatchery and wild for the DPS doesn't require any actual changes as far as I can tell for the stan script;
		major changes required moving between DPSs
	- seed101_100iter_chain1_UC_H_T_Swinterdays.R
	- job_ckpt_UC_H_T_S.slurm (and other slurm scripts, need to point at the correct R scripts)
	- submit chains script
	- results analysis script - rear_temp_spill_winterdays.R
	




scp -r ./upper_columbia_hatchery/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/



### 2023-10-03

I think that the upper columbia hatchery model ran. Let's pull that from hyak:
# from within /rear_temp_spill_winterdays/
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/upper_columbia_hatchery/ ./


Chain inspection from UCW indicates that some are not mixing well at all - for example btemp1_matrix_2_10_DE
- need to increase burn and thin
- are the chains being so different a result of not having enough data?
	- added lines to bottom of data prep script to see how many observations of different movements
	- this doesn't seem to explain it
	- if you had no data to inform it, it would look like the prior, not have this weird behavior

Inspection for UCH - chains have the same story, some not mixing well at all, same as above
- Parameter estimates are what?? 
- every spill window parameter is significant
- winter spill is bad for fallback for hatchery?? that doesn't make any sense

- Okay, so looks like we haven't actually run this model with a year effect yet. Next step is to do that
- haven't yet run a model with spill window, spill days, temp, and year


### 2023-10-04

Created a new folder /rear_temp_spill_winterdays_year/ to develop models with spill/temp/year - i.e., the final (?) model

Made updates to the stan model, let's try to run it on ckpt
scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

### 2023-10-05
So, things look to be running ok for UCW on ckpt, so I submitted everything to stf (four chains x 100iter)

Developed UCH version, and now submitting that to ckpt
Move that folder to hyak:
scp -r ./upper_columbia_hatchery/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

### 2023-10-09

Looks like chains ran for UCW:
Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 56409.8 seconds.

But not for UCH:
Chain 1 Iteration: 140 / 200 [ 70%]  (Sampling)
slurmstepd: error: *** JOB 4930005 ON n2280 CANCELLED AT 2023-10-08T11:17:48 DUE TO TIME LIMIT ***

Makes sense, far more data in UCH

Resubmitted UCH chains with double the time allocations on stf

### 2023-10-10

Pull the four runs for UCW
# from within /Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_temp_spill_winterdays_year
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/upper_columbia_wild/ ./

So it looks like I had the wrong setup for the year effects in the complete models so far - see
Steelhead_meeting_2023-07-19.pdf
- basically, we moved away from having a year effect for every movement (led to estimating
year effects where we wouldn't expect to see an effect of year, like moving upstream past BON)
and instead having year effects only within the DPS and origin-specific. Notable that this
actually leads to less parameters
- Canceling the UCH runs with 48 hours on stf that are still in the queue, resubmitting the UCW
runs
- It looks like I made a typo in the year_v2 script, where I used the raw instead of actual (transformed
by scaling parameter) year effects to estimate movement probabilities; going to fix that in the current script
- This might explain the strange sigma_year parameters (scaling) which seemed not to be informed by data
- There's another issue, where I create the byearxorigin1_raw_parameters_array_DE objects but then
don't actually use them (I just use the vectors of year parameters)
	- JUST KIDDING - these are necessary. They aren't used when evaluating lpdf, but they
	are used to make sure that the reduce_sum function is manipulating the constituent vectors.
	- see this note: // 2023-07-05 - I do think we need these, because you need to be able
    // to pass them as arguments to reduce_sum so it knows to change them (and the 
    // parameters within) when finding the minimum NLL
    
I think I was able to update the script (wild only), let's see if we can run it on ckpt
scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/
Going to need to update the hatchery script as well



# What do I need to do? For each DPS:
1. Develop the stan model with spill, temp, year, and rear
	a. This is going to take a while for every other DPS that's not the Upper Columbia; those will be developed
	based on the Upper Columbia script, as the models for other DPSs weren't developed beyond just the int/origin stage
2. Update the data prep script for each DPS
3. Update auxiliary scripts necessary to run models on stf:
	- seed101_100iter_chain1_UC_H_T_Swinterdays.R
	- job_ckpt_UC_H_T_S.slurm (and other slurm scripts, need to point at the correct R scripts)
	- submit chains script


# One for all DPS (with some modifications required to accomodate different states/origins in different DPSs)
1. Create a script that can create figures/tables for presentation and publication
	a. Given that there are so many outputs for these models, a key consideration is
	which outputs to visualize that would be appropriate for a paper
	b. Parameter estimates for exploration (in an appendix)
	c. Comparisons between wild and hatchery parameter estimates
	d. Final fates
	e. Predicted/fitted values for temp and spill

# At some point:
1. Need to make this actually reproducible. There is no way that someone would look at 
my scripts right now and understand what to do. At this point I barely do.
	a. Good practice would be to develop the MC and SR models in a way that's more reproducible, with more comments
	b. Going to develop from scratch (a fresh stan script, and move chunks into that script), rather than copy and paste
	the UCH/UCW model and try to figure out what to change
	
	
Notes on the different DPS/rear type models:
- Middle Columbia Hatchery looks to only have two origins: Walla Walla and Umatilla (totaling about 3000 fish)
- Middle Columbia Wild has six: John Day, Umatilla, Walla Walla, Yakima, Fifteenmile Creek, and Deschutes (totaling about 7000 fish)
- Snake River Hatchery has five origins: Tucannon, Clearwater, Imnaha, Grande Ronde, Salmon (about 27,000 fish)
- Snake River Wild has six origins: Tucannon, Clearwater, Imnaha, Grande Ronde, Salmon, Asotin Creek (about 5,000-6,000 fish)
	- Remember that many SR origins don't have really any detection capabilities in the tributaries

### 2023-10-12 ###

Typo in the stan script for UCW, need to reupload and try submitting on ckpt again
scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

Looks like it's now running appropriately; submit the STF runs for UCW

Develop updated UCH script with corrected year effects
- if the hatchery and wild have the same number of origins, I believe that the stan script between the two
should actually be identical
- Copied over the stan script fro updated UCW to UCH, re-ran the data_prep script to add in the
year_X_mat object, now reuploading and submitting on ckpt to check
scp -r ./upper_columbia_hatchery/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

Looks like it's running, submitted UCH on stf as well

### 2023-10-15 ###

Pull the two runs:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/upper_columbia_hatchery/ ./
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/upper_columbia_wild/ ./

# Damn, hatchery runs still didn't complete for 200iter (100 warmup and 100 sampling) in 48 hours. 
# Wild runs did all complete within 24 hours, so we can at least inspect those

# Welp, I think we have an issue here where it's not referencing the right .stan script
# The issue is that there's already a compiled executable on stf, and so it didn't
recompile. Need to delete that, resubmit all runs

# Up wild runs to 48 hours on stf

# Going to submit hatchery runs with 96 hours on stf


# Develop new scripts to run longer chains

### 2023-10-17 ###

With the corrected stan scripts, these models are taking much longer to run.

UCW, 200 total iter:
Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 133597.0 seconds.
- equivalent to 37.1 hours; total time on hyak was 42, due to number of parameters saved

UCH, 200 total iter:
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           4943860       stf   UC_H_2     mmin  R 1-17:16:40      1 n2279
Chain 1 Iteration: 110 / 200 [ 55%]  (Sampling)
Chain 1 Iteration: 120 / 200 [ 60%]  (Sampling)

The good news is that this should still finish in the allotted time of 96 hours, but wow these are slow.

# pull the UCW run:
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/upper_columbia_wild/ ./

# rhat measures convergence, ESS measures efficiency (autocorrelation)

Diagnostic notes (from https://mc-stan.org/docs/reference-manual/effective-sample-size.html):

For independent draws, the effective sample size is just the number of iterations. For correlated draws, 
the effective sample size will be lower than the number of iterations. For anticorrelated draws, the 
effective sample size can be larger than the number of iterations. In this latter case, MCMC can work 
better than independent sampling for some estimation problems. Hamiltonian Monte Carlo, including the 
no-U-turn sampler used by default in Stan, can produce anticorrelated draws if the posterior is close 
to Gaussian with little posterior correlation.

### 2023-10-19 ###

# Submit UCW, with 1000 iter (1000 burnin and 1000 sampling)
# Also here changing to max_treedepth = 15; this is 32 times bigger (2^5) than default of 10

# Created four new R scripts to run 4 chains, 1000 iter, max_treedepth = 15

# Wrote a new script: UCW_submit_chains_1000iter.sh

# Create four new slurm scripts to run 1000iter, with 480 hours requested; hopefully that's okay?
# It is not! 240 hours is allowed so I submitted those chains

# Move that all to hyak:

scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

# submitted the runs at 11:30 am on 10/19/23; let's see how long this takes!

Find the maximum difference in successive dates
Find the row index of this biggest gap
Display rows of tripData immediately before (three rows) and after (three rows) the biggest gap

### 2023-10-20 ###

Checking in on runs at 11:30 am on 10/20/23
- we are only at 20 iter of 2000. This is not going to finish in ten days; perhaps tree depth was increased too much.
	- solution: cancel all of the runs, change max_treedepth to 12 (15 seems too much)

### 2023-10-23 ###

Checkin in on runs at 3:30 PM on 10/23/23
           4954294       stf   UC_W_1     mmin  R 3-03:50:22      1 n2277
Chain 1 Iteration:  380 / 2000 [ 19%]  (Warmup)
Chain 1 Iteration:  390 / 2000 [ 19%]  (Warmup)

So, it's been 3 days, and we're only through 400 iter. This is currently on pace to finish in 15 days. So it won't
finish in time.
A 500iter/500warmup run would finish in 10 days.
I'm going to let this keep running, but submit a 500iter run.

Made new UCH_submit_chains_500iter.sh; job_stf_500iter runs; seed101_500iter  runs

# Move that all to hyak:

scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

Submitted the chains; note that it apparently killed the other runs?
Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup)
Warning: Chain 1 finished unexpectedly!

Warning message:
No chains finished successfully. Unable to retrieve the fit.
Error: No chains finished successfully. Unable to retrieve the draws.
Execution halted

### 2023-10-24 ###

Checking in on runs at 8:30 am on 10/24/23
           4958688       stf   UC_W_3     mmin  R   16:34:25      1 n2312
Chain 1
Chain 1 Iteration:  10 / 1000 [  1%]  (Warmup)
Chain 1 Iteration:  20 / 1000 [  2%]  (Warmup)
Chain 1 Iteration:  30 / 1000 [  3%]  (Warmup)

So a bit slower than I was hoping... If we can get through 100 iter every 24 hours though, this will finish

What if a big problem is that we have covariate correlation? Like with temperature? Or is that being
taken care of via indexing...?

### 2023-10-25 ###

Checking in on runs at 9:15 am on 10/25/23
[mmin@mox1 ~]$ squeue -u mmin
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           4958689       stf   UC_W_4     mmin  R 1-13:14:22      1 n2305
           4958686       stf   UC_W_1     mmin  R 1-17:16:31      1 n2309
           4958687       stf   UC_W_2     mmin  R 1-17:16:31      1 n2311
           4958688       stf   UC_W_3     mmin  R 1-17:16:31      1 n2312
Chain 1
Chain 1 Iteration:  10 / 1000 [  1%]  (Warmup)
Chain 1 Iteration:  20 / 1000 [  2%]  (Warmup)
Chain 1 Iteration:  30 / 1000 [  3%]  (Warmup)
Chain 1 Iteration:  40 / 1000 [  4%]  (Warmup)
Chain 1 Iteration:  50 / 1000 [  5%]  (Warmup)
Chain 1 Iteration:  60 / 1000 [  6%]  (Warmup)
Chain 1 Iteration:  70 / 1000 [  7%]  (Warmup)

Not even at 10% completed nearly two days in. These runs are not finishing in 10 days. At this pace,
we are only getting through 1.7 iterations per hour. So in 240 hours, we could only finish about 400 iter.
I don't honestly understand why these runs are suddenly slower than the runs that we checked in on 
on 10/23/23, which were approximately 3 times faster

Where to go from here?
Let's control for just max_treedepth and run that again, back with 100iter.
# Modified the seed101_100iter R scripts to run with max_treedepth = 12; saved these as new scripts

# Wrote a new script: UCW_submit_chains_100iter_mtd12.sh

# Create four new slurm scripts to run 100iter_mtd12, with 240 hours requested
# Move that all to hyak:

scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

# started running at 9:35 am on 10/25/23

### 2023-10-26 ###

Checking in on runs at 9:30 am on 10/26/23

[mmin@mox1 upper_columbia_wild]$ squeue -u mmin
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           4962855       stf   UC_W_1     mmin  R   23:56:09      1 n2305
           4962856       stf   UC_W_2     mmin  R   23:56:09      1 n2309
           4962857       stf   UC_W_3     mmin  R   23:56:09      1 n2311
           4962858       stf   UC_W_4     mmin  R   23:56:09      1 n2312
[mmin@mox1 upper_columbia_wild]$ nano slurm-4962858.out 

Chain 1
Chain 1 Iteration:  10 / 200 [  5%]  (Warmup)
Chain 1 Iteration:  20 / 200 [ 10%]  (Warmup)
Chain 1 Iteration:  30 / 200 [ 15%]  (Warmup)
Chain 1 Iteration:  40 / 200 [ 20%]  (Warmup)

Well, they'll definitely finish. But 40 iter every 24 hours? Pretty rough, but at the same pace as earlier

### 2023-10-27 ###

Checking in on runs at 11:00 am on 10/27/23
[mmin@mox1 ~]$ squeue -u mmin
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           4962855       stf   UC_W_1     mmin  R 2-01:25:18      1 n2305
           4962856       stf   UC_W_2     mmin  R 2-01:25:18      1 n2309
           4962857       stf   UC_W_3     mmin  R 2-01:25:18      1 n2311
           4962858       stf   UC_W_4     mmin  R 2-01:25:18      1 n2312
[mmin@mox1 upper_columbia_wild]$ nano slurm-4962858.out 
Chain 1
Chain 1 Iteration:  10 / 200 [  5%]  (Warmup)
Chain 1 Iteration:  20 / 200 [ 10%]  (Warmup)
Chain 1 Iteration:  30 / 200 [ 15%]  (Warmup)
Chain 1 Iteration:  40 / 200 [ 20%]  (Warmup)
Chain 1 Iteration:  50 / 200 [ 25%]  (Warmup)
Chain 1 Iteration:  60 / 200 [ 30%]  (Warmup)
Chain 1 Iteration:  70 / 200 [ 35%]  (Warmup)
Chain 1 Iteration:  80 / 200 [ 40%]  (Warmup)

Yup, same 40 iter/24 hour pace. Should finish by Monday afternoon (?)

### 2023-10-30 ###

Checkin in on runs at 4:10 pm on 10/30/23

[mmin@mox1 ~]$ squeue -u mmin
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           4962858       stf   UC_W_4     mmin  R 5-06:35:22      1 n2312
Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 451799.0 seconds.

All chains finished, this run is just still saving to memory

### 2023-10-31 ###

# Pull runs off hyak:

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/upper_columbia_wild/ ./

# So, these runs aren't running into the treedepth warning. Old runs looked like this:

Chain 1 Iteration: 190 / 200 [ 95%]  (Sampling)
Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 55805.9 seconds.
Warning: 100 of 100 (100.0%) transitions hit the maximum treedepth limit of 10.
See https://mc-stan.org/misc/warnings for details.

# Last run looks like this:

Chain 1 Iteration: 190 / 200 [ 95%]  (Sampling)
Chain 1 Iteration: 200 / 200 [100%]  (Sampling)
Chain 1 finished in 451799.0 seconds.

# It is RIDICULOUSLY slow (almost 10x slower) but not hitting that warning at least

Okay, so there are these massive (2.5 GB) core.(number) - e.g., core.8022 files that are 
being produced along with our outputs. Is this what is causing the horrible performance?

It's a core dump file - https://en.wikipedia.org/wiki/Core_dump
Running $ file core
(base) markusmin@Markuss-MBP-2 rear_temp_spill_winterdays_year % file upper_columbia_wild/core.20615 
upper_columbia_wild/core.20615: ELF 64-bit LSB core file, x86-64, version 1 (SYSV), SVR4-style, from './parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_spill_winterdays_', real uid: 1089588, effective uid: 1089588, real gid: 20725, effective gid: 20725


There are eight of them. A multiple of 4, which to me indicates that perhaps it's related to these runs?
-rw------- 1 mmin hyak-stf 2358505472 Oct 23 15:44 core.12513
-rw------- 1 mmin hyak-stf 2495987712 Oct 25 09:34 core.13878
-rw------- 1 mmin hyak-stf 2365718528 Oct 25 09:34 core.20615
-rw------- 1 mmin hyak-stf 3901366272 Oct 25 09:34 core.20636
-rw------- 1 mmin hyak-stf 2812739584 Oct 25 09:34 core.23709
-rw------- 1 mmin hyak-stf 2670714880 Oct 23 15:44 core.3991
-rw------- 1 mmin hyak-stf 2598658048 Oct 23 15:44 core.5603
-rw------- 1 mmin hyak-stf 4090806272 Oct 23 15:44 core.8022

those times coincide with the times that I started some new runs. And at least one
of those runs killed the previous run; is that related?

The date (10/23) on the oldest of these core objects aligns with the first run that was
killed, and also coincides with a massive decrease in performance.

For now: Remove those core. objects, pull the run, inspect

So now, we don't actually know if the bad performance is due to a treedepth change, or due
to those horrendous core. objects

On max_treedepth: 
We do not generally recommend increasing max treedepth. In practice, the max treedepth 
limit being reached can be a sign of model misspecification, and to increase max treedepth 
can then result in just taking longer to fit a model that you don’t want to be fitting.

Okay, so model is probably misspecified. I think we knew that.

Let's look into differences in treedepth and see if that makes a difference.

### 2023-11-01 ###

The treedepth did help. We now don't have those warnings, and the rhat values are down
across the board. But not enough - all it did was make reduce the magnitude of those values,
but many are still above 1.2 or so; Thinning might help

Let's see if we can get a longer mtd = 12 run to go; try 250 iter

# Created seed101_250iter R scripts to run with max_treedepth = 12; saved these as new scripts

# Wrote a new script: UCW_submit_chains_250iter_mtd12.sh

# Create four new slurm scripts to run 250iter_mtd12, with 240 hours requested
# Move that all to hyak:
scp -r ./upper_columbia_wild/ mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/

# submitted 250iter runs at 9:35 am on 11/1

# checkin on run 11/1/23 at 3:38 pm
[mmin@mox1 ~]$ squeue -u mmin
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           4971943       stf   UC_W_4     mmin  R    5:31:40      1 n2264
           4971942       stf   UC_W_3     mmin  R    5:31:50      1 n2274
           4971940       stf   UC_W_1     mmin  R    5:57:56      1 n2276
           4971941       stf   UC_W_2     mmin  R    5:57:56      1 n2277
Chain 1
Chain 1 Iteration:  10 / 500 [  2%]  (Warmup)
Chain 1 Iteration:  20 / 500 [  4%]  (Warmup)


It looks like we're getting back to our original performance before the core dump issue;
still not very fast, but more like 4-5 iter per hour instead of 1.7



# what is our plan for understanding model misspecification?

Overarching: Peel back layers, build model back up iteratively, figure out at what layer
the misspecification occurs


# The current model has these parameters:
> table(upper_columbia_wild_fit_draws_summary_mtd10_sub$param_type)

            b0       borigin1       borigin2   bspillwindow         btemp0 btemp0xorigin1 
            68             22             22              7             28             13 
btemp0xorigin2 btemp0xorigin3         btemp1 btemp1xorigin1 btemp1xorigin2 btemp1xorigin3 
            13             13             28             13             13             13 
  bwinterspill  byearxorigin1  byearxorigin2  byearxorigin3          sigma 
             2            306            306            306             51 
             
Standardized diagnostics script
1. Intercept + origin
2. Intercept + origin + spill (days and window)
3. Intercept + origin + spill + temp0 + temp1 
4. Intercept + origin + spill + temp0 + temp1 + temp0xorigin + temp1xorigin
5. Intercept + origin + spill + temp0 + temp1 + temp0xorigin + temp1xorigin + year model


### 2023-11-02 ###

Checking on runs at 8:50 am
[mmin@mox1 ~]$ squeue -u mmin
c             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           4971943       stf   UC_W_4     mmin  R   22:44:54      1 n2264
           4971942       stf   UC_W_3     mmin  R   22:45:04      1 n2274
           4971940       stf   UC_W_1     mmin  R   23:11:10      1 n2276
           4971941       stf   UC_W_2     mmin  R   23:11:10      1 n2277

Chain 1
Chain 1 Iteration:  10 / 500 [  2%]  (Warmup)
Chain 1 Iteration:  20 / 500 [  4%]  (Warmup)
Chain 1 Iteration:  30 / 500 [  6%]  (Warmup)
Chain 1 Iteration:  40 / 500 [  8%]  (Warmup)
Chain 1 Iteration:  50 / 500 [ 10%]  (Warmup)

at least 50 iter in 24 hours; perhaps not the best performance, but good enough
to finish 500 iter in 10 days

### 2023-11-07 ###

All runs finished!
Chain 1 Iteration: 490 / 500 [ 98%]  (Sampling)
Chain 1 Iteration: 500 / 500 [100%]  (Sampling)
Chain 1 finished in 266084.0 seconds.

Finished in 74 hours. Where did the efficiency jump come from? Almost 7 iter
per hour, at that rate I could have finished double the iter in ten days.

Pull the runs and inspect

scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_year_models/upper_columbia_wild/ ./

# Inspecting diagnostics - things suddenly look so much better!!
# I think that this model might be usable.


# Okay - some thoughts on efficiency
Currently, we are tracking upwards of 450k parameters, when we really only care about 1250.
The reason for this is how we set up indexing; we just use a ton of big arrays, that are mostly
just empty, but we are still tracking the emptiness
The purpose of the arrays is to index [from, to]
BUT - what if we changed it so that our parameters were in a vector instead
And then the arrays were coded outside of the model - so you don't have to track the
full array of parameters, but instead have an array which is just input,
and each element of the array (which is [from, to]) is a number, which gives you the index
number of the vector. So like from = 1, to = 1 would be the first element of the vector
from = 1, to = 2 would be the second element of the vector.
The vector for each type of parameter would only have to be the length of the possible movements, aka 61.
Currently the array for each type of parameter is 43 x 43 = 1849.
So in theory we could cut down the number of things we have to track by 30.
There are some parameters (like spill) that only have a handful of places that even get a parameter.
Those could in theory be only the same length as the number of parameters, but I think that keeping
them all length 61 would be much better for indexing/convenience.


### MOX to KLONE migration ###

# Can I just copy everything over?
# From within mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/
scp -r mmin@mox.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/ ./

# Yep! That worked

# Let's try submitting some jobs

# Edits that need to be made
1) stf is not a partition on klone. It's called compute
[mmin@klone-login01 upper_columbia_wild]$ hyakalloc
         Account resources available to user: mmin          
╭─────────┬─────────────────┬──────┬────────┬──────┬───────╮
│ Account │       Partition │ CPUs │ Memory │ GPUs │       │
├─────────┼─────────────────┼──────┼────────┼──────┼───────┤
│     stf │         compute │  640 │  2800G │    0 │ TOTAL │
│         │                 │  559 │  2685G │    0 │ USED  │
│         │                 │   81 │   115G │    0 │ FREE  │
├─────────┼─────────────────┼──────┼────────┼──────┼───────┤
│     stf │      gpu-2080ti │   40 │   363G │    8 │ TOTAL │
│         │                 │   22 │   268G │    6 │ USED  │
│         │                 │   18 │    95G │    2 │ FREE  │
├─────────┼─────────────────┼──────┼────────┼──────┼───────┤
│     stf │ compute-hugemem │  120 │  2226G │    0 │ TOTAL │
│         │                 │    0 │     0G │    0 │ USED  │
│         │                 │  120 │  2226G │    0 │ FREE  │
├─────────┼─────────────────┼──────┼────────┼──────┼───────┤
│     stf │     interactive │   40 │   175G │    0 │ TOTAL │
│         │                 │    0 │     0G │    0 │ USED  │
│         │                 │   40 │   175G │    0 │ FREE  │
╰─────────┴─────────────────┴──────┴────────┴──────┴───────╯

2) For account:
 [mmin@klone-login01 upper_columbia_wild]$ groups
all stf test

For testing - editing the following job
job_stf_UC_W_T_Swinterdays_Y_chain1_250iter_mtd12.slurm

All I did was change from "-p stf" to "-p compute"

[mmin@klone-login01 upper_columbia_wild]$ sbatch job_stf_UC_W_T_Swinterdays_Y_chain1_250iter_mtd12.slurm
Submitted batch job 15283322
[mmin@klone-login01 upper_columbia_wild]$ squeue -u mmin
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          15283322   compute   UC_W_1     mmin PD       0:00      1 (QOSGrpMemLimit)
          
          
Worked? But what does that reason mean?

Okay so back on 06-01-22, I tried reinstalling everything on KLONE, but gave up. I thought
I needed more memory, but I solved my problem by using stan and so I stayed on MOX. Now
I need to go through and actually set up my full computing environment on KLONE.

# On hyak I'm going to create a test script just to figure out modules and dependencies
mv job_stf_UC_W_T_Swinterdays_Y_chain1_250iter_mtd12.slurm 01_testjob_klone.slurm

I need two modules - on mox they were:
module load contrib/gcc/6.2.0 
module load contrib/R-4.2.0/4.2.0

Trying to figure out if the same software exists on KLONE:
[mmin@n3319 upper_columbia_wild]$ module avail

GCC looks like there are multiple versions:
/sw/klone/gcc/13.2.0
- this can be loaded with this command:
[mmin@n3319 upper_columbia_wild]$ module load gcc/13.2.0

For R - it seems that I tried to install it in June 2022, but I'm not sure that I ever was
able to load it as a module?

How do I create personal LMOD modules on KLONE?#

This advanced user documentation page from the LMOD developers walks you through this [link]. 
You need to compile your code separately first. In short, you provide a command directing it 
to the folder with your collection of module files:

So - I think I need to compile R
And I don't think that I ever achieved that?

Going to try and work with the R installation that I was working on and configure it

./configure \
    --prefix=/sw/contrib/stf-src/R-4.2.0 \
    --enable-R-shlib \
    --enable-memory-profiling

A bunch of output, and then:
configure: error: --with-readline=yes (default) and headers/libs are not available

Trying to follow advice found here: https://github.com/tinyheero/tinyheero.github.io/blob/master/_drafts/installing-R-from-source.md#

Looks like readline is perhaps already installed - load it:
[mmin@n3319 R-4.2.0]$ module load cesg/readline/8.1
# try configuring again after loading module
[mmin@n3319 R-4.2.0]$ ./configure     --prefix=/sw/contrib/stf-src     --enable-R-shlib     --enable-memory-profiling

# slightly different error:
configure: error: --with-x=yes (default) and X11 headers/libs are not available




Things that I tried but didn't work:
[mmin@n3319 upper_columbia_wild]$ module load contrib/stf-src/R-4.2.0
Lmod has detected the following error: The following module(s) are unknown: "contrib/stf-src/R-4.2.0"

[mmin@n3319 upper_columbia_wild]$ module load stf-src/R-4.2.0
Lmod has detected the following error: The following module(s) are unknown: "stf-src/R-4.2.0"

[mmin@n3319 upper_columbia_wild]$ module load sw/contrib/stf-src/R-4.2.0
Lmod has detected the following error: The following module(s) are unknown: "sw/contrib/stf-src/R-4.2.0"

So it looks like back in the day I installed R, but never actually was able to turn it into
a usable module.

These modules are available for use:
[mmin@klone-login01 stf]$ ls
abyss   autoconf  bedtools  cellranger  hdf5      jellyfish  links       minimap  netcdf  paraview_catalyst  purge_dups  seqtk       texinfo
arcs    automake  bwa       cgreene3    help2man  julia      longstitch  mpich    ntlink  petsc              samblaster  sparsehash  tigmint
aspera  bamtools  canu      gdb         hisat     libdb      masurca     mummer   orca    pigz               samtools    sratoolkit  xz

And here's where I installed R:
[mmin@klone-login01 stf-src]$ ls
abyss     automake  bwa         hdf5      jellyfish    links       mpich   orca               purge_dups  seqtk       texinfo
arcs      bamtools  canu        help2man  julia-1.9.0  longstitch  mummer  paraview_catalyst  R-4.2.0     share       tigmint
aspera    bedtools  cellranger  hisat     lib          masurca     netcdf  petsc              samblaster  sparsehash  xz-5.2.5
autoconf  bin       gdb         include   libdb        minimap     ntlink  pigz               samtools    sratoolkit  xz-5.2.5.tar.gz

So it looks like most other files are in both folders

Following the instructions from here: https://uwrc.github.io/docs/tools/modules/
And here: https://lmod.readthedocs.io/en/latest/100_modulefile_examples.html

Let's use the orca example

[mmin@klone-login01]$ nano /sw/contrib/modulefiles/stf/orca/4.2.1
#%Module4.2.1

module-whatis "Short description goes here"

proc ModulesHelp { } {
puts stderr "Longer description goes here"
}

prepend-path PATH "/sw/contrib/stf-src/orca/4.2.1"

### Here's another example:
[mmin@klone-login01]$ nano /sw/contrib/modulefiles/stf/samtools/1.13
#%Module1.0

module-whatis "Tools (written in C using htslib) for manipulating next-generation sequencing data."

proc ModulesHelp { } {
puts stderr "Tools (written in C using htslib) for manipulating next-generation sequencing data."
}

module load "cesg/ncurses/6.2"

prepend-path PATH "/sw/contrib/stf-src/samtools/1.13"

### Here's another example:
[mmin@klone-login01]$ nano /sw/contrib/modulefiles/stf/julia/1.9.0

#%Module1.0

module-whatis "High performance scientific computing"

prepend-path PATH "/sw/contrib/stf-src/julia-1.9.0/bin"

### Here's another example - I made this 1.5 years ago
[mmin@klone-login01]$ nano /sw/contrib/modulefiles/stf/xz/5.2.5

#%Module1.0####################################################################
##
proc ModulesHelp { } {
        puts stderr "\tAdds xz 5.2.5 to the PATH."
}

module-whatis "Adds xz 5.2.5 to the PATH."

prepend-path    PATH            "/sw/contrib/stf-src/xz-5.2.5"




#### Here's what I changed the R modulefile to:
#%Module4.2.0

module-whatis "R version 4.2.0"

proc ModulesHelp { } {
puts stderr "R version 4.2.0"
}

prepend-path PATH "/sw/contrib/stf-src/R-4.2.0"

### 2023-11-10 ###

Going to keep working on getting R installed and running on Klone.
The problem isn't the modulefile, it's that I don't have a version of R that's compiled
on Klone.
I'm going to work on reinstalling R from scratch, rather than trying to use the existing 
files where installation isn't working.

# I'm going to try again with the strategy that worked on mox the first time.
Second option from first response at this link: https://stackoverflow.com/questions/46343044/install-r-in-linux-server

# What's the typical file organization for a module on stf?
- Making a folder called R at /sw/contrib/stf-src/R
- This is where I'm going to try reinstalling R-4.2.0

# First, let's get into an interactive node
salloc -p compute -A stf -N 1 -c 1 --time=2:00:00 --mem=10G

# Now, let's try to follow the instructions from stackoverflow:
# Taking some elements from https://docs.posit.co/resources/install-r-source/#specify-r-version as well

````
wget https://cran.rstudio.com/src/base/R-4/R-4.2.0.tar.gz
tar xvf R-4.2.0.tar.gz
cd R-4.2.0
./configure
make && make install
````

# So - it breaks at ./configure with the usual error:
configure: error: --with-readline=yes (default) and headers/libs are not available

# Okay, it's recommended that I configure with readline, and the stackoverflow link
is telling me that I can force the dependency path into the config file. So let's try that

# There is a working installation of readline at cesg/readline/8.1
# I believe that this is the full path: /sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin

# Run this line:
./configure --with-x=no --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin
# Same error message

# Tried loading readline library with
[mmin@n3319 R-4.2.0]$ module load cesg/readline/8.1
# Then ran above again

# Now getting different error:
configure: error: "liblzma library and headers are required"

# we've gotten this error before

# adding this flag that we added with our installation of R on Mox:
--with-pcre1
# Same error!

# So based on our troubleshooting back in 2022, we tried this approach: https://stackoverflow.com/questions/42170752/building-package-using-configure-how-to-rope-in-updated-versions-of-libs-heade

# which had us install another dependency at
/sw/contrib/stf-src/xz-5.2.5

# So now, let's try loading that module and adding that dependency to the configure call
[mmin@n3319 R-4.2.0]$ module load stf/xz/5.2.5

# Run this line:
./configure --with-x=no --with-xz=sw/contrib/stf-src/xz-5.2.5 --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin

# It doesn't recognize the --with-xz flag.

# Okay, maybe I just need to install the liblzma library.
# So it's my understanding that liblzma and xz utils are the same thing? Based on these links:
- https://github.com/kobolabs/liblzma/blob/master/INSTALL
- https://tukaani.org/xz/
"XZ Utils consist of several components:

liblzma is a compression library with an API similar to that of zlib.
xz is a command line tool with syntax similar to that of gzip.
xzdec is a decompression-only tool smaller than the full-featured xz tool.
A set of shell scripts (xzgrep, xzdiff, etc.) have been adapted from gzip to ease viewing, grepping, and comparing compressed files.
Emulation of command line tools of LZMA Utils eases transition from LZMA Utils to XZ Utils."

# Will it recognize --with-liblzma?
# Run this line:
[mmin@n3226 R-4.2.0]$ ./configure --with-x=no  --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin
configure: WARNING: unrecognized options: --with-liblzma

Same error

# Maybe it's a FLAGS issue? https://groups.google.com/g/r-sig-mac/c/4smMULZWKPc

[mmin@n3226 R-4.2.0]$ ./configure --with-x=no LDFLAGS="-Lsw/contrib/stf-src/xz-5.2.5" --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin

This didn't return any "unrecognized options" warnings but returned the exact same error:
configure: error: "liblzma library and headers are required"

Alright, I'm giving up. Let's turn off readline despite the warnings I've seen

[mmin@n3226 R-4.2.0]$ ./configure --with-readline=no --with-x=no
hahahahaha same error
configure: error: "liblzma library and headers are required"

Well at least we know that readline isn't the issue.

# Alright, a new solution to try to implement: https://stackoverflow.com/questions/74821884/liblzma-library-and-headers-are-required-when-installing-r-4-2-2-and-dependenc

[mmin@n3226 R-4.2.0]$ ./configure --with-x=no --with-lzma=sw/contrib/stf-src/xz-5.2.5 --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin
liars!!!!
configure: WARNING: unrecognized options: --with-lzma

[mmin@n3226 R-4.2.0]$ ./configure --prefix=/sw/contrib/stf-src/R/R-4.2.0 --with-x=no --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin
# Maybe it's just a prefix flag issue?
Nope.
configure: error: "liblzma library and headers are required"

# xz-devel instead of just xz?
https://stackoverflow.com/questions/38091418/installation-of-r-3-3-1-in-red-hat-lzma-version-5-0-3-required

unloaded xz, tried running above line again; same error.

# It looks like another lab has a working installation of R?
 module load pedslab/R/4.2.3
# But if I try to load it...
[mmin@n3226 R-4.2.0]$ R
FATAL:   While checking container encryption: could not open image /mmfs1/gscratch/pedslabs/Resources/containers/R/R-4.2.3.sif: failed to retrieve path for /mmfs1/gscratch/pedslabs/Resources/containers/R/R-4.2.3.sif: lstat /mmfs1/gscratch/pedslabs/Resources: permission denied

Yeah, so all of the existing R installations have permission denied issues. But clearly
R can be installed with the existing packages on hyak.

pedslabs apparently uses apptainer to do this

# Maybe I should just try to reinstall xz utils? because according to the page (https://tukaani.org/xz/)
# one component of it is liblzma library. The 5.2.5 version of xz is what I have on klone;
we're up to 5.4.5 now

# alright, let's reinstall xz
https://github.com/tukaani-project/xz/releases/download/v5.4.5/xz-5.4.5.tar.gz
xz-5.4.5.tar.gz

First, navigate to /sw/contrib/stf-src/xz/
wget https://tukaani.org/xz/xz-5.4.5.tar.gz
tar xzvf xz-5.4.5.tar.gz
cd xz-5.4.5
./configure --prefix=/sw/contrib/stf-src
# make -j3
# make install
make && make install

# Then create a module file using these instructions: https://wiki.cac.washington.edu/display/hyakusers/Hyak_modules
# module file lives here:
/sw/contrib/modulefiles/stf/xz/5.4.5

# to load:
module load stf/xz/5.4.5

# Let's try to configure R again

module load cesg/readline/8.1
[mmin@n3319 R-4.2.0]$ ./configure --prefix=/sw/contrib/stf-src/R/R-4.2.0 --with-x=no --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin
configure: error: "liblzma library and headers are required"

So, I guess that didn't fix it

Back to this post: https://stackoverflow.com/questions/74821884/liblzma-library-and-headers-are-required-when-installing-r-4-2-2-and-dependenc

It seems to indicate that while I have liblzma installed (should be with xz utils), R
is not finding it. The environment variable PKG_CONFIG_PATH might help. From configure --help:
PKG_CONFIG_PATH
              directories to add to pkg-config's search path
              
So perhaps if I add the directory for xz, I could get it to search correctly

# Check current PKG_CONFIG_PATH:
[mmin@n3319 R-4.2.0]$ echo $PKG_CONFIG_PATH
/mmfs1/sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/lib/pkgconfig

# Woah, so it's just looking there? or it's additionally looking there? I think it's adding that:
From https://askubuntu.com/questions/210210/pkg-config-path-environment-variable:
PKG_CONFIG_PATH is a environment variable that specifies additional paths in which pkg-config 
will search for its .pc files.

This variable is used to augment pkg-config's default search path.

Can we add to it? Or will it just overwrite?
export PKG_CONFIG_PATH=/sw/contrib/stf-src/xz/xz-5.4.5

# Just overwrites. Well, let's see what happens when we try to configure now

./configure --prefix=/sw/contrib/stf-src/R/R-4.2.0 --with-x=no --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin
configure: error: "liblzma library and headers are required"
[mmin@n3319 R-4.2.0]$ echo $PKG_CONFIG_PATH
/sw/contrib/stf-src/xz/xz-5.4.5

So it's changed it now, but still not finding the right libraries?

.pc files are package configuration files

There is a liblzma.pc in /sw/contrib/stf-src/xz/xz-5.4.5/src/liblzma
# Let's try this:
export PKG_CONFIG_PATH=/sw/contrib/stf-src/xz/xz-5.4.5/src/liblzma/

[mmin@n3319 R-4.2.0]$ 
module load stf/xz/5.4.5
module load cesg/readline/8.1
./configure --prefix=/sw/contrib/stf-src/R/R-4.2.0 --with-x=no --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin
configure: error: "liblzma library and headers are required"

Tried export PKG_CONFIG_PATH=/sw/contrib/stf-src/xz/xz-5.4.5/src/liblzma/
(so added a backslash) - same configure error

### 2023-12-08 ###

Let's get a build node again
salloc -p compute -A stf -N 1 -c 1 --time=2:00:00 --mem=10G
Lol, no nodes available. Let's write then

### 2023-11-13 ###

Got a build node. Let's see if we can figure this out

[mmin@n3319 R-4.2.0]$ ./configure --prefix=/sw/contrib/stf-src/R/R-4.2.0 --with-x=no --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin
configure: error: --with-readline=yes (default) and headers/libs are not available

# try loading some modules
[mmin@n3319 R-4.2.0]$ module load cesg/readline/8.1
[mmin@n3319 R-4.2.0]$ ./configure --prefix=/sw/contrib/stf-src/R/R-4.2.0 --with-x=no --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin
configure: error: "liblzma library and headers are required"
# So we got readline working, but now we're back to that old error

# Trying to follow instructions at this link:
https://stackoverflow.com/questions/74821884/liblzma-library-and-headers-are-required-when-installing-r-4-2-2-and-dependenc

[mmin@n3396 R-4.2.0]$ echo $PKG_CONFIG_PATH
/mmfs1/sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/lib/pkgconfig

# So it looks like because we loaded the module for readline, we're in this bizarre pkg config path?
# If we cd that path, we find just this:
[mmin@klone-login03 /]$ cd /mmfs1/sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/lib/pkgconfig
[mmin@klone-login03 pkgconfig]$ ls
readline.pc

# So okay, the .pc file is there. Can we add that for liblzma library?

# It sounds like we can separate paths using a colon. So let's try and add some others

# first load the right module
[mmin@n3396 R-4.2.0]$ module load stf/xz/5.4.5

# okay, that didn't change the pkg config path at all; let's add to it

[mmin@n3396 R-4.2.0]$ export PKG_CONFIG_PATH=/sw/contrib/stf-src/xz/xz-5.4.5/src/liblzma

# For now, that just overwrote the old path

# Try config
[mmin@n3319 R-4.2.0]$ ./configure --prefix=/sw/contrib/stf-src/R/R-4.2.0 --with-x=no --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin

configure: error: "liblzma library and headers are required"

# Ok, what if we combine our configure paths?

# What if I forgot the mmfs1 prefix for liblzma?

[mmin@n3319 R-4.2.0]$ export PKG_CONFIG_PATH=/mmfs1/sw/contrib/stf-src/xz/xz-5.4.5/src/liblzma:/mmfs1/sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/lib/pkgconfig
[mmin@n3319 R-4.2.0]$ export PKG_CONFIG_PATH=/mmfs1/sw/contrib/stf-src/xz/xz-5.4.5/src/liblzma

configure: error: "liblzma library and headers are required"

# Alright, I'm stumped. Let's go directly into the configure file and start messing with that (bad idea? probably)

# Let's go back to the top and start trying stuff that worked before

[mmin@n3319 R-4.2.0]$ ./configure --prefix=/sw/contrib/stf-src/R-4.2.0 --with-pcre1

configure: error: --with-x=yes (default) and X11 headers/libs are not available

Well I mean that's a different error, so maybe that's progress?

# Let's try following the advice from here: https://unix.stackexchange.com/questions/215728/with-x-yes-default-and-x11-headers-libs-are-not-available
# That link leads to this link: https://forums.centos.org/viewtopic.php?t=5933

[mmin@n3396 R-4.2.0]$ ./configure --prefix=/sw/contrib/stf-src/R-4.2.0 --with-x=no
configure: error: "liblzma library and headers are required"

hahahaha ok well then that's settled, back to that problem

Well I found a different path to liblzma:
/mmfs1/sw/contrib/stf-src/lib/pkgconfig

change the config path to that?

[mmin@n3396 R-4.2.0]$ export PKG_CONFIG_PATH=/mmfs1/sw/contrib/stf-src/lib/pkgconfig
[mmin@n3396 R-4.2.0]$ echo $PKG_CONFIG_PATH

[mmin@n3319 R-4.2.0]$ ./configure --prefix=/sw/contrib/stf-src/R/R-4.2.0 --with-x=no --with-readline=sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/bin
configure: error: "liblzma library and headers are required"

[mmin@n3396 R-4.2.0]$ export PKG_CONFIG_PATH=/mmfs1/sw/contrib/stf-src/lib/pkgconfig:/mmfs1/sw/contrib/cesg-src/spack/opt/spack/linux-centos8-cascadelake/gcc-10.2.0/readline-8.1-utocvvei345jaiunes7cvd7t4ci5f4e5/lib/pkgconfig

Alright, let's actually try messing with the ./configure script

From: /Users/markusmin/Documents/CBR/steelhead

scp -r mmin@klone.hyak.uw.edu:/sw/contrib/stf-src/R-4.2.0/configure ./

I cannot figure out how to install from source. Let's start back at the beginning:
https://hyak.uw.edu/docs/tools/r/

# navigate to /gscratch/stf/mmin
# Now, let's use the latest version of R from Docker hub

[mmin@n3207 mmin]$ apptainer pull docker://r-base:4.3.2

# Looks like it created the SIF file

Can we run this?

[mmin@n3207 mmin]$ apptainer run r-base_4.3.2.sif R

[mmin@n3207 mmin]$ apptainer pull docker://r-base:4.3.2
INFO:    Converting OCI blobs to SIF format
INFO:    Starting build...
Getting image source signatures
Copying blob 82bb7b5b9130 done  
Copying blob c3a3551a86bb done  
Copying blob a5588a4c2d5f done  
Copying blob 1936a419ec91 done  
Copying blob 8abf5944b508 done  
Copying blob 786c9ee75a8d done  
Copying config c3d29bbf5c done  
Writing manifest to image destination
Storing signatures
2023/12/13 11:45:39  info unpack layer: sha256:786c9ee75a8d6bb4c357dfb6a4cff0b70faa3dd8a36a5024897ebb5c8c6cbba9
2023/12/13 11:45:41  info unpack layer: sha256:8abf5944b5083a0c580671271699cab00c74f246317e31ec85f267f886f5304d
2023/12/13 11:45:41  info unpack layer: sha256:1936a419ec91c802ef8611dd485e7008a355a2522a910bb9ec6341e580b45f5e
2023/12/13 11:45:41  info unpack layer: sha256:a5588a4c2d5f4c9e0e082001f0b9879811afe304b5027bf09166e26111640523
2023/12/13 11:45:41  info unpack layer: sha256:c3a3551a86bba8722ce6bcec6b17a41c8ffebde92618af58fe6ae01c44bd0056
2023/12/13 11:45:41  info unpack layer: sha256:82bb7b5b91304066c1d3399a85ed026abf276474590098710f0670d6ac6e79e7
INFO:    Creating SIF file...
[mmin@n3207 mmin]$ ls
JAGS_runs  r-base_4.3.2.sif  stan_runs
[mmin@n3207 mmin]$ apptainer run r-base_4.3.2.sif R

R version 4.3.2 (2023-10-31) -- "Eye Holes"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.



Wow. Well this would have saved me about 10 hours of frustration. 
But at least we now seem to have a working R version on Klone.

Let's see if we can reproduce our full R environment on Klone

# We need the following packages (at least) - also stan
install.packages("cmdstanr")
install.packages("posterior")
install.packages("lubridate")

> install.packages("tidyverse"")
Lots of messages like this:
Warning in dir.create(lockdir, recursive = TRUE) :
  cannot create dir '/usr/local/lib/R/site-library/00LOCK-fastmap', reason 'Read-only file system'
ERROR: failed to create lock directory ‘/usr/local/lib/R/site-library/00LOCK-fastmap’

> library(tidyverse)
Error in library(tidyverse) : there is no package called ‘tidyverse’

Installation failed. So yeah, I think I need to deal with where packages are installed,
since it's trying to write to places that I'm not allowed to

Trying to resolve this R library issue with apptainer

Note that all of this is occuring in /mmfs1/home/mmin, which is where you start by
default when you log in to Klone

[mmin@n3319 ~]$ salloc -p compute -A stf -N 1 -c 1 --time=2:00:00 --mem=10G
[mmin@n3319 ~]$ module load apptainer
[mmin@n3319 ~]$ cat ~/.Renviron
R_LIBS="/gscratch/stf/mmin/R"

# Okay, so that file I made earlier is showing up. That's good! What's weird
though is that when I run ls I don't see that file. So I guess the tilde is putting
me back in my home directory, and then it's looking there for the file?
# Either way, let's see now if I'm able to deal with the packages

[mmin@n3319 ~]$ apptainer run r-base_4.3.2.sif R
FATAL:   While checking container encryption: could not open image /mmfs1/home/mmin/r-base_4.3.2.sif: failed to retrieve path for /mmfs1/home/mmin/r-base_4.3.2.sif: lstat /mmfs1/home/mmin/r-base_4.3.2.sif: no such file or directory

# Okay, so now let's move over to gscratch where my apptainer is
> install.packages("lubridate")
Installing package into ‘/usr/local/lib/R/site-library’
(as ‘lib’ is unspecified)

- this didn't work again

Well, the interesting thing is that no matter what directory I run cat ~/.Renviron from, 
it now is able to find that file. TBH I still don't fully know where that is though or
how I created it

Let's try following the instructions here and see if I can install packages:
https://stackoverflow.com/questions/14382209/r-install-packages-returns-failed-to-create-lock-directory
[mmin@n3319 ~]$ apptainer run r-base_4.3.2.sif R
> install.packages("lubridate", INSTALL_opts = '--no-lock')
ERROR: unable to create ‘/usr/local/lib/R/site-library/cpp11’
ERROR: unable to create ‘/usr/local/lib/R/site-library/generics’
ERROR: unable to create ‘/usr/local/lib/R/site-library/timechange’
ERROR: unable to create ‘/usr/local/lib/R/site-library/lubridate’

So yeah, that didn't work


Let's try and get RStudio working
[mmin@n3319 mmin]$ apptainer pull docker://rocker/rstudio:4.1.0
INFO:    Creating SIF file...
FATAL:   While making image from oci registry: error copying image out of cache: could not copy file: write ./tmp-copy-2413335: disk quota exceeded

https://unix.stackexchange.com/questions/67890/disk-quota-exceeded-problem
I might be out of inodes? 
I'm going to try running installing instead from my home directory and see if that makes a difference

INFO:    Creating SIF file...
[mmin@n3278 ~]$ ls
rstudio_4.1.0.sif

Hey look, that worked!
[mmin@n3278 ~]$ wget https://hyak.uw.edu/files/rstudio-server.job
[mmin@n3278 ~]$ nano rstudio-server.job
``
RSTUDIO_CWD="/gscratch/stf/mmin/" # update this line
RSTUDIO_SIF="rstudio_4.1.0.sif" # update this line

export R_LIBS_USER="/gscratch/stf/mmin/R/"
``

Okay, so that actually worked! I now have RStudio working. I just
have to run sbatch rstudio-server.job from where my RStudio .SIF file is
However, we now have this problem when we try to install packages:

* installing *binary* package ‘Rcpp’ ...
cp: error writing '/gscratch/stf/mmin/R/Rcpp/./announce/ANNOUNCE-0.11.0.txt': Disk quota exceeded

I wonder if this is a gscratch thing? I guess we can try for now moving everything to 
a new folder on gscratch/scrubbed/R, with the knowledge that I'll have to reinstall packages
(not the end of the world)


code I ran:
``
mkdir /gscratch/scrubbed/mmin

# change a bunch of paths
[mmin@n3319 mmin]$ cat ~/.Renviron
R_LIBS="/gscratch/scrubbed/mmin"

[mmin@n3278 ~]$ nano rstudio-server.job
RSTUDIO_CWD="/gscratch/scrubbed/mmin/"
RSTUDIO_SIF="../../../mmfs1/home/mmin/rstudio_4.1.0.sif"

export R_LIBS_USER="/gscratch/scrubbed/mmin/"


# Try installing packages again
> install.packages("lubridate")
Installing package into ‘/gscratch/scrubbed/mmin’
(as ‘lib’ is unspecified)
also installing the dependencies ‘generics’, ‘Rcpp’

trying URL 'https://packagemanager.posit.co/cran/__linux__/focal/2021-08-09/src/contrib/generics_0.1.0.tar.gz'
Content type 'binary/octet-stream' length 68868 bytes (67 KB)
==================================================
downloaded 67 KB

trying URL 'https://packagemanager.posit.co/cran/__linux__/focal/2021-08-09/src/contrib/Rcpp_1.0.7.tar.gz'
Content type 'binary/octet-stream' length 4155309 bytes (4.0 MB)
==================================================
downloaded 4.0 MB

trying URL 'https://packagemanager.posit.co/cran/__linux__/focal/2021-08-09/src/contrib/lubridate_1.7.10.tar.gz'
Content type 'binary/octet-stream' length 2025845 bytes (1.9 MB)
==================================================
downloaded 1.9 MB

* installing *binary* package ‘generics’ ...
* DONE (generics)
* installing *binary* package ‘Rcpp’ ...
* DONE (Rcpp)
* installing *binary* package ‘lubridate’ ...
* DONE (lubridate)

The downloaded source packages are in
	‘/tmp/RtmpNh8fWA/downloaded_packages’
	
``

It worked!!!!! 
But it's still not perfect. Namely, we now have gscratch/scrubbed mounted,
which means I have to move my files over from gscratch/stf/mmin. But not the end of the world.

# Let's see if we can get cmdstanr using docker

Currently having issues with the version of R
Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) : 
  namespace ‘posterior’ 1.0.1 is being loaded, but >= 1.4.1 is required
  
# Let's see if we can install a more recent version of RStudio

$ apptainer pull docker://rocker/rstudio:4.3.2

# ok, change the .SIF path

# Holy smokes, this new RStudio has so many packages already installed, including
posterior, tidyverse, lubridate

# Skip right to installing cmdstanr
This looks like it worked too? Let's see if we can run anything

> setwd("/gscratch/stf/mmin")
> getwd()
[1] "/gscratch/stf/mmin"
> temps <- read.csv("/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/upper_columbia_hatchery/window_temps_for_stan.csv")
> View(temps)

LFG that worked! Ok, let's try submitting a job...?

Navigate and try to submit chains
mmin@n3338:/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/upper_columbia_hatchery$ bash UCH_submit_chains.sh 

UCH_submit_chains.sh: line 4: sbatch: command not found
UCH_submit_chains.sh: line 5: sbatch: command not found
UCH_submit_chains.sh: line 6: sbatch: command not found
UCH_submit_chains.sh: line 7: sbatch: command not found

So I have RStudio working, but that doesn't translate to being able to run jobs. Poop

# Alright, let's go back to our R .SIF file
[mmin@n3319 ~]$ salloc -p compute -A stf -N 1 -c 1 --time=2:00:00 --mem=10G
[mmin@n3319 ~]$ module load apptainer
[mmin@n3319 ~]$ apptainer run r-base_4.3.2.sif R

# Install packages isn't working, what if we change the lib argument?
> install.packages("lubridate", lib = "/gscratch/scrubbed/mmin")
Warning in install.packages("lubridate", lib = "/gscratch/scrubbed/mmin") :
  'lib = "/gscratch/scrubbed/mmin"' is not writable
Would you like to use a personal library instead? (yes/No/cancel) install.packages("lubridate", lib = "/gscratch/scrubbed/mmin/")
Error in askYesNo(gettext("Would you like to use a personal library instead?"),  : 
  Unrecognized response “install.packages("lubridate", lib = "/gscratch/scrubbed/mmin/")”
  
# Alright so that's not it

# So I don't actually think I need a custom apptainer, because I really just need to run cmdstan 
on hyak - everything else doesn't take as long and therefore can be run locally.
From here: https://github.com/storopoli/cmdstanr-docker/
$ apptainer pull docker://jstoropoli/cmdstanr

# File was created, let's try running it
[mmin@n3319 ~]$ apptainer run cmdstanr_latest.sif R
works!

# Let's see if we can get a very simple job to run

# Made a folder locally: /Users/markusmin/Documents/CBR/steelhead/stan_actual/klone_testing/

# Navigate to /Users/markusmin/Documents/CBR/steelhead/stan_actual/

# then run:
(base) markusmin@Markuss-MacBook-Pro-2 stan_actual % scp -r ./klone_testing/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

[mmin@klone-login03 klone_testing]$ sbatch job_klone_test.slurm 
Submitted batch job 15673863

### 2023-12-14 ###

Having a hard time getting it to find the R script that's in the same directory...

[mmin@n3396 klone_testing]$ apptainer exec ../../r-base_4.3.2.sif Rscript klone_test.R
Fatal error: cannot open file 'klone_test.R': No such file or directory

$ apptainer exec r-base_4.3.2.sif Rscript klone_test.R
Fatal error: cannot open file 'klone_test.R': No such file or directory

[mmin@n3396 klone_testing]$ apptainer exec ../../cmdstanr_latest.sif Rscript klone_test.R
Fatal error: cannot open file 'klone_test.R': No such file or directory
[mmin@n3396 klone_testing]$ apptainer run ../../cmdstanr_latest.sif Rscript klone_test.R
/usr/local/bin/Rscript: /usr/local/bin/Rscript: cannot execute binary file

So exec and run are different, give different errors
# helpful for understanding apptainer commands: https://medium.com/@dcat52/more-ways-to-run-apptainer-containers-d6fc688c3f1a

[mmin@n3396 klone_testing]$ apptainer shell ../../cmdstanr_latest.sif 
Apptainer> ls
Apptainer> pwd
/gscratch/stf/mmin/stan_runs/klone_testing

Why is there nothing here...?

Maybe I have to move things into my container: https://stackoverflow.com/questions/22907231/how-to-copy-files-from-host-to-docker-container

Maybe a persistent overlay is the answer?
[mmin@n3396 mmin]$ apptainer overlay create --size 1024 cmdstanr_latest.sif 
INFO:    Creating overlay image for use without fakeroot.
INFO:    Consider re-running with --fakeroot option.

Well, I have no idea what that did

A helpful apptainer guide: https://hsf-training.github.io/hsf-training-singularity-webpage/07-file-sharing/

Okay, so it sounds like my home directory should be accessible, and when I look there
for files from within my apptainer I can actually see things (unlike other places)

[mmin@n3396 stf]$ cd ../../../mmfs1/home/mmin
[mmin@n3396 ~]$ pwd
/mmfs1/home/mmin
[mmin@n3396 ~]$ apptainer exec ../../../gscratch/stf/mmin/r-base_4.3.2.sif Rscript klone_test.R
[mmin@n3396 ~]$ apptainer exec ../../../gscratch/stf/mmin/cmdstanr_latest.sif Rscript klone_test.R

Ok, these both worked! That is very good. But let's see if we can get gscratch bound so we can access

Let's try to bind gscratch/stf/mmin

[mmin@n3396 ~]$ pwd
/mmfs1/home/mmin
[mmin@n3396 ~]$ apptainer shell --bind ../../../gscratch/stf/mmin/:/gscratch/stf/mmin ../../../gscratch/stf/mmin/cmdstanr_latest.sif

Holy heck this worked!! Okay I have hope again, maybe I can try running some models tomorrow

Okay, let's try to figure out how to structure these jobs now

navigate to the folder where the models are
/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/upper_columbia_hatchery

the UCH_submit_chains.sh doesn't need to be changed
the jobs themselves definitely do

Modified job_stf_UC_H_T_Swinterdays_chain1.slurm 

Fatal error: cannot open file 'seed101_100iter_chain1_UC_H_T_Swinterdays.R': No such file or directory

Okay, maybe I need to bind gscratch every time I want to use it?

section from job:
##  Scripts to be executed here
apptainer shell --bind ../../../:/gscratch/stf/mmin ../../../cmdstanr_latest.sif
apptainer exec ../../../cmdstanr_latest.sif Rscript seed101_100iter_chain1_UC_H_T_Swinterdays.R

Fatal error: cannot open file 'seed101_100iter_chain1_UC_H_T_Swinterdays.R': No such file or directory

Okay yes you need to bind, but you also need to bind it every time you run apptainer apparently

apptainer shell --bind ../../../:/gscratch/stf/mmin ../../../cmdstanr_latest.sif 
Rscript seed101_100iter_chain1_UC_H_T_Swinterdays.R

^ this works when you run it from cmd line, will it work when you put it in the job?
Nope: /var/spool/slurmd/job15685926/slurm_script: line 26: Rscript: command not found

apptainer exec --bind ../../../:/gscratch/stf/mmin ../../../cmdstanr_latest.sif Rscript seed101_100iter_chain1_UC_H_T_Swinterdays.R

### 2023-12-15 ###

The model compiled, but then didn't run:

Running MCMC with 1 chain, with 28 thread(s) per chain...

Chain 1 [n3363:32348] OPAL ERROR: Unreachable in file ext3x_client.c at line 112
Chain 1 --------------------------------------------------------------------------
Chain 1 The application appears to have been direct launched using "srun",
Chain 1 but OMPI was not built with SLURM's PMI support and therefore cannot
Chain 1 execute. There are several options for building PMI support under
Chain 1 SLURM, depending upon the SLURM version you are using:
Chain 1
Chain 1   version 16.05 or later: you can use SLURM's PMIx support. This
Chain 1   requires that you configure and build SLURM --with-pmix.
Chain 1
Chain 1   Versions earlier than 16.05: you must use either SLURM's PMI-1 or
Chain 1   PMI-2 support. SLURM builds PMI-1 by default, or you can manually
Chain 1   install PMI-2. You must then build Open MPI using --with-pmi pointing
Chain 1   to the SLURM PMI library location.
Chain 1
Chain 1 Please configure as appropriate and try again.
Chain 1 --------------------------------------------------------------------------
Chain 1 *** An error occurred in MPI_Init
Chain 1 *** on a NULL communicator
Chain 1 *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
Chain 1 ***    and potentially your MPI job)
Chain 1 [n3363:32348] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes wer$
Warning: Chain 1 finished unexpectedly!

Warning message:
No chains finished successfully. Unable to retrieve the fit.
Error: No chains finished successfully. Unable to retrieve the draws.
Execution halted



It appears related to how cmdstan was configured? Which to me indicates that perhaps 
it is related to the docker container I pulled?

I'm going to try pulling some other cmdstanr containers and see if those work
Try this one: https://github.com/mattocci27/r-containers
First removed old cmdstanr container (see like 4299 - just search "apptainer pull")
$ apptainer pull docker://mattocci/cmdstanr
[mmin@n3319 ~]$ apptainer run cmdstanr_latest.sif R
Okay, this one works too. Let's try submitting the job with this new .sif file

[mmin@n3396 upper_columbia_hatchery]$ sbatch job_stf_UC_H_T_Swinterdays_chain1.slurm 

### 2024-01-03 ###

Back in the saddle! Let's see how that run did that I submitted

Attaching package: ‘lubridate’

The following objects are masked from ‘package:base’:

    date, intersect, setdiff, union

Error: CmdStan path has not been set yet. See ?set_cmdstan_path.
Execution halted

So we got less far than last time.

It also looks like that is actually the wrong job - we're running a model without year effects.
So we should make sure that we actually set things up with the year model. But then again we 
need to get cmdstan to work, so let's not wory about that for now.


Try this cmdstanr container:
https://github.com/JBris/stan-cmdstanr-docker

[mmin@klone-login01 mmin] $ salloc -p compute -A stf -N 1 -c 1 --time=2:00:00 --mem=10G
[mmin@n3319 ~]$ module load apptainer
[mmin@n3319 ~]pwd
/gscratch/stf/mmin
[mmin@n3319 ~]$ apptainer pull docker://ghcr.io/jbris/stan-cmdstanr-docker:latest
[mmin@n3319 ~]$ apptainer run stan-cmdstanr-docker_latest.sif R

Works!

Okay, let's modify job_stf_UC_H_T_Swinterdays_chain1.slurm to run this new container
old: apptainer exec --bind ../../../:/gscratch/stf/mmin ../../../cmdstanr_latest.sif Rscript seed101_100iter_chain1_UC_H_T_Swinterdays.R
new: apptainer exec --bind ../../../:/gscratch/stf/mmin ../../../stan-cmdstanr-docker_latest.sif Rscript seed101_100iter_chain1_UC_H_T_Swinterdays.R

[mmin@klone-login01 upper_columbia_hatchery]$ nano job_stf_UC_H_T_Swinterdays_chain1.slurm 
[mmin@klone-login01 upper_columbia_hatchery]$ sbatch job_stf_UC_H_T_Swinterdays_chain1.slurm

### 2024-01-04 ###

Checking run status

[mmin@klone-login01 upper_columbia_hatchery]$ pwd
/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_models/upper_columbia_hatchery
[mmin@klone-login01 upper_columbia_hatchery]$ nano slurm-15926170.out 

````
[1] "Compiling model"
[1] "model compiled at"
[1] "2024-01-03 18:32:09 PST"
Running MCMC with 1 chain, with 28 thread(s) per chain...

Chain 1 ./parallel_upper_columbia_03_stan_actual_int_origin_hatchery_temp_spill_winterdays_deteff: error while loading shared libraries: libboost_serialization.so.1.78.0: cannot open shared object file: No such file or directory
Warning: Chain 1 finished unexpectedly!
````

Okay, so we once again were able to compile the model, but this time we had an error while loading shared libraries.


# This other run got similarly far (i.e., compiled then broke)
# Not sure which got further
[mmin@klone-login01 upper_columbia_hatchery]$ nano slurm-15685941.out 

Honestly I don't know what to do here. Let's try messing with the MPI stuff?
From the klone documentation, maybe I need to load the intel software suite that includes MPI?
https://hyak.uw.edu/blog/klone/
"LMOD software modules:
Intel has bundled their software suite (e.g., compiler, MPI) as oneCLI and we created this module (i.e., module load intel/oneCLI).""

I am going to reinstall the storopoli version - this will overwrite the mattoci version

$ apptainer pull docker://jstoropoli/cmdstanr
[mmin@klone-login01 mmin] $ salloc -p compute -A stf -N 1 -c 1 --time=2:00:00 --mem=10G
[mmin@n3319 ~]$ module load apptainer
[mmin@n3319 ~]pwd
/gscratch/stf/mmin
[mmin@n3319 ~]$ apptainer pull docker://jstoropoli/cmdstanr
[mmin@n3319 ~]$ apptainer run stan-cmdstanr-docker_latest.sif R

Alright, let's modify the job to load the intel module

[mmin@n3296 mmin]$ module load intel/oneCLI
Lmod has detected the following error:  The following module(s) are unknown: "intel/oneCLI"

Well, that didn't work. Let's check module list

Okay, so by default we have this module loaded: ompi/4.1.4        (D)

Do we have the wrong version of ompi in our docker compare to what's loaded by default? Could that be causing incompatibility?
https://users.open-mpi.narkive.com/jbBefsgW/ompi-openmpi-pmix-slurm
- this seems to indicate that perhaps this could be an issue

On the storopoli dockerfile we have these lines for openMPI
# install openMPI and MPI's mpicxx binary and libglpk for brms
RUN apt-get update && apt-get install -y --no-install-recommends build-essential curl libopenmpi-dev mpi-default-dev libglpk-dev

- doesn't list a version number for openMPI

What version of slurm do we have?
[mmin@n3296 mmin]$ sinfo -V
slurm 23.02.6

Okay, so definitely later than version 16. From the error message:
Chain 1 The application appears to have been direct launched using "srun",
Chain 1 but OMPI was not built with SLURM's PMI support and therefore cannot
Chain 1 execute. There are several options for building PMI support under
Chain 1 SLURM, depending upon the SLURM version you are using:
Chain 1
Chain 1   version 16.05 or later: you can use SLURM's PMIx support. This
Chain 1   requires that you configure and build SLURM --with-pmix.

Configure and build SLURM again? Seems basically impossible

What if we just add flags to the sbatch? Changed the .sif file back
to the storopoli cmdstanr version, then trying this:
[mmin@klone-login01 upper_columbia_hatchery]$ sbatch job_stf_UC_H_T_Swinterdays_chain1.slurm —mpi=pmix_v1
Submitted batch job 15948124

Holy shit it's running. Praise the maker!
Confirmed that the max time is 240 hours on klone

To-do:
1. Let's add some information to our sbatch scripts (like email notifications)
2. It also looks like that is actually the wrong job - we're running a model without year effects.
So we should make sure that we actually set things up with the year model.
3. Make sure that every model runs with mtd 12
4. Modify the UCW_submit_chains.sh scripts to include necessary flags

Okay, I did this. Let's move this folder to klone
(base) markusmin@Markuss-MacBook-Pro-2 stan_actual % scp -r ./rear_temp_spill_winterdays_year/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

There are way too much test scripts in the UCW folder. Will need to clean up soon.

Naming of folders is different on klone and on local. That's uber confusing.

The most recent model runs are in rear_temp_spill_winterdays_year, on both local and on hyak

Alright, I submitted four chains each for UCH and UCW. We don't yet have a node available, but
hopefully soon.

### 2024-01-09 ###

Two chains are successfully running, the other six failed due to typos. Going to correct and resubmit

Ok, two of the UCW runs failed, and not because of a typo:

````
Running MCMC with 1 chain, with 28 thread(s) per chain...

Chain 1 ./parallel_upper_columbia_03_stan_actual_int_origin_wild_temp_spill_winterdays_year_deteff: error while loading shared libraries: libtbb.so.2: cannot open shared object file: No such file or directory
Warning: Chain 1 finished unexpectedly!

Warning message:
No chains finished successfully. Unable to retrieve the fit.
Error: No chains finished successfully. Unable to retrieve the draws.
Execution halted
````

We've had that error before, but I'm not sure why we'd be getting that now - that seems
to be a docker/apptainer issue

Ah - I still had an old executable. Removed it, resubmitted, and now all runs are going!

### 2024-01-10 ###

Let's check on our runs! They need to be getting through 100 iter per hour in order for them to finish in the 240
allotted hours.
In 20 hours, UCH:
Chain 1
Chain 1 Iteration:   10 / 2000 [  0%]  (Warmup)
Chain 1 Iteration:   20 / 2000 [  1%]  (Warmup)
Chain 1 Iteration:   30 / 2000 [  1%]  (Warmup)
Chain 1 Iteration:   40 / 2000 [  2%]  (Warmup)


Yeah, so that's not going to finish. I might get 500 iter, which would presumably be good enough
for inference, but I'd want to run it longer in the future. Or I could try to reparameterize
to get better performance.

UCW has about the same performance, which is surprising
Chain 1 Iteration:   10 / 2000 [  0%]  (Warmup)
Chain 1 Iteration:   20 / 2000 [  1%]  (Warmup)
Chain 1 Iteration:   30 / 2000 [  1%]  (Warmup)
Chain 1 Iteration:   40 / 2000 [  2%]  (Warmup)
Chain 1 Iteration:   50 / 2000 [  2%]  (Warmup)

These are not going to finish in time. I should probably resubmit them as 500 iter runs.
Or work on reparameterization.

### 2024-01-11 ###

Checking on UCW runs, it looks like they're taking very different amounts of time to run. Some might finish, others definitely won't.
No way the UCH runs will finish. About 30 iter per day, which is actually terrible. Like worse than before...? 
But maybe that was just a different model

### 2024-01-12 ###

Looks like the UCW runs have caught up to each other. They're at 35%, with under three days of run time.
They'll finish in ten days! Unless it takes forever to save...
No way the UCH runs are finishing.

Let's try improving efficiency. A note from 2023-11-07:
````
# Okay - some thoughts on efficiency
Currently, we are tracking upwards of 450k parameters, when we really only care about 1250.
The reason for this is how we set up indexing; we just use a ton of big arrays, that are mostly
just empty, but we are still tracking the emptiness
The purpose of the arrays is to index [from, to]
BUT - what if we changed it so that our parameters were in a vector instead
And then the arrays were coded outside of the model - so you don't have to track the
full array of parameters, but instead have an array which is just input,
and each element of the array (which is [from, to]) is a number, which gives you the index
number of the vector. So like from = 1, to = 1 would be the first element of the vector
from = 1, to = 2 would be the second element of the vector.
The vector for each type of parameter would only have to be the length of the possible movements, aka 61.
Currently the array for each type of parameter is 43 x 43 = 1849.
So in theory we could cut down the number of things we have to track by 30.
There are some parameters (like spill) that only have a handful of places that even get a parameter.
Those could in theory be only the same length as the number of parameters, but I think that keeping
them all length 61 would be much better for indexing/convenience.
````

Let's work on tracking less parameters, this in theory should be a relatively minor change.

When I inspect the output of the model as it is right now, a 250iter chain
is 15 MB. For UCW, we are tracking  467241 parameters. We only care about 1224 of those.
A whole lot of zeros are being tracked.

We should be able to remove one dimension of each array that contains parameters. So,
the arrays that currently contain most parameters can be converted from matrices to vectors,
and the arrays for year effects can be converted from arrays to matrices

For example, to convert the container that contains b0 parameters from a matrix to array
Current code: b0_matrix_DE[current, k]
We basically need another matrix (that, crucially, is passed as data!) converts [current,k]
into a single number that we can use to index 
- we'll call this parameter_indices_matrix

there are 43 states total, including an upstream and a downstream state for some
tributaries and the loss state

there are 61 possible non-loss movements which means that...

We need a 61x1 vector for most parameters

Need to figure out what the deal is with dropping states that no fish have ever visited.
We currently drop them using a very roundabout, and very not reproducible, and very easy
to mess up method. And I'm not sure it's even entirely necessary. I should update the
workflow. But we can have a few blanks in that vector, and I think it should be fine

In the old model we included parameters for movements that aren't observed in our data.

The new parameterization makes the tracking of derived parameters way too complicated. I'm
fairly confident we can do this afterwards with the chains, instead of tracking it in the model. Deleted
that portion of the model

Okay, I changed the organization for the b0 parameters into a vector from a matrix. Let's test.

# move the reparameterization folder over to klone
(base) markusmin@Markuss-MacBook-Pro-2 stan_actual % pwd
/Users/markusmin/Documents/CBR/steelhead/stan_actual
(base) markusmin@Markuss-MacBook-Pro-2 stan_actual % scp -r ./reparameterization/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/


# Get a build node on klone:

[mmin@klone-login01 mmin] $ salloc -p compute -A stf -N 1 -c 1 --time=2:00:00 --mem=10G

# try submitting chain 1

[mmin@n3392 upper_columbia_wild]$ pwd
/gscratch/stf/mmin/stan_runs/reparameterization/upper_columbia_wild

apptainer exec --bind ../../../:/gscratch/stf/mmin ../../../cmdstanr_latest.sif Rscript seed101_1000iter_chain1_UCW_reparam.R

After fixing a few typos, model is compiling. I think this approach will work. Time to do it to all of the parameters!

### 2024-01-14 ###

The UCW runs finished!

Chain 1 Iteration: 1990 / 2000 [ 99%]  (Sampling)
Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1 finished in 371747.0 seconds.
Warning: 1 of 1000 (0.0%) transitions ended with a divergence.
See https://mc-stan.org/misc/warnings for details.

Diagnostics look good. It would appear that the iterations don't run at a consistent speed
- they started off really slow, in the first 20 hours we got through about 50 iter
- in the next 48 hours, we got through about 700 iter
- then in the next 48 hours (maybe less), we got through 1300 iter and saving the run

UCH:
Chain 1 Iteration:  280 / 2000 [ 14%]  (Warmup)
Chain 1 Iteration:  290 / 2000 [ 14%]  (Warmup)

So, this one seems to be going faster too, but not at the same rate

### 2024-01-16 ###

Working on reparameterizing. Looks like I never actually wrote the R code to cat the
stan code for all of the changes I made before. Working on fixing that in the
stan_data_prep_UCW_reparam.R script.

Some weird observations:
-100000 values
	- They should definitely be in there for the b0 parameters, but for everything else I don't
believe that we want them (I'm not sure that it actually interferes with the model, but they don't serve a purpose)
	- In the model they're currently found in the year parameters; in the script that uses cat to produce the model,
it's found in the temp and the year section. Get rid of those! They should just be skipped, and
left with zeros
- Inconsistency - report says that origin and year setup are the same - they're not.
Currently, year setup doesn't have an effect for movements back from tributaries into
the mainstem, but origin does. Why? I mean this could be justified, but it currently isn't.
Meeting notes from 7/19/23 mention stuff with tributaries and year effects; it would
make sense to drop the year effect for tributaries back to mainstem, because presumably
there's basically no data to estimate that effect. Also notes from 7/7/23 talking 
about this same issue, where we just don't have enough data to include every effect.
So it makes sense to drop it, it's just not documented well.

Bigger question - do we need DE v NDE parameters for movements that aren't into tributaries, but
presumably would be affected by that DE in the tributary, since it's a different option?
- I think no, because the only term that would be biased by that is the loss term

What's the deal with movements into upstream states?
- in DE, they should be impossible. in NDE, they get all of the same parameters
as a movement into the mouth state

### 2024-01-17 ###

In the last version of the model, there weren't origin effects for 
all movements from upstream back to mainstem
- In DE, these shouldn't be allowable, but in NDE they should just be given
the same parameter as movements from river mouth to mainstem
- but is this related to the states in which we observed fish?

Old:
borigin1_matrix_DE[parameter_indices_matrix[24,5] = borigin1_matrix_24_5;
borigin1_matrix_NDE[parameter_indices_matrix[24,5] = borigin1_matrix_24_5;
borigin1_matrix_DE[parameter_indices_matrix[26,6] = borigin1_matrix_26_6;
borigin1_matrix_NDE[parameter_indices_matrix[26,6] = borigin1_matrix_26_6;

New:
borigin2_vector_DE[52] = borigin2_matrix_24_5;
borigin2_vector_NDE[52] = borigin2_matrix_24_5;
borigin2_vector_NDE[53] = borigin2_matrix_24_5;
borigin2_vector_DE[54] = borigin2_matrix_26_6;
borigin2_vector_NDE[54] = borigin2_matrix_26_6;
borigin2_vector_NDE[55] = borigin2_matrix_26_6;

Put them back for NDE. The old model also allowed movements from upstream back to mainstem
in DE, which doesn't make sense...?

It would appear that I was very inconsistent with how I did or didn't remove states -
it looks like there were priors on parameters for movements that I removed from the model
because no fish ever visited that state

There are some minor discrepancies between the code to detect which states were never visited,
and which are coded as such in the model. Code shows 23 states never visited, model previously
had 21; differences are "Hood River Mouth"  and "Entiat River Upstream"

Code has now been updated to cut those out. When I copy over the parameters and such there were probably be a lot
of small changes
- TOMORROW: Run through all of the code that generates stan code via cat, and copy it all over again
- Ideally, it would be great if we could create an entire stan script from scratch from an R script...
- Might be worth it if we need to keep recreating models!
	- I don't think is actually practical, because the actual modeling portion (minus declaring parameters and priors etc.)
	doesn't lend itself well to automation

### 2024-01-18 ###

Fixing some more stuff in the data:
# elements of data affected by transition_matrix:
x possible_movements
x movements
x not_movements
x nmovements
x parameter_indices_matrix (this isn't vital to update, but I think it would be more efficient if it were updated)
x n_notmovements
x transition_matrix

this will have to be changed, but isn't affected by transition_matrix: sigma_year_indices = as.matrix(sigma_year_matrix_names[,1:2]),


# Use the transition matrix to calculate the possible movements
possible_movements <- rowSums(transition_matrix)

# Get the indices that are 1s (except loss, since that's 1 minus the others)
movements <- which(transition_matrix[,1:(nstates-1)] == 1, arr.ind = TRUE)
nmovements <- dim(movements)[1]

# Now get all of the movements which are fixed to zero
not_movements <- which(transition_matrix[,1:(nstates-1)] == 0, arr.ind = TRUE)
n_notmovements <- dim(not_movements)[1]

### Now that I've cut out all of the states that aren't visited, let's revise the model

# the number of possible transitions has gone WAY down - here's the old block vs. new block of intercept parameters:

# OLD:
real b0_matrix_1_2;
real b0_matrix_2_1;
real b0_matrix_2_3;
real b0_matrix_2_10_DE;
real b0_matrix_2_10_NDE;
real b0_matrix_2_12_DE;
real b0_matrix_2_12_NDE;
real b0_matrix_2_14_DE;
real b0_matrix_2_14_NDE;
real b0_matrix_2_16_DE;
real b0_matrix_2_16_NDE;
real b0_matrix_2_18_DE;
real b0_matrix_2_18_NDE;
real b0_matrix_2_41;
real b0_matrix_3_2;
real b0_matrix_3_4;
real b0_matrix_3_8;
real b0_matrix_3_20_DE;
real b0_matrix_3_20_NDE;
real b0_matrix_3_22_DE;
real b0_matrix_3_22_NDE;
real b0_matrix_4_3;
real b0_matrix_4_5;
real b0_matrix_5_4;
real b0_matrix_5_6;
real b0_matrix_5_24_DE;
real b0_matrix_5_24_NDE;
real b0_matrix_6_5;
real b0_matrix_6_7;
real b0_matrix_6_26_DE;
real b0_matrix_6_26_NDE;
real b0_matrix_7_6;
real b0_matrix_7_28_DE;
real b0_matrix_7_28_NDE;
real b0_matrix_7_30_DE;
real b0_matrix_7_30_NDE;
real b0_matrix_7_42;
real b0_matrix_8_3;
real b0_matrix_8_9;
real b0_matrix_8_32_DE;
real b0_matrix_8_32_NDE;
real b0_matrix_9_8;
real b0_matrix_9_34_DE;
real b0_matrix_9_34_NDE;
real b0_matrix_9_36;
real b0_matrix_9_37;
real b0_matrix_9_38;
real b0_matrix_9_39_DE;
real b0_matrix_9_39_NDE;
real b0_matrix_10_2;
real b0_matrix_12_2;
real b0_matrix_14_2;
real b0_matrix_16_2;
real b0_matrix_18_2;
real b0_matrix_20_3;
real b0_matrix_22_3;
real b0_matrix_24_5;
real b0_matrix_26_6;
real b0_matrix_28_7;
real b0_matrix_30_7;
real b0_matrix_32_8;
real b0_matrix_34_9;
real b0_matrix_36_9;
real b0_matrix_37_9;
real b0_matrix_38_9;
real b0_matrix_39_9;
real b0_matrix_41_2;
real b0_matrix_42_7;


# NEW:
real b0_matrix_1_2;
real b0_matrix_2_1;
real b0_matrix_2_3;
real b0_matrix_2_10_DE;
real b0_matrix_2_10_NDE;
real b0_matrix_3_2;
real b0_matrix_3_4;
real b0_matrix_3_8;
real b0_matrix_4_3;
real b0_matrix_4_5;
real b0_matrix_5_4;
real b0_matrix_5_6;
real b0_matrix_5_24_DE;
real b0_matrix_5_24_NDE;
real b0_matrix_6_5;
real b0_matrix_6_7;
real b0_matrix_6_26_DE;
real b0_matrix_6_26_NDE;
real b0_matrix_7_6;
real b0_matrix_7_28_DE;
real b0_matrix_7_28_NDE;
real b0_matrix_7_30_DE;
real b0_matrix_7_30_NDE;
real b0_matrix_7_42;
real b0_matrix_8_3;
real b0_matrix_8_9;
real b0_matrix_9_8;
real b0_matrix_9_36;
real b0_matrix_10_2;
real b0_matrix_24_5;
real b0_matrix_26_6;
real b0_matrix_28_7;
real b0_matrix_30_7;
real b0_matrix_36_9;
real b0_matrix_42_7;

# What happened:
- we cut out a ton of tributary states!
	- With the setup we have, we now have an issue with how the parameter_indices_matrix is set up
	- We need those states to get zeros, but the indexing is weird
	- Solution: Put a single spot in the vector that contains each type of parameter, and
	populate that with either a -100000 (for b0) or 0 (for all covariates)
		- Every spot in the transition matrix that isn't allowable (is a zero) will get this
		
		
Also, removed this from data, because it wasn't being referenced in the script:
 sigma_year_indices = as.matrix(sigma_year_matrix_names[,1:2]),

Next step: Try compiling! Locally first

# Resolved a bunch of errors, but now getting an odd compilation error when running locally:
> print("Compiling model")
[1] "Compiling model"
> mod$compile(cpp_options = list(stan_threads = TRUE))
Compiling Stan program...
error: PCH file uses an older PCH format that is no longer supported

1 error generated.

make: *** [/var/folders/8c/vz24vqzd2nzfvknzmfcybj0m0000gn/T/RtmpacV70I/model-65ab874ae9] Error 1

Error: An error occured during compilation! See the message above for more information.
In addition: Warning messages:
1: In readLines(hpp_path) :
  incomplete final line found on '/var/folders/8c/vz24vqzd2nzfvknzmfcybj0m0000gn/T//RtmpacV70I/model-65ab874ae9.hpp'
2: CmdStan's precompiled header (PCH) files may need to be rebuilt.
If your model failed to compile please run rebuild_cmdstan().
If the issue persists please open a bug report. 


# Let's try moving to klone and running there

(base) markusmin@Markuss-MacBook-Pro-2 stan_actual % scp -r ./reparameterization/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

# Get a build node to try compiling

[mmin@klone-login03 upper_columbia_wild]$ pwd
/gscratch/stf/mmin/stan_runs/reparameterization/upper_columbia_wild
[mmin@klone-login03 upper_columbia_wild]$ salloc -p compute -A stf -N 1 -c 1 --time=1:00:00 --mem=10G
[mmin@n3263 upper_columbia_wild]$  apptainer exec --bind ../../../:/gscratch/stf/mmin ../../../cmdstanr_latest.sif Rscript seed101_1000iter_chain1_UCW_reparam.R

# This failed with the familiar OMPI error, so I resubmitted using a job and the —mpi=pmix_v1 flag
[mmin@n3263 upper_columbia_wild]$ sbatch job_stf_UCW_reparam_chain1.slurm —mpi=pmix_v1
Submitted batch job 16151272

### 2024-01-19 ###
Job is taking forever to start, can we run this using a build node + the flag?
[mmin@n3263 upper_columbia_wild]$ apptainer exec --bind ../../../:/gscratch/stf/mmin ../../../cmdstanr_latest.sif Rscript seed101_1000iter_chain1_UCW_reparam.R —mpi=pmix_v1
Didn't work, same error as before. Maybe if I ask for a build node with that flag?
[mmin@klone-login03 ~]$ salloc -p compute -A stf -N 1 -c 1 --time=1:00:00 --mem=10G —mpi=pmix_v1
salloc: Pending job allocation 16156223
salloc: job 16156223 queued and waiting for resources
salloc: job 16156223 has been allocated resources
salloc: Granted job allocation 16156223
salloc: Nodes n3263 are ready for job
salloc: error: _fork_command: Unable to find command "—mpi=pmix_v1"

Nope!! Yeah I don't know how to do this then... Let's set up test runs using ckpt instead so we don't have to wait for stf
- It's running! And it looks to be running quite quickly. 40 iter in 23 minutes total run time
- Cancelled the ckpt run

submitted 4 stf runs at 9 am on 1/19

UCH runs got through 60% in 10 days, so will certainly require a different solution for that model

### 2024-01-23 ###


Chain 1 Iteration: 1980 / 2000 [ 99%]  (Sampling)
Chain 1 Iteration: 1990 / 2000 [ 99%]  (Sampling)
Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1 finished in 14154.4 seconds.
Warning: 5 of 1000 (0.0%) transitions ended with a divergence.
See https://mc-stan.org/misc/warnings for details.


UCW reparam chains finished in just four hours!!
For comparison, the old model took over 100 hours to run. This is a 25x improvement in performance. Wild


# Pull these runs, also pull the other runs pre-reparam for comparison

(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % pwd
/Users/markusmin/Documents/CBR/steelhead/stan_actual/reparameterization
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/upper_columbia_wild ./

(base) markusmin@Markuss-MacBook-Pro-2 rear_temp_spill_winterdays_year % pwd
/Users/markusmin/Documents/CBR/steelhead/stan_actual/rear_temp_spill_winterdays_year
(base) markusmin@Markuss-MacBook-Pro-2 rear_temp_spill_winterdays_year % scp -r mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/rear_temp_spill_winterdays_year/upper_columbia_wild ./

# Okay, I made a typo in the job names and accidentally ran chain 1 four times. Most unfortunate.
# I dropped the time requested from 240 to 24 hours, move it back and resubmit:

(base) markusmin@Markuss-MacBook-Pro-2 stan_actual % scp -r ./reparameterization/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/
[mmin@klone-login03 upper_columbia_wild]$ bash UCW_reparam_submit_chains.sh 

# Running into an issue trying to work with the files from the old parameterization - going to try this approach to deal with it:
https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached

````
Step 1: Open terminal,

Step 2:

cd ~
touch .Renviron
open .Renviron
Step 3: Save the following as the first line of .Renviron:

R_MAX_VSIZE=100Gb 
````

Ok, this fixed it!

Ok, this still takes forever to compare model outputs though. Why don't we run it directly on klone?

### 2024-01-24 ###

Let's get an interactive RStudio window on Klone
- I created a new job to run RStudio on ckpt, so that hopefully I don't have to wait in the queue as long
- I renamed the other job to explicitly say it's running RStudio on stf

[mmin@klone-login03 ~]$ pwd
/mmfs1/home/mmin
[mmin@klone-login03 ~]$ sbatch rstudio-server-ckpt.job 
Submitted batch job 16247807

             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          16247807      ckpt rstudio-     mmin PD       0:00      1 (AssocGrpJobsLimit)


### 2024-01-25 ###

Comparing model outputs and they look very similar - there are just a few transitions/states that are issues
across multiple parameters for the same movement, that weren't an issue in the old model
7 -> 30 and 7 -> 6 are common issues
6 = mainstem, RRE to WEL
7 = mainstem, upstream of WEL
30 = Methow River Mouth

7 -> 30 seems to only be an issue for DE, not NDE


# what to investigate?
- model outputs
	- check on the chains
- model code
	- the indexing looks fine, in terms of the parameter_indices_matrix, and how it's being indexed to in the stan model
- data itself (but this doesn't seem like a good hypothesis, since the data is the same between the two models)

Okay, so those two are very correlated for a portion of the chain - they wander together

Let's download the latest runs and then see how that looks

(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/upper_columbia_wild ./

Same issue as before, but it only happens in some chains
- when you go to four chains, it's 7_30 and 7_6 for intercept, both origin terms
- 7_28 seems to have some issues as well but less

Okay I get that those parameters would be correlated given how it's set up with a categorical logit.
But why is it only happening for movements out of state 7? That part doesn't make sense
- given that the model code doesn't seem to be wrong, let's check the data
- but why would it be the data?? it was fine last time
	- update: it was not as bad last time, but it was not fine. Similar issues of correlations between parameters,
	just to a lesser degree

>   table(state_transitions$transition_numeric)
 1 - 2 10 - 2  2 - 1 2 - 10  2 - 3 24 - 5 26 - 6 28 - 7  3 - 2  3 - 4  3 - 8 30 - 7  4 - 3  4 - 5 5 - 24 5 - 25  5 - 4  5 - 6 6 - 26 
    31     37     21     44   1026      4     10      1      9   1029      3     32     18   1011    126     96     13    731    301 
6 - 43  6 - 5  6 - 7 7 - 28 7 - 29 7 - 30 7 - 31 7 - 42  7 - 6  8 - 9 9 - 36 
     7     40    518     12      2    220      8      1    169      3      1 

So those are reasonable, those state transitions...
Note that the two dominant transitions out of state 7 (7 - 30 and 7-6) are the ones that have that issue
- but note that this is pretty similar to the counts of transitions out of state 6, but that state doesn't have the same issues
- My guess is that it's the proportion of transitions that fall into just two states that's the main thing
	- If you look, all of the movements to states that aren't 30 or 6 have max 12 observations, whereas those each have a lot more...
	the distributions are just more skewed
- But also, it might be related to the origin issue - because those are negatively correlated with the intercept,
which brings the overall probability back closer to zero

# Mark says to try to constrain the parameter values by reducing the SD in the prior
- Okay, I changed all of the parameters that used to have normal(0,10) priors to 
instead be normal(0,5) priors

# Move this to hyak and try running again
(base) markusmin@Markuss-MacBook-Pro-2 stan_actual % scp -r ./reparameterization/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

[mmin@klone-login03 upper_columbia_wild]$ rm UCW_reparam
[mmin@klone-login03 upper_columbia_wild]$ bash UCW_reparam_submit_chains.sh 


### 2024-01-26 ###

Runs with tighter priors finished faster.
With normal(0,10) priors: run time 4:06, 4:01, 3:53, 4:28
With normal(0,5) priors: run time 2:36, 3:13, 2:33, 2:32

# Pull the tighter prior runs, re-run comparison analysis
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/upper_columbia_wild ./


# Looks way better, rhat and ESS values are great. But some chains still have some annoying pathologies

Mark's suggestion is to run longer and start thinning.

Let's resubmit the UCW chains, now with longer chains and thinning
10000 iter, thin = 10 should be good, and should finish in <48 hours easily

Updated the existing R scripts to be 10000 iter warmup, sampling, refresh = 100, thin = 10

Move to hyak and run:
(base) markusmin@Markuss-MacBook-Pro-2 stan_actual % scp -r ./reparameterization/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

# 10000 iter thin 10 chain finished in 15-16 hours

### 2024-01-29 ###

Chains seemed to run with 10000 iter, so let's download and examine

# Pull the tighter prior runs, re-run comparison analysis
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/upper_columbia_wild ./

Turns out I messed up some of the file naming conventions for the .rds files but that's okay

Diagnostics look great, except that thin = 10 led to anticorrelated chains. So now let's 
resubmit the runs, but instead don't thin at all and just save the full 10000 iter

Edit the seed101 rscripts to not thin and save with corrected names

Move to hyak and run:
(base) markusmin@Markuss-MacBook-Pro-2 stan_actual % scp -r ./reparameterization/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/

### 2024-01-31 ###

Runs finished in about the same time as before. Let's pull them - hopefully the files aren't massive

(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/upper_columbia_wild ./

Each chain is 290 MB - so yeah, 10x larger than the old ones with thin = 10


### 2024-02-01 ###

The thinning comparison seems to indicate that thin = 2 is optimal for UCW. But that may not
be the case for the other datasets, so let's get those up and running.

Working on UCH today, with the reparam. Starting from UCW reparam files for everything.

Asking for one week (144 hours) on stf to try running 2000 warmup/2000 iter for UCH

Making the names of files like the R scripts that we are running on hyak shorter
chain1_UCH_reparam.R instead of seed101_1000iter_chain1_UCW_reparam
- different seeds are implicit, number of iter might change

I need to organize files a bit better to keep track of them; the best practice might
be to have subfolders within each of the dataset runs

Okay, I think UCH run is ready. Let's move to hyak and test on ckpt
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r ./upper_columbia_hatchery/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/

Model successfully compiled and is running. STF chains submitted at 10:33 am on 2/1
There's a typo in the R script where the output is incorrectly named "UCW" - need to change that once they download

### Let's work on Middle Columbia models next

# Start with MCH
Duplicated UCH as a starting point

# Needed to move the states complete file to the new folder.
Based on the 3.5 R script, I copied this file over:
/Users/markusmin/Documents/CBR/steelhead/stan_actual/deteff_ESU_models/middle_columbia/middle_columbia_adults_states_complete.csv

# What origins for MCH?
> midcol_hatchery_origin_table
       natal_origin    n
1   Deschutes_River   17
2    John_Day_River   46
3    Umatilla_River  591
4 Walla_Walla_River 2272
5      Yakima_River    1

So... just Umatilla River and Walla Walla River for hatchery.
Most Middle Columbia steelhead are wild

> midcol_wild_origin_table
       natal_origin    n
1   Deschutes_River 1031
2 Fifteenmile_Creek  485
3    John_Day_River 2802
4    Umatilla_River 1210
5 Walla_Walla_River  731
6      Yakima_River  670

> model_states
 [1] "mainstem, mouth to BON"         "mainstem, BON to MCN"           "mainstem, MCN to ICH or PRA"    "mainstem, PRA to RIS"           "mainstem, RIS to RRE"          
 [6] "mainstem, RRE to WEL"           "mainstem, upstream of WEL"      "mainstem, ICH to LGR"           "mainstem, upstream of LGR"      "Deschutes River Mouth"         
[11] "Deschutes River Upstream"       "John Day River Mouth"           "John Day River Upstream"        "Hood River Mouth"               "Hood River Upstream"           
[16] "Fifteenmile Creek Mouth"        "Fifteenmile Creek Upstream"     "Umatilla River Mouth"           "Umatilla River Upstream"        "Yakima River Mouth"            
[21] "Yakima River Upstream"          "Walla Walla River Mouth"        "Walla Walla River Upstream"     "Wenatchee River Mouth"          "Wenatchee River Upstream"      
[26] "Entiat River Mouth"             "Entiat River Upstream"          "Okanogan River Mouth"           "Okanogan River Upstream"        "Methow River Mouth"            
[31] "Methow River Upstream"          "Tucannon River Mouth"           "Tucannon River Upstream"        "Asotin Creek Mouth"             "Asotin Creek Upstream"         
[36] "Clearwater River"               "Salmon River"                   "Grande Ronde River"             "Imnaha River Mouth"             "Imnaha River Upstream"         
[41] "BON to MCN other tributaries"   "Upstream WEL other tributaries" "loss" 

# Made the necessary changes. One thing that would be good: for the portions of the script
that generate the lines of stan code, we should replace the 1:3 (if there are three origins)
in a loop with instead 1:norigins; would make things a lot easier

# Let's try running this with ckpt

(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r ./middle_columbia_hatchery/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/

[mmin@klone-login03 middle_columbia_hatchery]$ sbatch job_ckpt_MCH_reparam_chain1.slurm —mpi=pmix_v1

MCH successfully compiled and is running on ckpt! Let's submit the chains on stf

(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r ./middle_columbia_hatchery/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/
[mmin@klone-login03 middle_columbia_hatchery]$ bash MCH_reparam_submit_chains.sh 
Submitted batch job 16346095
Submitted batch job 16346096
Submitted batch job 16346097
Submitted batch job 16346098

# I think I'm going to hold off and wait to see how these runs go before running all of the other models as well.

### 2024-02-02 ###

Checking in on runs

MCH finished in 4-5 hours per chain

UCH is still running; at 20 hours, we are at 10-15% (400-600 iter)
- This should be fine to complete in 144 hours given past performance

# move MCH back to local
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/middle_columbia_hatchery ./

# MCH - Some observations:
Some of the detection efficiency parameters are really anticorrelated. Not really sure why that would be. But
the estimates are basically just the 

Some parameters that I don't think should be the same are the same...?
34 byearxorigin1_raw_parameters_matrix_DE[4,8]    -0.00481  -0.00135  0.988 0.980 -1.61   1.64    1.00   19743.    5698.
 35 byearxorigin2_raw_parameters_matrix_DE[4,8]    -0.00481  -0.00135  0.988 0.980 -1.61   1.64    1.00   19743.    5698.
 
- This is a typo in the model code. This typo is also in the UCW and UCH scripts. Frick!!!
 
 # some other weird parameters:
    variable                 mean     median    sd   mad        q5        q95   rhat ess_bulk ess_tail
   <chr>                   <num>      <num> <num> <num>     <num>      <num>  <num>    <num>    <num>
 1 b0_vector_DE[25]   -100000    -100000     0     0    -100000   -100000    NA          NA       NA 
 2 b0_vector_DE[41]   -100000    -100000     0     0    -100000   -100000    NA          NA       NA 
 3 b0_vector_NDE[41]  -100000    -100000     0     0    -100000   -100000    NA          NA       NA 

Oh, those are fixed at those values. But... why? b0_vector_DE[41] and b0_vector_NDE[41] probably 
shouldn't both be -100000, I think
No no, that's right. That is the last spot in the vector, and it should get -100000, because
that's how we're making sure the other movements are impossible (this last value in the 
vector is getting indexed by parameter_indices_matrix)
- b0_vector_DE[25] also makes sense to be -100000, that's upstream LGR to Asotin Creek Upstream, which isn't
allowed for DE, but it's also one of the few upstream states that was actually visited by a fish

### Let's fix those typos
First, cancel all runs in progress
# Get rid of old executables
[mmin@klone-login03 middle_columbia_hatchery]$ rm MCH_reparam
[mmin@klone-login03 upper_columbia_hatchery]$ rm UCH_reparam
[mmin@klone-login03 upper_columbia_wild]$ rm UCW_reparam

# Fix typos in stan_data_prep scripts, change in the stan script, move to hyak
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r ./middle_columbia_hatchery/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r ./upper_columbia_hatchery/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r ./upper_columbia_wild/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/

# Re-submit runs
[mmin@klone-login03 middle_columbia_hatchery]$ bash MCH_reparam_submit_chains.sh
[mmin@klone-login03 upper_columbia_hatchery]$ bash UCH_reparam_submit_chains.sh
[mmin@klone-login03 upper_columbia_wild]$ bash UCW_reparam_submit_chains.sh



## Develop MCW
In this script, we are going to have norigins as a global variable to make the part of the 
script where we cat the lines of stan code less typo-prone (and more generalizable) 

# Move to hyak, try on ckpt
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r ./middle_columbia_wild/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/

[mmin@klone-login03 middle_columbia_wild]$ sbatch job_ckpt_MCW_reparam_chain1.slurm —mpi=pmix_v1

Random runs keep strangely failing due to compilation errors:
[1] "Compiling model"
[1] "model compiled at"
[1] "2024-02-02 10:36:21 PST"
Error: Model not compiled. Try running the compile() method first.
Execution halted

- I believe that this is happening when we don't already have an executable, because 
running the submit chains shell script asks the model to compile four times, and then if they
are all in sync sometimes that can confuse the compiling

# Need to resubmit specific runs
[mmin@klone-login03 upper_columbia_wild]$ sbatch job_stf_UCW_reparam_chain1.slurm —mpi=pmix_v1
[mmin@klone-login03 upper_columbia_wild]$ sbatch job_stf_UCW_reparam_chain4.slurm —mpi=pmix_v1
[mmin@klone-login03 upper_columbia_hatchery]$ sbatch job_stf_UCH_reparam_chain2.slurm —mpi=pmix_v1
[mmin@klone-login03 upper_columbia_hatchery]$ sbatch job_stf_UCH_reparam_chain3.slurm —mpi=pmix_v1


# MCW ckpt:
[1] "Compiling model"
[1] "model compiled at"
[1] "2024-02-02 13:47:29 PST"
Running MCMC with 1 chain, with 28 thread(s) per chain...

Chain 1 Rejecting initial value:
Chain 1   Gradient evaluated at the initial value is not finite.
Chain 1   Stan can't start sampling from this initial value.
Chain 1 Initialization between (-1, 1) failed after 100 attempts.
Chain 1  Try specifying initial values, reducing ranges of constrained values, or reparameterizing the model.
Chain 1 Initialization failed.
Warning: Chain 1 finished unexpectedly!

Alright, so there's almost certainly a typo somewhere in here!
This is likely happening because it's evaluating the probability of something that is possible as zero, 
or we're getting a negative infinity or something there.
One approach would be to reintroduce the statement where we print the logits for each step

the movements which are currently getting a -100000 for DE are 2-11, 2-17, 2-19
thats (BON to MCN) to Deschutes River Upstream, Fifteenmile Creek Upstream, and Umatilla River Upstream
Makes sense, those should have that. Checked on the data itself and it also looks fine...?

Well, I found a typo in all of the other models. Currently NDE is not getting a universal 
temperature effect, just origin temperature effects (DE is getting a universal temperature effect)

# Cancel all the jobs currently running
[mmin@klone-login03 upper_columbia_wild]$ scancel --user=mmin

# Move the model files to hyak
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r ./middle_columbia_hatchery/MCH_reparam.stan mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/middle_columbia_hatchery/
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r ./upper_columbia_hatchery/UCH_reparam.stan mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/upper_columbia_hatchery/
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r ./upper_columbia_wild/UCW_reparam.stan mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/upper_columbia_wild/

# remove the existing executable
# submit the jobs
done!


### 2024-02-05 ###

MCH and UCW runs all completed, some UCH runs completed

MCH: around 5 hours
UCW: 15-22 hours (these are still 10000 iter no thin runs, whoops)
UCH: 2 days 14 hours

# Let's fix UCW to be 2000 iter and re-submit
- Deleted all of the old .rds model output files (1.4 GB total!)
- Created new R scripts with names "chainX_UCW_reparam.R" to match names in other folders
(base) markusmin@Markuss-MacBook-Pro-2 reparameterization % scp -r ./upper_columbia_wild/ mmin@klone.hyak.uw.edu:/gscratch/stf/mmin/stan_runs/reparameterization/
[mmin@klone-login03 upper_columbia_wild]$ bash UCW_reparam_submit_chains.sh 

# Still not sure why MCW is not running. Perhaps we should try having it print logits and actual and then checking
# Added some print statements. Re-run on ckpt:
[mmin@klone-login03 middle_columbia_wild]$ sbatch job_ckpt_MCW_reparam_chain1.slurm —mpi=pmix_v1






